"""Chinese GraphRAG CLI æŸ¥è©¢å‘½ä»¤ã€‚

æä¾›ä¸­æ–‡å•ç­”å’Œæª¢ç´¢åŠŸèƒ½ã€‚
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import List, Optional

import click
from loguru import logger
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich.text import Text

from ..monitoring import get_error_tracker, get_logger, get_metrics_collector
from ..query import QueryEngine

console = Console()
logger = get_logger(__name__)


def _get_default_llm_config(config, logger):
    """
    ç²å–é»˜èª LLM é…ç½®ï¼Œå„ªå…ˆä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„é»˜èªæ¨¡å‹

    Args:
        config: GraphRAG é…ç½®å°è±¡
        logger: æ—¥èªŒè¨˜éŒ„å™¨

    Returns:
        List[LLMConfig]: LLM é…ç½®åˆ—è¡¨
    """
    from ..query.manager import LLMConfig, LLMProvider

    # å„ªå…ˆä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„é»˜èª LLM æ¨¡å‹
    default_llm_name = config.model_selection.default_llm
    default_llm_config_obj = config.get_llm_config(default_llm_name)

    llm_configs = []

    if default_llm_config_obj:
        # æª¢æŸ¥ API é‡‘é‘°ï¼ˆOllama æ¨¡å‹ä¸éœ€è¦ï¼‰
        api_key = getattr(default_llm_config_obj, "api_key", None)
        model_type_str = str(default_llm_config_obj.type).lower()

        # Ollama æ¨¡å‹ä¸éœ€è¦ API é‡‘é‘°
        if api_key or "ollama" in model_type_str:
            # æ˜ å°„æ¨¡å‹é¡å‹åˆ° LLMProvider
            if "openai" in model_type_str:
                provider = LLMProvider.OPENAI
            elif "ollama" in model_type_str:
                provider = LLMProvider.OLLAMA
            else:
                provider = LLMProvider.MOCK

            default_config = LLMConfig(
                provider=provider,
                model=default_llm_config_obj.model,
                config={
                    "model": default_llm_config_obj.model,
                    "api_key": api_key,
                    "base_url": getattr(default_llm_config_obj, "api_base", None),
                    "temperature": getattr(default_llm_config_obj, "temperature", 0.7),
                },
                max_tokens=getattr(default_llm_config_obj, "max_tokens", 4000),
                temperature=getattr(default_llm_config_obj, "temperature", 0.7),
            )
            llm_configs.append(default_config)
            logger.info(
                f"ä½¿ç”¨é»˜èª LLM æ¨¡å‹: {default_llm_name} ({default_llm_config_obj.model})"
            )
            return llm_configs
        else:
            logger.warning(f"é»˜èª LLM æ¨¡å‹ {default_llm_name} ç¼ºå°‘ API é‡‘é‘°")

    # å¦‚æœé»˜èªæ¨¡å‹ä¸å¯ç”¨ï¼Œå˜—è©¦å‚™ç”¨æ¨¡å‹
    if hasattr(config.model_selection, "fallback_models"):
        fallback_name = config.model_selection.fallback_models.get(default_llm_name)
        if fallback_name:
            fallback_config = config.get_llm_config(fallback_name)
            if fallback_config:
                logger.info(f"ä½¿ç”¨å‚™ç”¨ LLM æ¨¡å‹: {fallback_name}")
                api_key = getattr(fallback_config, "api_key", None)
                model_type_str = str(fallback_config.type).lower()

                if api_key or "ollama" in model_type_str:
                    if "openai" in model_type_str:
                        provider = LLMProvider.OPENAI
                    elif "ollama" in model_type_str:
                        provider = LLMProvider.OLLAMA
                    else:
                        provider = LLMProvider.MOCK

                    fallback_llm_config = LLMConfig(
                        provider=provider,
                        model=fallback_config.model,
                        config={
                            "model": fallback_config.model,
                            "api_key": api_key,
                            "base_url": getattr(fallback_config, "api_base", None),
                            "temperature": getattr(fallback_config, "temperature", 0.7),
                        },
                        max_tokens=getattr(fallback_config, "max_tokens", 4000),
                        temperature=getattr(fallback_config, "temperature", 0.7),
                    )
                    llm_configs.append(fallback_llm_config)
                    return llm_configs

    # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œå‰µå»ºä¸€å€‹é»˜èªçš„æ¸¬è©¦é…ç½®
    logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®ï¼Œä½¿ç”¨æ¸¬è©¦æ¨¡å‹")
    default_llm_config = LLMConfig(
        provider=LLMProvider.MOCK,
        model="test_model",
        config={},
        max_tokens=4000,
        temperature=0.7,
    )
    llm_configs.append(default_llm_config)
    return llm_configs


@click.command()
@click.argument("question", type=str)
@click.option(
    "--search-type",
    "-t",
    type=click.Choice(["auto", "global", "local"]),
    default="auto",
    help="æœå°‹é¡å‹ï¼ˆè‡ªå‹•ã€å…¨åŸŸã€æœ¬åœ°ï¼‰",
)
@click.option("--max-tokens", type=int, help="å›ç­”æœ€å¤§ token æ•¸ï¼ˆè¦†è“‹é…ç½®è¨­å®šï¼‰")
@click.option(
    "--community-level", type=int, help="ç¤¾ç¾¤å±¤ç´šï¼ˆå…¨åŸŸæœå°‹ä½¿ç”¨ï¼Œè¦†è“‹é…ç½®è¨­å®šï¼‰"
)
@click.option(
    "--response-type",
    type=click.Choice(
        ["multiple_paragraphs", "single_paragraph", "real_time", "single_sentence"]
    ),
    help="å›ç­”æ ¼å¼é¡å‹",
)
@click.option(
    "--format",
    "output_format",
    type=click.Choice(["rich", "plain", "json", "markdown"]),
    default="rich",
    help="è¼¸å‡ºæ ¼å¼",
)
@click.option("--show-sources", is_flag=True, help="é¡¯ç¤ºè³‡æ–™ä¾†æº")
@click.option("--show-reasoning", is_flag=True, help="é¡¯ç¤ºæ¨ç†éç¨‹")
@click.option("--interactive", "-i", is_flag=True, help="é€²å…¥äº’å‹•æ¨¡å¼")
@click.option(
    "--enable-global-search",
    is_flag=True,
    help="å•Ÿç”¨å…¨åŸŸæœå°‹ï¼ˆé è¨­é—œé–‰ï¼Œåƒ…ä½¿ç”¨æœ¬åœ°æœå°‹ï¼‰",
)
@click.option(
    "--use-jieba-segmentation",
    is_flag=True,
    help="ä½¿ç”¨ jieba åˆ†è©ï¼ˆé è¨­ä½¿ç”¨ LLM åˆ†è©ï¼‰",
)
@click.option("--simple", "-s", is_flag=True, help="è¿”å›ç²¾ç°¡ç­”æ¡ˆï¼ˆä¸åŒ…å«è©³ç´°æ¨ç†éç¨‹ï¼‰")
@click.pass_context
def query(
    ctx: click.Context,
    question: str,
    search_type: str,
    max_tokens: Optional[int],
    community_level: Optional[int],
    response_type: Optional[str],
    output_format: str,
    show_sources: bool,
    show_reasoning: bool,
    interactive: bool,
    enable_global_search: bool,
    use_jieba_segmentation: bool,
    simple: bool,
):
    """åŸ·è¡Œä¸­æ–‡å•ç­”æŸ¥è©¢ã€‚

    ä½¿ç”¨ç¯„ä¾‹:

    \b
    # åŸºæœ¬æŸ¥è©¢ï¼ˆä½¿ç”¨ LLM åˆ†è©ï¼‰
    chinese-graphrag query "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"

    \b
    # ä½¿ç”¨ jieba åˆ†è©
    chinese-graphrag query "å°æ˜å§“ä»€éº¼" --use-jieba-segmentation

    \b
    # å•Ÿç”¨å…¨åŸŸæœå°‹
    chinese-graphrag query "ä»‹ç´¹æ©Ÿå™¨å­¸ç¿’çš„æ‡‰ç”¨" --enable-global-search

    \b
    # æŒ‡å®šæœå°‹é¡å‹
    chinese-graphrag query "æ·±åº¦å­¸ç¿’çš„æ­·å²" --search-type local

    \b
    # é¡¯ç¤ºè³‡æ–™ä¾†æº
    chinese-graphrag query "æ·±åº¦å­¸ç¿’çš„æ­·å²" --show-sources

    \b
    # äº’å‹•æ¨¡å¼
    chinese-graphrag query "" --interactive
    """
    if not ctx.obj["config"]:
        console.print(
            "[red]æœªæ‰¾åˆ°é…ç½®æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œ 'chinese-graphrag init' åˆå§‹åŒ–ç³»çµ±ã€‚[/red]"
        )
        sys.exit(1)

    config = ctx.obj["config"]

    try:
        # è¦†è“‹é…ç½®è¨­å®š
        if max_tokens:
            config.query.max_tokens = max_tokens
        if community_level:
            config.query.community_level = community_level
        if response_type:
            config.query.response_type = response_type

        # æ ¹æ“š CLI é¸é …è¦†è“‹å…¨åŸŸæœå°‹è¨­å®š
        if enable_global_search:
            config.query.enable_global_search = True
        else:
            config.query.enable_global_search = False

        # å»ºç«‹æŸ¥è©¢å¼•æ“
        from ..config.models import VectorStoreType
        from ..indexing import GraphRAGIndexer
        from ..vector_stores import VectorStoreManager

        # åˆå§‹åŒ–ç´¢å¼•å™¨
        indexer = GraphRAGIndexer(config)

        # åˆå§‹åŒ–å‘é‡å­˜å„²
        vector_store_type = config.vector_store.type
        if isinstance(vector_store_type, str):
            vector_store_type = VectorStoreType(vector_store_type)
        vector_store = VectorStoreManager(vector_store_type)

        # å‰µå»º QueryEngineConfig
        from ..query.engine import QueryEngineConfig
        from ..query.manager import LLMConfig, LLMProvider

        # ä½¿ç”¨æ–°çš„è¼”åŠ©å‡½æ•¸ç²å–é»˜èª LLM é…ç½®
        llm_configs = _get_default_llm_config(config, logger)

        query_engine_config = QueryEngineConfig(
            llm_configs=llm_configs,
            enable_global_search=getattr(config.query, "enable_global_search", False),
            enable_local_search=getattr(config.query, "enable_local_search", True),
            enable_drift_search=getattr(config.query, "enable_drift_search", False),
            max_global_communities=getattr(config.query, "max_global_communities", 5),
            max_local_entities=getattr(config.query, "max_local_entities", 10),
            max_text_units=getattr(config.query, "max_text_units", 20),
        )

        query_engine = QueryEngine(query_engine_config, config, indexer, vector_store)

        # æ±ºå®šåˆ†è©æ–¹æ³•ï¼šé»˜èªä½¿ç”¨ LLM åˆ†è©ï¼Œé™¤éç”¨æˆ¶æ˜ç¢ºæŒ‡å®šä½¿ç”¨ jieba
        use_llm_segmentation = not use_jieba_segmentation

        # äº’å‹•æ¨¡å¼
        if interactive or not question.strip():
            _run_interactive_mode(
                query_engine,
                search_type,
                show_sources,
                show_reasoning,
                output_format,
                ctx.obj["quiet"],
                use_llm_segmentation,
            )
            return

        # å–®æ¬¡æŸ¥è©¢
        _execute_single_query(
            query_engine,
            question,
            search_type,
            show_sources,
            show_reasoning,
            output_format,
            ctx.obj["quiet"],
            use_llm_segmentation,
        )

    except Exception as e:
        # è¨˜éŒ„éŒ¯èª¤
        error_tracker = get_error_tracker()
        from ..monitoring.error_tracker import ErrorCategory, ErrorSeverity

        error_tracker.track_error(
            e, category=ErrorCategory.PROCESSING, severity=ErrorSeverity.HIGH
        )

        logger.error(f"æŸ¥è©¢å¤±æ•—: {e}")
        if not ctx.obj["quiet"]:
            console.print(f"[red]æŸ¥è©¢å¤±æ•—: {e}[/red]")
        sys.exit(1)


@click.command()
@click.argument("question", required=True)
@click.pass_context
def test_llm_segmentation(ctx: click.Context, question: str):
    """æ¸¬è©¦ LLM åˆ†è©åŠŸèƒ½

    ä½¿ç”¨ç¯„ä¾‹:
    chinese-graphrag test-llm-segmentation "å°æ˜å§“ä»€éº¼"
    """
    if not ctx.obj["config"]:
        console.print(
            "[red]æœªæ‰¾åˆ°é…ç½®æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œ 'chinese-graphrag init' åˆå§‹åŒ–ç³»çµ±ã€‚[/red]"
        )
        sys.exit(1)

    config = ctx.obj["config"]

    try:
        # ä½¿ç”¨æ–°çš„è¼”åŠ©å‡½æ•¸ç²å–é»˜èª LLM é…ç½®
        from ..query.manager import LLMManager
        from ..query.processor import ChineseTextNormalizer

        llm_configs = _get_default_llm_config(config, logger)

        if not llm_configs:
            console.print("[red]æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®[/red]")
            sys.exit(1)

        # åˆå§‹åŒ– LLM ç®¡ç†å™¨
        llm_manager = LLMManager(llm_configs)

        # æ¸¬è©¦åˆ†è©
        async def run_test():
            console.print(f"[bold blue]æ¸¬è©¦æŸ¥è©¢: {question}[/bold blue]\n")

            # å…ˆæ¸¬è©¦ä¸ä½¿ç”¨ LLM çš„æƒ…æ³
            normalizer_no_llm = ChineseTextNormalizer()

            # å°æ¯” jieba åˆ†è©
            import jieba

            jieba_segments = list(jieba.cut(question, cut_all=False))
            jieba_keywords = normalizer_no_llm.extract_keywords(question)

            console.print("[cyan]jieba åˆ†è©çµæœ:[/cyan]")
            console.print(f"  åˆ†è©: {jieba_segments}")
            console.print(f"  é—œéµè©: {jieba_keywords}\n")

            # æ¸¬è©¦ LLM åˆ†è©
            normalizer = ChineseTextNormalizer(llm_manager)

            try:
                with console.status("[bold green]æ­£åœ¨ä½¿ç”¨ LLM åˆ†è©..."):
                    llm_segments = await normalizer.llm_segment_text(question)
                    llm_keywords = await normalizer.extract_keywords_with_llm(question)

                console.print("[cyan]LLM åˆ†è©çµæœ:[/cyan]")
                console.print(f"  åˆ†è©: {llm_segments}")
                console.print(f"  é—œéµè©: {llm_keywords}")

            except Exception as e:
                console.print(f"[red]LLM åˆ†è©å¤±æ•—: {e}[/red]")

        # åŸ·è¡Œæ¸¬è©¦
        asyncio.run(run_test())

    except Exception as e:
        logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")
        console.print(f"[red]æ¸¬è©¦å¤±æ•—: {e}[/red]")
        sys.exit(1)


def _execute_single_query(
    query_engine: QueryEngine,
    question: str,
    search_type: str,
    show_sources: bool,
    show_reasoning: bool,
    output_format: str,
    quiet: bool,
    use_llm_segmentation: bool = True,  # æ–°å¢åƒæ•¸ï¼Œé»˜èªä½¿ç”¨ LLM åˆ†è©
    simple: bool = False,  # æ–°å¢ç²¾ç°¡æ¨¡å¼åƒæ•¸
):
    """åŸ·è¡Œå–®æ¬¡æŸ¥è©¢ã€‚"""
    start_time = time.time()

    if not quiet and output_format == "rich":
        console.print(f"\n[bold blue]ğŸ¤” å•é¡Œ: {question}[/bold blue]")

        with console.status("[bold green]æ­£åœ¨æ€è€ƒ..."):
            # æ ¹æ“šåƒæ•¸é¸æ“‡ä½¿ç”¨ LLM åˆ†è©æˆ–æ™®é€šåˆ†è©
            if use_llm_segmentation:
                unified_result = asyncio.run(
                    query_engine.query_with_llm_segmentation(
                        question, search_type=search_type
                    )
                )
            else:
                unified_result = asyncio.run(
                    query_engine.query(question, search_type=search_type)
                )
        result = unified_result.to_dict()
        # æ·»åŠ  CLI æœŸæœ›çš„å­—æ®µæ˜ å°„
        result["response"] = unified_result.answer
        result["reasoning"] = unified_result.reasoning_path
    else:
        # æ ¹æ“šåƒæ•¸é¸æ“‡ä½¿ç”¨ LLM åˆ†è©æˆ–æ™®é€šåˆ†è©
        if use_llm_segmentation:
            unified_result = asyncio.run(
                query_engine.query_with_llm_segmentation(
                    question, search_type=search_type
                )
            )
        else:
            unified_result = asyncio.run(
                query_engine.query(question, search_type=search_type)
            )
        result = unified_result.to_dict()
        # æ·»åŠ  CLI æœŸæœ›çš„å­—æ®µæ˜ å°„
        result["response"] = unified_result.answer
        result["reasoning"] = unified_result.reasoning_path

    elapsed_time = time.time() - start_time

    # å¦‚æœæ˜¯ç²¾ç°¡æ¨¡å¼ï¼Œç›´æ¥è¼¸å‡ºç­”æ¡ˆ
    if simple:
        _display_simple_result(result, output_format, elapsed_time, quiet)
    else:
        # æ ¼å¼åŒ–è¼¸å‡º
        _display_query_result(
            result, show_sources, show_reasoning, output_format, elapsed_time, quiet
        )

    # è¨˜éŒ„æŒ‡æ¨™
    metrics_collector = get_metrics_collector()
    metrics_collector.record_counter("query.completed", 1)
    metrics_collector.record_timer("query.response_time", elapsed_time)
    metrics_collector.record_gauge(
        "query.search_type", 1, labels={"type": result.get("search_type", "unknown")}
    )

    # åœ¨æ—¥èªŒä¸­æ¨™è¨»ä½¿ç”¨çš„åˆ†è©æ–¹æ³•
    segmentation_method = "LLMåˆ†è©" if use_llm_segmentation else "jiebaåˆ†è©"
    mode = "ç²¾ç°¡æ¨¡å¼" if simple else "å®Œæ•´æ¨¡å¼"
    logger.info(
        f"æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚ {elapsed_time:.2f} ç§’ï¼Œæœå°‹é¡å‹: {result.get('search_type', 'unknown')}ï¼Œåˆ†è©æ–¹æ³•: {segmentation_method}ï¼Œæ¨¡å¼: {mode}"
    )


def _run_interactive_mode(
    query_engine: QueryEngine,
    default_search_type: str,
    show_sources: bool,
    show_reasoning: bool,
    output_format: str,
    quiet: bool,
    use_llm_segmentation: bool = True,  # æ–°å¢åƒæ•¸
):
    """é‹è¡Œäº’å‹•æ¨¡å¼ã€‚"""
    if not quiet:
        console.print("\n[bold green]ğŸ¯ é€²å…¥äº’å‹•æŸ¥è©¢æ¨¡å¼[/bold green]")
        segmentation_method = "LLMåˆ†è©" if use_llm_segmentation else "jiebaåˆ†è©"
        console.print(f"ç•¶å‰åˆ†è©æ–¹æ³•: [cyan]{segmentation_method}[/cyan]")
        console.print("è¼¸å…¥ 'exit' æˆ– 'quit' é€€å‡ºï¼Œè¼¸å…¥ 'help' æŸ¥çœ‹å¹«åŠ©")
        console.print("-" * 50)

    while True:
        try:
            # ç²å–ç”¨æˆ¶è¼¸å…¥
            question = console.input(
                "\n[bold cyan]è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: [/bold cyan]"
            ).strip()

            if not question:
                continue

            # è™•ç†ç‰¹æ®Šå‘½ä»¤
            if question.lower() in ["exit", "quit", "q"]:
                console.print("[yellow]å†è¦‹ï¼[/yellow]")
                break
            elif question.lower() in ["help", "h"]:
                _show_interactive_help()
                continue
            elif question.lower().startswith("set "):
                _handle_settings_command(question)
                continue

            # åŸ·è¡ŒæŸ¥è©¢
            _execute_single_query(
                query_engine,
                question,
                default_search_type,
                show_sources,
                show_reasoning,
                output_format,
                quiet,
                use_llm_segmentation,
            )

        except KeyboardInterrupt:
            console.print("\n[yellow]å†è¦‹ï¼[/yellow]")
            break
        except EOFError:
            console.print("\n[yellow]å†è¦‹ï¼[/yellow]")
            break


def _show_interactive_help():
    """é¡¯ç¤ºäº’å‹•æ¨¡å¼å¹«åŠ©ã€‚"""
    help_text = """
[bold]äº’å‹•æ¨¡å¼å‘½ä»¤:[/bold]

[cyan]æŸ¥è©¢å‘½ä»¤:[/cyan]
  ç›´æ¥è¼¸å…¥å•é¡Œ                     - åŸ·è¡ŒæŸ¥è©¢
  
[cyan]è¨­å®šå‘½ä»¤:[/cyan]
  set search-type <type>          - è¨­å®šæœå°‹é¡å‹ (auto/global/local)
  set sources <on/off>            - é–‹å•Ÿ/é—œé–‰ä¾†æºé¡¯ç¤º
  set reasoning <on/off>          - é–‹å•Ÿ/é—œé–‰æ¨ç†éç¨‹é¡¯ç¤º
  set segmentation <llm/jieba>    - è¨­å®šåˆ†è©æ–¹æ³• (LLMåˆ†è©/jiebaåˆ†è©)
  
[cyan]ç³»çµ±å‘½ä»¤:[/cyan]
  help, h                         - é¡¯ç¤ºæ­¤å¹«åŠ©
  exit, quit, q                   - é€€å‡ºäº’å‹•æ¨¡å¼

[yellow]æ³¨æ„:[/yellow] åˆ†è©æ–¹æ³•è¨­å®šåŠŸèƒ½å°šæœªå¯¦ä½œï¼Œè«‹ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸ --use-jieba-segmentation
"""
    console.print(Panel(help_text, title="å¹«åŠ©", expand=False))


def _handle_settings_command(command: str):
    """è™•ç†è¨­å®šå‘½ä»¤ã€‚"""
    # é€™è£¡å¯ä»¥å¯¦ä½œè¨­å®šè®Šæ›´é‚è¼¯
    # æš«æ™‚é¡¯ç¤ºèªªæ˜
    console.print("[yellow]è¨­å®šåŠŸèƒ½å°šæœªå¯¦ä½œ[/yellow]")


def _display_query_result(
    result: dict,
    show_sources: bool,
    show_reasoning: bool,
    output_format: str,
    elapsed_time: float,
    quiet: bool,
):
    """é¡¯ç¤ºæŸ¥è©¢çµæœã€‚"""
    if output_format == "json":
        import json

        console.print(json.dumps(result, indent=2, ensure_ascii=False))
        return

    if output_format == "plain":
        console.print(result.get("response", ""))
        return

    if output_format == "markdown":
        markdown_output = f"# æŸ¥è©¢çµæœ\n\n{result.get('response', '')}"
        if show_sources and result.get("sources"):
            markdown_output += "\n\n## è³‡æ–™ä¾†æº\n\n"
            for i, source in enumerate(result["sources"], 1):
                markdown_output += f"{i}. {source}\n"
        console.print(Syntax(markdown_output, "markdown"))
        return

    # Rich æ ¼å¼ï¼ˆé è¨­ï¼‰
    if not quiet:
        # ä¸»è¦å›ç­”
        response_text = result.get("response", "æœªæ‰¾åˆ°å›ç­”")
        console.print(
            Panel(
                response_text,
                title="[bold green]ğŸ“ å›ç­”",
                expand=False,
                border_style="green",
            )
        )

        # æœå°‹é¡å‹å’Œçµ±è¨ˆ
        stats_table = Table(show_header=False, box=None, pad_edge=False)
        stats_table.add_column("é …ç›®", style="cyan", width=12)
        stats_table.add_column("å€¼", style="green")

        stats_table.add_row("æœå°‹é¡å‹", result.get("search_type", "unknown"))
        stats_table.add_row("å›æ‡‰æ™‚é–“", f"{elapsed_time:.2f} ç§’")
        if result.get("tokens_used"):
            stats_table.add_row("ä½¿ç”¨ Token", str(result["tokens_used"]))

        console.print("\n[bold]æŸ¥è©¢çµ±è¨ˆ:[/bold]")
        console.print(stats_table)

        # è³‡æ–™ä¾†æº
        if show_sources and result.get("sources"):
            console.print("\n[bold]ğŸ“š è³‡æ–™ä¾†æº:[/bold]")
            sources_table = Table(show_header=False)
            sources_table.add_column("#", style="dim", width=3)
            sources_table.add_column("ä¾†æº", style="blue")

            for i, source in enumerate(result["sources"], 1):
                sources_table.add_row(str(i), source)

            console.print(sources_table)

        # æ¨ç†éç¨‹
        if show_reasoning and result.get("reasoning"):
            console.print("\n[bold]ğŸ§  æ¨ç†éç¨‹:[/bold]")
            reasoning_text = result["reasoning"]
            console.print(Panel(reasoning_text, border_style="blue", expand=False))
    else:
        # éœé»˜æ¨¡å¼åªè¼¸å‡ºå›ç­”
        console.print(result.get("response", ""))


@click.command()
@click.option(
    "--input",
    "-i",
    "input_path",
    type=click.Path(exists=True, path_type=Path),
    required=True,
    help="ç´¢å¼•çµæœç›®éŒ„è·¯å¾‘",
)
@click.option(
    "--output",
    "-o",
    "output_path",
    type=click.Path(path_type=Path),
    help="æŸ¥è©¢çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘",
)
@click.option(
    "--questions-file",
    type=click.Path(exists=True, path_type=Path),
    help="æ‰¹æ¬¡æŸ¥è©¢å•é¡Œæª”æ¡ˆï¼ˆæ¯è¡Œä¸€å€‹å•é¡Œï¼‰",
)
@click.option(
    "--search-type",
    "-t",
    type=click.Choice(["auto", "global", "local"]),
    default="auto",
    help="æœå°‹é¡å‹",
)
@click.option(
    "--format",
    "output_format",
    type=click.Choice(["json", "csv", "markdown", "txt"]),
    default="json",
    help="è¼¸å‡ºæ ¼å¼",
)
@click.pass_context
def batch_query(
    ctx: click.Context,
    input_path: Path,
    output_path: Optional[Path],
    questions_file: Optional[Path],
    search_type: str,
    output_format: str,
):
    """æ‰¹æ¬¡æŸ¥è©¢è™•ç†ã€‚

    å¾æª”æ¡ˆè®€å–å•é¡Œåˆ—è¡¨ä¸¦æ‰¹æ¬¡åŸ·è¡ŒæŸ¥è©¢ã€‚
    """
    if not ctx.obj["config"]:
        console.print(
            "[red]æœªæ‰¾åˆ°é…ç½®æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œ 'chinese-graphrag init' åˆå§‹åŒ–ç³»çµ±ã€‚[/red]"
        )
        sys.exit(1)

    if not questions_file:
        console.print("[red]è«‹æŒ‡å®šå•é¡Œæª”æ¡ˆè·¯å¾‘ï¼ˆ--questions-fileï¼‰[/red]")
        sys.exit(1)

    try:
        # è®€å–å•é¡Œåˆ—è¡¨
        with open(questions_file, "r", encoding="utf-8") as f:
            questions = [line.strip() for line in f if line.strip()]

        if not questions:
            console.print("[yellow]æœªæ‰¾åˆ°æœ‰æ•ˆå•é¡Œ[/yellow]")
            return

        config = ctx.obj["config"]
        from ..config.models import VectorStoreType
        from ..indexing import GraphRAGIndexer
        from ..vector_stores import VectorStoreManager

        # åˆå§‹åŒ–ç´¢å¼•å™¨
        indexer = GraphRAGIndexer(config)

        # åˆå§‹åŒ–å‘é‡å­˜å„²
        vector_store_type = config.vector_store.type
        if isinstance(vector_store_type, str):
            vector_store_type = VectorStoreType(vector_store_type)
        vector_store = VectorStoreManager(vector_store_type)

        # å‰µå»º QueryEngineConfig
        from ..query.engine import QueryEngineConfig
        from ..query.manager import LLMConfig, LLMProvider

        # ä½¿ç”¨æ–°çš„è¼”åŠ©å‡½æ•¸ç²å–é»˜èª LLM é…ç½®
        llm_configs = _get_default_llm_config(config, logger)

        query_engine_config = QueryEngineConfig(
            llm_configs=llm_configs,
            enable_global_search=getattr(config.query, "enable_global_search", False),
            enable_local_search=getattr(config.query, "enable_local_search", True),
            enable_drift_search=getattr(config.query, "enable_drift_search", False),
            max_global_communities=getattr(config.query, "max_global_communities", 5),
            max_local_entities=getattr(config.query, "max_local_entities", 10),
            max_text_units=getattr(config.query, "max_text_units", 20),
        )

        query_engine = QueryEngine(query_engine_config, config, indexer, vector_store)

        results = []

        if not ctx.obj["quiet"]:
            console.print(f"[bold]é–‹å§‹æ‰¹æ¬¡æŸ¥è©¢ï¼Œå…± {len(questions)} å€‹å•é¡Œ[/bold]")

        # åŸ·è¡Œæ‰¹æ¬¡æŸ¥è©¢
        for i, question in enumerate(questions, 1):
            if not ctx.obj["quiet"]:
                console.print(f"\n[cyan]{i}/{len(questions)}[/cyan] {question}")

            start_time = time.time()
            unified_result = asyncio.run(
                query_engine.query(question, search_type=search_type)
            )
            result = unified_result.to_dict()
            # æ·»åŠ  CLI æœŸæœ›çš„å­—æ®µæ˜ å°„
            result["response"] = unified_result.answer
            result["reasoning"] = unified_result.reasoning_path

            elapsed_time = time.time() - start_time

            result["question"] = question
            result["query_time"] = elapsed_time
            result["index"] = i
            results.append(result)

            if not ctx.obj["quiet"]:
                console.print(f"[green]âœ“ å®Œæˆ ({elapsed_time:.2f}s)[/green]")

        # ä¿å­˜çµæœ
        if output_path:
            _save_batch_results(results, output_path, output_format)
            if not ctx.obj["quiet"]:
                console.print(
                    f"\n[bold green]âœ“ çµæœå·²ä¿å­˜è‡³: {output_path}[/bold green]"
                )
        else:
            # é¡¯ç¤ºçµæœæ‘˜è¦
            if not ctx.obj["quiet"]:
                _display_batch_summary(results)

        # è¨˜éŒ„æŒ‡æ¨™
        metrics_collector = get_metrics_collector()
        metrics_collector.record_counter("batch_query.completed", 1)
        metrics_collector.record_gauge("batch_query.questions_count", len(questions))

        logger.info(f"æ‰¹æ¬¡æŸ¥è©¢å®Œæˆï¼Œè™•ç†äº† {len(questions)} å€‹å•é¡Œ")

    except Exception as e:
        logger.error(f"æ‰¹æ¬¡æŸ¥è©¢å¤±æ•—: {e}")
        if not ctx.obj["quiet"]:
            console.print(f"[red]æ‰¹æ¬¡æŸ¥è©¢å¤±æ•—: {e}[/red]")
        sys.exit(1)


def _save_batch_results(results: List[dict], output_path: Path, format: str):
    """ä¿å­˜æ‰¹æ¬¡æŸ¥è©¢çµæœã€‚"""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if format == "json":
        import json

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    elif format == "csv":
        import csv

        with open(output_path, "w", encoding="utf-8", newline="") as f:
            if results:
                fieldnames = [
                    "index",
                    "question",
                    "response",
                    "search_type",
                    "query_time",
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

                for result in results:
                    writer.writerow(
                        {
                            "index": result.get("index"),
                            "question": result.get("question"),
                            "response": result.get("response", "").replace("\n", " "),
                            "search_type": result.get("search_type"),
                            "query_time": result.get("query_time", 0),
                        }
                    )

    elif format == "markdown":
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("# æ‰¹æ¬¡æŸ¥è©¢çµæœ\n\n")
            for result in results:
                f.write(f"## {result.get('index')}. {result.get('question')}\n\n")
                f.write(f"{result.get('response', '')}\n\n")
                f.write(f"**æœå°‹é¡å‹**: {result.get('search_type')}\n")
                f.write(f"**æŸ¥è©¢æ™‚é–“**: {result.get('query_time', 0):.2f} ç§’\n\n")
                f.write("---\n\n")

    elif format == "txt":
        with open(output_path, "w", encoding="utf-8") as f:
            for result in results:
                f.write(f"å•é¡Œ {result.get('index')}: {result.get('question')}\n")
                f.write(f"å›ç­”: {result.get('response', '')}\n")
                f.write(f"æœå°‹é¡å‹: {result.get('search_type')}\n")
                f.write(f"æŸ¥è©¢æ™‚é–“: {result.get('query_time', 0):.2f} ç§’\n")
                f.write("=" * 50 + "\n\n")


def _display_batch_summary(results: List[dict]):
    """é¡¯ç¤ºæ‰¹æ¬¡æŸ¥è©¢æ‘˜è¦ã€‚"""
    total_time = sum(r.get("query_time", 0) for r in results)
    avg_time = total_time / len(results) if results else 0

    search_type_counts = {}
    for result in results:
        search_type = result.get("search_type", "unknown")
        search_type_counts[search_type] = search_type_counts.get(search_type, 0) + 1

    # æ‘˜è¦è¡¨
    table = Table(title="æ‰¹æ¬¡æŸ¥è©¢æ‘˜è¦")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("å€¼", style="green")

    table.add_row("ç¸½å•é¡Œæ•¸", str(len(results)))
    table.add_row("ç¸½è™•ç†æ™‚é–“", f"{total_time:.2f} ç§’")
    table.add_row("å¹³å‡è™•ç†æ™‚é–“", f"{avg_time:.2f} ç§’")

    for search_type, count in search_type_counts.items():
        table.add_row(f"{search_type} æŸ¥è©¢", str(count))

    console.print(table)


# å°‡æŸ¥è©¢å‘½ä»¤å°å‡ºåˆ° main.py ä½¿ç”¨
__all__ = ["query", "batch_query"]
