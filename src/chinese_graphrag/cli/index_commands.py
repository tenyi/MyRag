"""Chinese GraphRAG CLI ç´¢å¼•å‘½ä»¤ã€‚

æä¾›æ–‡ä»¶ç´¢å¼•å’ŒçŸ¥è­˜åœ–è­œå»ºæ§‹åŠŸèƒ½ã€‚
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Optional

import click
from rich.console import Console
from rich.progress import (
    Progress, 
    SpinnerColumn, 
    TextColumn, 
    BarColumn, 
    TaskProgressColumn,
    TimeElapsedColumn
)
from rich.table import Table

from ..indexing import IndexingEngine
from ..monitoring import get_logger, get_metrics_collector, get_error_tracker

console = Console()
logger = get_logger(__name__)


@click.command()
@click.option(
    "--input", "-i",
    "input_path",
    type=click.Path(exists=True, path_type=Path),
    required=True,
    help="è¼¸å…¥æ–‡ä»¶ç›®éŒ„æˆ–æª”æ¡ˆè·¯å¾‘"
)
@click.option(
    "--output", "-o", 
    "output_path",
    type=click.Path(path_type=Path),
    help="è¼¸å‡ºç›®éŒ„è·¯å¾‘ï¼ˆé è¨­ä½¿ç”¨é…ç½®ä¸­çš„è¨­å®šï¼‰"
)
@click.option(
    "--resume",
    is_flag=True,
    help="å¾ä¸Šæ¬¡ä¸­æ–·çš„åœ°æ–¹ç¹¼çºŒç´¢å¼•"
)
@click.option(
    "--incremental",
    is_flag=True,
    help="å¢é‡ç´¢å¼•æ¨¡å¼ï¼ˆåªè™•ç†æ–°å¢æˆ–ä¿®æ”¹çš„æª”æ¡ˆï¼‰"
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="è©¦é‹è¡Œæ¨¡å¼ï¼ˆä¸å¯¦éš›åŸ·è¡Œç´¢å¼•ï¼‰"
)
@click.option(
    "--parallel-jobs", "-j",
    type=int,
    help="ä¸¦è¡Œä½œæ¥­æ•¸é‡ï¼ˆè¦†è“‹é…ç½®è¨­å®šï¼‰"
)
@click.option(
    "--chunk-size",
    type=int,
    help="æ–‡æœ¬åˆ†å¡Šå¤§å°ï¼ˆè¦†è“‹é…ç½®è¨­å®šï¼‰"
)
@click.option(
    "--skip-extraction",
    type=click.Choice(["entities", "relationships", "communities"]),
    multiple=True,
    help="è·³éæŒ‡å®šçš„æå–æ­¥é©Ÿ"
)
@click.option(
    "--format",
    type=click.Choice(["auto", "txt", "pdf", "docx", "md"]),
    default="auto",
    help="å¼·åˆ¶æŒ‡å®šè¼¸å…¥æª”æ¡ˆæ ¼å¼"
)
@click.pass_context
def index(
    ctx: click.Context,
    input_path: Path,
    output_path: Optional[Path],
    resume: bool,
    incremental: bool,
    dry_run: bool,
    parallel_jobs: Optional[int],
    chunk_size: Optional[int],
    skip_extraction: tuple,
    format: str
):
    """å°æ–‡ä»¶é€²è¡Œç´¢å¼•ï¼Œå»ºæ§‹çŸ¥è­˜åœ–è­œã€‚
    
    ä½¿ç”¨ç¯„ä¾‹:
    
    \b
    # ç´¢å¼•å–®ä¸€æ–‡ä»¶
    chinese-graphrag index -i document.txt
    
    \b
    # ç´¢å¼•æ•´å€‹ç›®éŒ„
    chinese-graphrag index -i ./documents -o ./output
    
    \b
    # å¢é‡ç´¢å¼•
    chinese-graphrag index -i ./documents --incremental
    
    \b  
    # æŒ‡å®šä¸¦è¡Œä½œæ¥­æ•¸
    chinese-graphrag index -i ./documents -j 8
    """
    if not ctx.obj['config']:
        console.print("[red]æœªæ‰¾åˆ°é…ç½®æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œ 'chinese-graphrag init' åˆå§‹åŒ–ç³»çµ±ã€‚[/red]")
        sys.exit(1)
    
    config = ctx.obj['config']
    
    try:
        # è¦†è“‹é…ç½®è¨­å®š
        if output_path:
            config.storage.base_dir = str(output_path)
        if parallel_jobs:
            config.parallelization.num_threads = parallel_jobs
        if chunk_size:
            config.chunks.size = chunk_size
        
        # è™•ç†è·³éçš„æå–æ­¥é©Ÿ
        if "entities" in skip_extraction:
            config.indexing.enable_entity_extraction = False
        if "relationships" in skip_extraction:
            config.indexing.enable_relationship_extraction = False
        if "communities" in skip_extraction:
            config.indexing.enable_community_detection = False
        
        # å»ºç«‹ç´¢å¼•å¼•æ“
        indexing_engine = IndexingEngine(config)
        
        # æª¢æŸ¥è¼¸å…¥è·¯å¾‘
        if not input_path.exists():
            console.print(f"[red]è¼¸å…¥è·¯å¾‘ä¸å­˜åœ¨: {input_path}[/red]")
            sys.exit(1)
        
        # æ”¶é›†è¦è™•ç†çš„æª”æ¡ˆ
        files_to_process = _collect_files(input_path, format, config.input.supported_formats)
        
        if not files_to_process:
            console.print("[yellow]æœªæ‰¾åˆ°å¯è™•ç†çš„æª”æ¡ˆ[/yellow]")
            sys.exit(0)
        
        # é¡¯ç¤ºè™•ç†æ‘˜è¦
        if not ctx.obj['quiet']:
            _show_processing_summary(files_to_process, config, dry_run, resume, incremental)
        
        if dry_run:
            console.print("[yellow]è©¦é‹è¡Œæ¨¡å¼å·²å®Œæˆ[/yellow]")
            return
        
        # åŸ·è¡Œç´¢å¼•
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
            disable=ctx.obj['quiet']
        ) as progress:
            
            # å»ºç«‹é€²åº¦ä»»å‹™
            total_task = progress.add_task("ç¸½é€²åº¦", total=len(files_to_process))
            current_task = progress.add_task("ç•¶å‰æª”æ¡ˆ", total=100)
            
            # è¨­å®šé€²åº¦å›èª¿
            def update_progress(current_file: str, file_progress: float, overall_progress: int):
                progress.update(current_task, description=f"è™•ç†: {Path(current_file).name}", completed=file_progress)
                progress.update(total_task, completed=overall_progress)
            
            # åŸ·è¡Œç´¢å¼•
            result = asyncio.run(
                indexing_engine.process_documents(
                    files_to_process,
                    progress_callback=update_progress,
                    resume=resume,
                    incremental=incremental
                )
            )
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        elapsed_time = time.time() - start_time
        
        # é¡¯ç¤ºçµæœ
        if not ctx.obj['quiet']:
            _show_indexing_results(result, elapsed_time)
        
        # è¨˜éŒ„æŒ‡æ¨™
        metrics_collector = get_metrics_collector()
        metrics_collector.record_counter("indexing.completed", 1)
        metrics_collector.record_gauge("indexing.documents_processed", len(files_to_process))
        metrics_collector.record_timer("indexing.total_time", elapsed_time)
        
        logger.info(f"ç´¢å¼•å®Œæˆï¼Œè™•ç†äº† {len(files_to_process)} å€‹æª”æ¡ˆï¼Œè€—æ™‚ {elapsed_time:.2f} ç§’")
        
    except Exception as e:
        # è¨˜éŒ„éŒ¯èª¤
        import traceback
        full_traceback = traceback.format_exc()
        error_tracker = get_error_tracker()
        from ..monitoring.error_tracker import ErrorCategory, ErrorSeverity
        error_tracker.track_error(e, category=ErrorCategory.PROCESSING, severity=ErrorSeverity.HIGH)
        
        logger.error(f"ç´¢å¼•å¤±æ•—: {e}")
        logger.error(f"å®Œæ•´éŒ¯èª¤è¿½è¸ª: {full_traceback}")
        if not ctx.obj['quiet']:
            console.print(f"[red]ç´¢å¼•å¤±æ•—: {e}[/red]")
            console.print(f"[red]å®Œæ•´éŒ¯èª¤è¿½è¸ª:[/red]")
            console.print(full_traceback)
        sys.exit(1)


def _collect_files(input_path: Path, format_filter: str, supported_formats: list) -> list[Path]:
    """æ”¶é›†è¦è™•ç†çš„æª”æ¡ˆã€‚"""
    files = []
    
    if input_path.is_file():
        # å–®ä¸€æª”æ¡ˆ
        if format_filter == "auto":
            if input_path.suffix.lstrip('.') in supported_formats:
                files.append(input_path)
        else:
            if input_path.suffix.lstrip('.') == format_filter:
                files.append(input_path)
    else:
        # ç›®éŒ„
        for ext in supported_formats:
            if format_filter == "auto" or format_filter == ext:
                files.extend(input_path.rglob(f"*.{ext}"))
    
    return sorted(files)


def _show_processing_summary(files: list[Path], config, dry_run: bool, resume: bool, incremental: bool):
    """é¡¯ç¤ºè™•ç†æ‘˜è¦ã€‚"""
    table = Table(title="ç´¢å¼•è¨­å®šæ‘˜è¦")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("å€¼", style="green")
    
    table.add_row("æª”æ¡ˆæ•¸é‡", str(len(files)))
    table.add_row("æ¨¡å¼", "è©¦é‹è¡Œ" if dry_run else ("æ¢å¾©" if resume else ("å¢é‡" if incremental else "å®Œæ•´")))
    table.add_row("ä¸¦è¡Œä½œæ¥­", str(config.parallelization.num_threads))
    table.add_row("æ–‡æœ¬åˆ†å¡Šå¤§å°", str(config.chunks.size))
    table.add_row("è¼¸å‡ºç›®éŒ„", config.storage.base_dir)
    table.add_row("å¯¦é«”æå–", "âœ“" if config.indexing.enable_entity_extraction else "Ã—")
    table.add_row("é—œä¿‚æå–", "âœ“" if config.indexing.enable_relationship_extraction else "Ã—")
    table.add_row("ç¤¾ç¾¤åµæ¸¬", "âœ“" if config.indexing.enable_community_detection else "Ã—")
    
    console.print(table)
    console.print()


def _show_indexing_results(result: dict, elapsed_time: float):
    """é¡¯ç¤ºç´¢å¼•çµæœã€‚"""
    console.print("\n[bold green]ğŸ‰ ç´¢å¼•å®Œæˆï¼[/bold green]")
    
    # çµæœçµ±è¨ˆè¡¨
    table = Table(title="è™•ç†çµæœ")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("æ•¸é‡", style="green")
    
    table.add_row("è™•ç†æª”æ¡ˆ", str(result.get('documents_processed', 0)))
    table.add_row("æ–‡æœ¬å¡Š", str(result.get('chunks_created', 0)))
    table.add_row("æå–å¯¦é«”", str(result.get('entities_extracted', 0)))
    table.add_row("ç™¼ç¾é—œä¿‚", str(result.get('relationships_found', 0)))
    table.add_row("è­˜åˆ¥ç¤¾ç¾¤", str(result.get('communities_detected', 0)))
    table.add_row("è™•ç†æ™‚é–“", f"{elapsed_time:.2f} ç§’")
    
    console.print(table)
    
    # è¼¸å‡ºæª”æ¡ˆä½ç½®
    if result.get('output_files'):
        console.print("\n[bold]è¼¸å‡ºæª”æ¡ˆ:[/bold]")
        for file_type, file_path in result['output_files'].items():
            console.print(f"  {file_type}: {file_path}")
    
    console.print("\n[bold]ä¸‹ä¸€æ­¥:[/bold]")
    console.print("åŸ·è¡ŒæŸ¥è©¢: chinese-graphrag query \"æ‚¨çš„å•é¡Œ\"")


@click.command()
@click.option(
    "--input", "-i",
    "input_path", 
    type=click.Path(exists=True, path_type=Path),
    required=True,
    help="ç´¢å¼•çµæœç›®éŒ„è·¯å¾‘"
)
@click.option(
    "--format",
    type=click.Choice(["table", "json", "detailed"]),
    default="table",
    help="è¼¸å‡ºæ ¼å¼"
)
@click.pass_context
def show_index(ctx: click.Context, input_path: Path, format: str):
    """é¡¯ç¤ºç´¢å¼•è³‡è¨Šå’Œçµ±è¨ˆã€‚"""
    try:
        from ..indexing import IndexAnalyzer
        
        # å»ºç«‹ç´¢å¼•åˆ†æå™¨
        analyzer = IndexAnalyzer(input_path)
        
        # ç²å–ç´¢å¼•è³‡è¨Š
        index_info = analyzer.get_index_info()
        
        if format == "table":
            # åŸºæœ¬çµ±è¨ˆè¡¨
            table = Table(title="ç´¢å¼•çµ±è¨ˆ")
            table.add_column("é …ç›®", style="cyan")
            table.add_column("æ•¸é‡", style="green")
            
            for key, value in index_info['statistics'].items():
                table.add_row(key, str(value))
            
            console.print(table)
            
        elif format == "json":
            import json
            console.print(json.dumps(index_info, indent=2, ensure_ascii=False))
            
        elif format == "detailed":
            # è©³ç´°è³‡è¨Š
            console.print(f"[bold]ç´¢å¼•ç›®éŒ„:[/bold] {input_path}")
            console.print(f"[bold]å»ºç«‹æ™‚é–“:[/bold] {index_info['created_at']}")
            console.print(f"[bold]æœ€å¾Œæ›´æ–°:[/bold] {index_info['updated_at']}")
            
            # çµ±è¨ˆè¡¨
            table = Table(title="è©³ç´°çµ±è¨ˆ")
            table.add_column("é¡åˆ¥", style="cyan")
            table.add_column("é …ç›®", style="yellow")
            table.add_column("æ•¸é‡", style="green")
            
            for category, items in index_info['detailed_stats'].items():
                for item, count in items.items():
                    table.add_row(category, item, str(count))
            
            console.print(table)
            
    except Exception as e:
        logger.error(f"é¡¯ç¤ºç´¢å¼•è³‡è¨Šå¤±æ•—: {e}")
        if not ctx.obj['quiet']:
            console.print(f"[red]é¡¯ç¤ºç´¢å¼•è³‡è¨Šå¤±æ•—: {e}[/red]")
        sys.exit(1)

# å°‡ç´¢å¼•å‘½ä»¤å°å‡ºåˆ° main.py ä½¿ç”¨
__all__ = ['index', 'show_index']
