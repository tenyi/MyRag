"""
æ•ˆèƒ½å„ªåŒ–ä½¿ç”¨ç¯„ä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Chinese GraphRAG ç³»çµ±çš„æ•ˆèƒ½å„ªåŒ–åŠŸèƒ½
"""

import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict, Any

from src.chinese_graphrag.performance import (
    OptimizerManager,
    OptimizationConfig,
    BatchOptimizer,
    QueryOptimizer,
    CostOptimizer,
    PerformanceMonitor
)


async def simulate_embedding_task(texts: List[str]) -> List[List[float]]:
    """æ¨¡æ“¬æ–‡æœ¬åµŒå…¥ä»»å‹™"""
    embeddings = []
    for text in texts:
        # æ¨¡æ“¬åµŒå…¥è¨ˆç®—æ™‚é–“
        await asyncio.sleep(0.01)
        # ç”Ÿæˆå‡çš„åµŒå…¥å‘é‡
        embedding = [hash(text + str(i)) % 1000 / 1000.0 for i in range(384)]
        embeddings.append(embedding)
    return embeddings


async def simulate_query_task(query: str) -> Dict[str, Any]:
    """æ¨¡æ“¬æŸ¥è©¢ä»»å‹™"""
    # æ¨¡æ“¬æŸ¥è©¢è™•ç†æ™‚é–“
    await asyncio.sleep(0.1)
    
    return {
        "query": query,
        "results": [
            {"text": f"çµæœ 1 for {query}", "score": 0.95},
            {"text": f"çµæœ 2 for {query}", "score": 0.87},
            {"text": f"çµæœ 3 for {query}", "score": 0.82}
        ],
        "processing_time": 0.1
    }


async def simulate_llm_inference(prompt: str, model: str = "gpt-3.5-turbo") -> Dict[str, Any]:
    """æ¨¡æ“¬ LLM æ¨ç†ä»»å‹™"""
    # æ¨¡æ“¬ä¸åŒæ¨¡å‹çš„è™•ç†æ™‚é–“
    processing_times = {
        "gpt-3.5-turbo": 0.5,
        "gpt-4": 1.0,
        "claude-3-sonnet": 0.8
    }
    
    await asyncio.sleep(processing_times.get(model, 0.5))
    
    return {
        "model": model,
        "prompt": prompt,
        "response": f"é€™æ˜¯ä¾†è‡ª {model} çš„å›æ‡‰ï¼š{prompt[:50]}...",
        "input_tokens": len(prompt.split()),
        "output_tokens": 50,
        "processing_time": processing_times.get(model, 0.5)
    }


class PerformanceOptimizationDemo:
    """æ•ˆèƒ½å„ªåŒ–ç¤ºç¯„"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¤ºç¯„"""
        # å»ºç«‹å„ªåŒ–é…ç½®
        self.config = OptimizationConfig(
            # æ‰¹æ¬¡è™•ç†è¨­å®š
            batch_enabled=True,
            batch_size=32,
            max_batch_size=128,
            parallel_workers=4,
            memory_threshold_mb=1024.0,
            
            # æŸ¥è©¢å¿«å–è¨­å®š
            query_cache_enabled=True,
            cache_ttl_seconds=1800,  # 30åˆ†é˜
            cache_max_size=5000,
            preload_enabled=True,
            
            # æˆæœ¬è¿½è¹¤è¨­å®š
            cost_tracking_enabled=True,
            budget_limit_usd=50.0,
            quality_threshold=0.85,
            
            # æ•ˆèƒ½ç›£æ§è¨­å®š
            monitoring_enabled=True,
            monitoring_interval=2.0,
            alert_thresholds={
                "cpu_usage": 75.0,
                "memory_usage": 80.0,
                "error_rate": 3.0
            },
            
            storage_path="logs/performance_demo"
        )
        
        self.optimizer_manager = None
    
    async def initialize(self):
        """åˆå§‹åŒ–å„ªåŒ–ç®¡ç†å™¨"""
        print("ğŸš€ åˆå§‹åŒ–æ•ˆèƒ½å„ªåŒ–ç®¡ç†å™¨...")
        self.optimizer_manager = OptimizerManager(self.config)
        await self.optimizer_manager.initialize()
        await self.optimizer_manager.start()
        print("âœ… æ•ˆèƒ½å„ªåŒ–ç®¡ç†å™¨å·²å•Ÿå‹•")
    
    async def demo_batch_optimization(self):
        """ç¤ºç¯„æ‰¹æ¬¡è™•ç†å„ªåŒ–"""
        print("\nğŸ“¦ æ‰¹æ¬¡è™•ç†å„ªåŒ–ç¤ºç¯„")
        print("=" * 50)
        
        # æº–å‚™æ¸¬è©¦è³‡æ–™
        test_texts = [f"é€™æ˜¯æ¸¬è©¦æ–‡æœ¬ {i}ï¼Œç”¨æ–¼æ¼”ç¤ºæ‰¹æ¬¡è™•ç†å„ªåŒ–åŠŸèƒ½ã€‚" for i in range(100)]
        
        # ä¸ä½¿ç”¨å„ªåŒ–çš„è™•ç†
        print("â±ï¸  åŸ·è¡Œæœªå„ªåŒ–çš„è™•ç†...")
        start_time = time.time()
        unoptimized_results = []
        for text in test_texts:
            result = await simulate_embedding_task([text])
            unoptimized_results.extend(result)
        unoptimized_time = time.time() - start_time
        
        # ä½¿ç”¨æ‰¹æ¬¡å„ªåŒ–çš„è™•ç†
        print("âš¡ åŸ·è¡Œæ‰¹æ¬¡å„ªåŒ–è™•ç†...")
        start_time = time.time()
        optimized_results = await self.optimizer_manager.optimize_batch_processing(
            items=test_texts,
            process_func=lambda texts: simulate_embedding_task([texts])
        )
        optimized_time = time.time() - start_time
        
        # æ¯”è¼ƒçµæœ
        speedup = unoptimized_time / optimized_time if optimized_time > 0 else 0
        print(f"ğŸ“Š æ•ˆèƒ½æ¯”è¼ƒ:")
        print(f"   æœªå„ªåŒ–è™•ç†æ™‚é–“: {unoptimized_time:.2f}s")
        print(f"   æ‰¹æ¬¡å„ªåŒ–æ™‚é–“: {optimized_time:.2f}s")
        print(f"   æ•ˆèƒ½æå‡: {speedup:.2f}x")
        print(f"   è™•ç†é …ç›®æ•¸é‡: {len(test_texts)}")
    
    async def demo_query_optimization(self):
        """ç¤ºç¯„æŸ¥è©¢å„ªåŒ–"""
        print("\nğŸ” æŸ¥è©¢å„ªåŒ–ç¤ºç¯„")
        print("=" * 50)
        
        test_queries = [
            "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
            "æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µ",
            "æ·±åº¦å­¸ç¿’èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’çš„å·®ç•°",
            "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",  # é‡è¤‡æŸ¥è©¢ï¼Œæ‡‰è©²å‘½ä¸­å¿«å–
            "è‡ªç„¶èªè¨€è™•ç†çš„æ‡‰ç”¨é ˜åŸŸ"
        ]
        
        total_time = 0
        cache_hits = 0
        
        for i, query in enumerate(test_queries, 1):
            print(f"ğŸ” åŸ·è¡ŒæŸ¥è©¢ {i}: {query}")
            
            start_time = time.time()
            result = await self.optimizer_manager.optimize_query(
                query=query,
                query_func=simulate_query_task
            )
            query_time = time.time() - start_time
            total_time += query_time
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå¿«å–å‘½ä¸­ï¼ˆå¿«é€ŸæŸ¥è©¢é€šå¸¸è¡¨ç¤ºå¿«å–å‘½ä¸­ï¼‰
            if query_time < 0.05:
                cache_hits += 1
                print(f"   âš¡ å¿«å–å‘½ä¸­! æŸ¥è©¢æ™‚é–“: {query_time:.3f}s")
            else:
                print(f"   ğŸ”„ åŸ·è¡ŒæŸ¥è©¢ï¼ŒæŸ¥è©¢æ™‚é–“: {query_time:.3f}s")
            
            print(f"   ğŸ“ çµæœæ•¸é‡: {len(result['results'])}")
        
        print(f"\nğŸ“Š æŸ¥è©¢çµ±è¨ˆ:")
        print(f"   ç¸½æŸ¥è©¢æ•¸: {len(test_queries)}")
        print(f"   å¿«å–å‘½ä¸­æ•¸: {cache_hits}")
        print(f"   å¿«å–å‘½ä¸­ç‡: {cache_hits/len(test_queries)*100:.1f}%")
        print(f"   ç¸½æŸ¥è©¢æ™‚é–“: {total_time:.2f}s")
        print(f"   å¹³å‡æŸ¥è©¢æ™‚é–“: {total_time/len(test_queries):.3f}s")
    
    async def demo_cost_optimization(self):
        """ç¤ºç¯„æˆæœ¬å„ªåŒ–"""
        print("\nğŸ’° æˆæœ¬å„ªåŒ–ç¤ºç¯„")
        print("=" * 50)
        
        test_prompts = [
            "è«‹ç¸½çµä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å…§å®¹...",
            "åˆ†æé€™æ®µæ–‡å­—çš„æƒ…æ„Ÿå‚¾å‘...",
            "å°‡ä»¥ä¸‹å…§å®¹ç¿»è­¯æˆè‹±æ–‡...",
            "å›ç­”é—œæ–¼æ©Ÿå™¨å­¸ç¿’çš„å•é¡Œ...",
            "ç”Ÿæˆä¸€å€‹å‰µæ„æ•…äº‹..."
        ]
        
        models = ["gpt-3.5-turbo", "gpt-4", "claude-3-sonnet"]
        
        for prompt in test_prompts:
            print(f"\nğŸ“ è™•ç†æç¤º: {prompt[:30]}...")
            
            # å–å¾—æ¨¡å‹å»ºè­°
            recommendation = await self.optimizer_manager.optimize_model_usage(
                model_name="auto",
                input_tokens=len(prompt.split()),
                operation_type="text_generation"
            )
            
            recommended_model = recommendation.get("recommended_model", "gpt-3.5-turbo")
            print(f"   ğŸ¯ å»ºè­°æ¨¡å‹: {recommended_model}")
            
            # åŸ·è¡Œæ¨ç†
            result = await simulate_llm_inference(prompt, recommended_model)
            
            # è¨˜éŒ„å¯¦éš›ä½¿ç”¨æƒ…æ³
            if self.optimizer_manager.cost_optimizer:
                await self.optimizer_manager.cost_optimizer.track_usage(
                    model_name=recommended_model,
                    input_tokens=result["input_tokens"],
                    output_tokens=result["output_tokens"],
                    operation_type="text_generation"
                )
            
            print(f"   â±ï¸  è™•ç†æ™‚é–“: {result['processing_time']:.2f}s")
            print(f"   ğŸ“Š Token ä½¿ç”¨: {result['input_tokens']} è¼¸å…¥ + {result['output_tokens']} è¼¸å‡º")
        
        # é¡¯ç¤ºæˆæœ¬çµ±è¨ˆ
        if self.optimizer_manager.cost_optimizer:
            cost_stats = self.optimizer_manager.cost_optimizer.get_usage_stats(60)
            print(f"\nğŸ’³ æˆæœ¬çµ±è¨ˆ:")
            print(f"   ç¸½æˆæœ¬: ${cost_stats.get('total_cost', 0):.4f}")
            print(f"   æ¨¡å‹ä½¿ç”¨æ¬¡æ•¸: {len(cost_stats.get('model_usage', {}))}")
            
            for model, usage in cost_stats.get('model_usage', {}).items():
                print(f"   {model}: {usage.get('count', 0)} æ¬¡ä½¿ç”¨")
    
    async def demo_performance_monitoring(self):
        """ç¤ºç¯„æ•ˆèƒ½ç›£æ§"""
        print("\nğŸ“ˆ æ•ˆèƒ½ç›£æ§ç¤ºç¯„")
        print("=" * 50)
        
        # åŸ·è¡Œä¸€äº›å·¥ä½œè² è¼‰ä»¥ç”¢ç”Ÿç›£æ§è³‡æ–™
        print("ğŸ”„ åŸ·è¡Œå·¥ä½œè² è¼‰ä»¥ç”¢ç”Ÿç›£æ§è³‡æ–™...")
        
        # æ¨¡æ“¬é«˜è² è¼‰å·¥ä½œ
        tasks = []
        for i in range(20):
            task = asyncio.create_task(simulate_llm_inference(f"æ¸¬è©¦æç¤º {i}", "gpt-3.5-turbo"))
            tasks.append(task)
        
        await asyncio.gather(*tasks)
        
        # ç­‰å¾…ç›£æ§è³‡æ–™æ”¶é›†
        await asyncio.sleep(3)
        
        # å–å¾—ç•¶å‰æ•ˆèƒ½æŒ‡æ¨™
        current_metrics = self.optimizer_manager.performance_monitor.get_current_metrics()
        if current_metrics:
            print(f"ğŸ“Š ç•¶å‰æ•ˆèƒ½æŒ‡æ¨™:")
            print(f"   CPU ä½¿ç”¨ç‡: {current_metrics.cpu_usage:.1f}%")
            print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {current_metrics.memory_usage:.1f}MB")
            print(f"   ç£ç¢Ÿä½¿ç”¨ç‡: {current_metrics.disk_usage:.1f}%")
            
            if current_metrics.gpu_usage > 0:
                print(f"   GPU ä½¿ç”¨ç‡: {current_metrics.gpu_usage:.1f}%")
                print(f"   GPU è¨˜æ†¶é«”: {current_metrics.gpu_memory_usage:.1f}MB")
        
        # å–å¾—æ•ˆèƒ½çµ±è¨ˆ
        performance_stats = self.optimizer_manager.performance_monitor.get_performance_stats(5)
        if performance_stats:
            print(f"\nğŸ“ˆ æ•ˆèƒ½çµ±è¨ˆ (éå»5åˆ†é˜):")
            system_stats = performance_stats.get("system", {})
            
            if "cpu" in system_stats:
                cpu_stats = system_stats["cpu"]
                print(f"   CPU - å¹³å‡: {cpu_stats.get('avg', 0):.1f}%, "
                      f"æœ€å¤§: {cpu_stats.get('max', 0):.1f}%, "
                      f"P95: {cpu_stats.get('p95', 0):.1f}%")
            
            if "memory" in system_stats:
                memory_stats = system_stats["memory"]
                print(f"   è¨˜æ†¶é«” - å¹³å‡: {memory_stats.get('avg_mb', 0):.1f}MB, "
                      f"æœ€å¤§: {memory_stats.get('max_mb', 0):.1f}MB, "
                      f"P95: {memory_stats.get('p95_mb', 0):.1f}MB")
            
            trends = performance_stats.get("trends", {})
            if trends:
                print(f"   è¶¨å‹¢ - CPU: {trends.get('cpu_trend', 'unknown')}, "
                      f"è¨˜æ†¶é«”: {trends.get('memory_trend', 'unknown')}, "
                      f"æ•´é«”ç‹€æ…‹: {trends.get('overall_status', 'unknown')}")
    
    async def demo_benchmark_testing(self):
        """ç¤ºç¯„åŸºæº–æ¸¬è©¦"""
        print("\nğŸ åŸºæº–æ¸¬è©¦ç¤ºç¯„")
        print("=" * 50)
        
        # å®šç¾©ä¸åŒçš„æ¸¬è©¦å ´æ™¯
        test_configs = [
            {
                "name": "å¿«é€ŸåµŒå…¥",
                "func": lambda: simulate_embedding_task(["çŸ­æ–‡æœ¬"]),
                "params": {}
            },
            {
                "name": "æ‰¹é‡åµŒå…¥",
                "func": lambda: simulate_embedding_task(["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", "æ–‡æœ¬4", "æ–‡æœ¬5"]),
                "params": {}
            },
            {
                "name": "ç°¡å–®æŸ¥è©¢",
                "func": lambda: simulate_query_task("ç°¡å–®å•é¡Œ"),
                "params": {}
            },
            {
                "name": "è¤‡é›œæŸ¥è©¢",
                "func": lambda: simulate_query_task("é€™æ˜¯ä¸€å€‹æ›´è¤‡é›œçš„æŸ¥è©¢ï¼Œéœ€è¦æ›´å¤šè™•ç†æ™‚é–“å’Œè³‡æº"),
                "params": {}
            }
        ]
        
        print("ğŸš€ åŸ·è¡ŒåŸºæº–æ¸¬è©¦...")
        results = await self.optimizer_manager.run_performance_benchmark(
            test_configs=test_configs,
            iterations=10
        )
        
        print(f"\nğŸ† åŸºæº–æ¸¬è©¦çµæœ:")
        for test_name, result in results.items():
            print(f"\nğŸ“‹ {test_name}:")
            print(f"   ååé‡: {result.throughput:.2f} ops/s")
            print(f"   å¹³å‡å»¶é²: {result.latency_ms:.2f}ms")
            print(f"   æˆåŠŸç‡: {result.success_rate:.1%}")
            print(f"   è¨˜æ†¶é«”å³°å€¼: {result.memory_peak_mb:.1f}MB")
            
            if result.latency_percentiles:
                print(f"   å»¶é²åˆ†ä½ˆ - P50: {result.latency_percentiles.get('p50', 0):.2f}ms, "
                      f"P95: {result.latency_percentiles.get('p95', 0):.2f}ms, "
                      f"P99: {result.latency_percentiles.get('p99', 0):.2f}ms")
    
    async def generate_optimization_report(self):
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆå„ªåŒ–å ±å‘Š")
        print("=" * 50)
        
        report = self.optimizer_manager.get_optimization_report(duration_minutes=10)
        
        print(f"ğŸ“Š å„ªåŒ–å ±å‘Š (éå» {report['period_minutes']} åˆ†é˜):")
        print(f"   ç”Ÿæˆæ™‚é–“: {report['timestamp']}")
        
        summary = report.get("summary", {})
        print(f"\nğŸ“ˆ å„ªåŒ–çµ±è¨ˆ:")
        print(f"   æ‰¹æ¬¡å„ªåŒ–æ¬¡æ•¸: {summary.get('batch_optimizations', 0)}")
        print(f"   å¿«å–å‘½ä¸­æ¬¡æ•¸: {summary.get('cache_hits', 0)}")
        print(f"   å¿«å–æœªå‘½ä¸­æ¬¡æ•¸: {summary.get('cache_misses', 0)}")
        print(f"   æˆæœ¬ç¯€çœ: ${summary.get('cost_savings', 0):.4f}")
        
        # å„²å­˜è©³ç´°å ±å‘Š
        report_path = Path("logs/performance_demo/optimization_report.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è©³ç´°å ±å‘Šå·²å„²å­˜è‡³: {report_path}")
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.optimizer_manager:
            await self.optimizer_manager.stop()
        print("ğŸ§¹ è³‡æºæ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ Chinese GraphRAG æ•ˆèƒ½å„ªåŒ–ç¤ºç¯„")
    print("=" * 60)
    
    demo = PerformanceOptimizationDemo()
    
    try:
        # åˆå§‹åŒ–
        await demo.initialize()
        
        # åŸ·è¡Œå„ç¨®ç¤ºç¯„
        await demo.demo_batch_optimization()
        await demo.demo_query_optimization()
        await demo.demo_cost_optimization()
        await demo.demo_performance_monitoring()
        await demo.demo_benchmark_testing()
        
        # ç”Ÿæˆå ±å‘Š
        await demo.generate_optimization_report()
        
        print("\nğŸ‰ æ‰€æœ‰ç¤ºç¯„å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ ç¤ºç¯„åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†è³‡æº
        await demo.cleanup()


if __name__ == "__main__":
    # åŸ·è¡Œç¤ºç¯„
    asyncio.run(main())