#!/usr/bin/env python3
"""
Embedding æ•ˆèƒ½å„ªåŒ–åŠŸèƒ½ç¤ºç¯„

å±•ç¤ºå¿«å–ã€GPU åŠ é€Ÿã€è¨˜æ†¶é«”å„ªåŒ–å’Œä½¿ç”¨é‡ç›£æ§åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•
"""

import asyncio
import time
from pathlib import Path

from chinese_graphrag.embeddings import (
    # ç®¡ç†å™¨
    EmbeddingManager,
    
    # å¿«å–
    create_embedding_cache,
    
    # GPU åŠ é€Ÿå’Œè¨˜æ†¶é«”å„ªåŒ–
    get_device_manager,
    get_memory_optimizer,
    create_batch_processor,
    
    # ç›£æ§
    get_usage_monitor,
    record_embedding_usage,
    
    # æœå‹™
    BGEM3EmbeddingService,
    ChineseOptimizedEmbeddingService
)


async def demo_basic_usage():
    """åŸºæœ¬ä½¿ç”¨ç¤ºç¯„"""
    print("=== åŸºæœ¬ä½¿ç”¨ç¤ºç¯„ ===")
    
    # å»ºç«‹å¸¶æœ‰æ‰€æœ‰å„ªåŒ–åŠŸèƒ½çš„ EmbeddingManager
    manager = EmbeddingManager(
        enable_cache=True,
        cache_config={
            "memory_cache_mb": 100,
            "disk_cache_mb": 500,
            "cache_dir": "./cache/demo"
        },
        enable_gpu_acceleration=True,
        enable_monitoring=True
    )
    
    # è¨»å†Šä¸€å€‹æ¨¡æ“¬çš„ embedding æœå‹™
    try:
        # å˜—è©¦ä½¿ç”¨ BGE-M3 æœå‹™
        service = BGEM3EmbeddingService(
            model_name="BAAI/bge-m3",
            device="cpu",  # ä½¿ç”¨ CPU ä»¥ç¢ºä¿ç›¸å®¹æ€§
            max_sequence_length=512
        )
        
        # è¨»å†Šæœå‹™
        manager.register_service("bge_m3", service, set_as_default=True)
        
        print("âœ“ æˆåŠŸè¨»å†Š BGE-M3 æœå‹™")
        
    except Exception as e:
        print(f"âœ— ç„¡æ³•è¼‰å…¥ BGE-M3 æœå‹™: {e}")
        print("ä½¿ç”¨æ¨¡æ“¬æœå‹™é€²è¡Œç¤ºç¯„...")
        
        # å»ºç«‹æ¨¡æ“¬æœå‹™
        from unittest.mock import Mock
        import numpy as np
        
        mock_service = Mock()
        mock_service.model_name = "mock_bge_m3"
        mock_service.model_type = "mock"
        mock_service.is_loaded = True
        mock_service.device = "cpu"
        
        async def mock_embed_texts(texts, normalize=True, show_progress=False):
            await asyncio.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
            embeddings = np.random.rand(len(texts), 768).astype(np.float32)
            from chinese_graphrag.embeddings.base import EmbeddingResult
            return EmbeddingResult(
                embeddings=embeddings,
                texts=texts,
                model_name="mock_bge_m3",
                dimensions=768,
                processing_time=0.1
            )
        
        mock_service.embed_texts = mock_embed_texts
        manager.register_service("mock_bge_m3", mock_service, set_as_default=True)
        print("âœ“ ä½¿ç”¨æ¨¡æ“¬æœå‹™")
    
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ï¼Œç”¨æ–¼å±•ç¤º embedding åŠŸèƒ½ã€‚",
        "ä¸­æ–‡ GraphRAG ç³»çµ±æ”¯æ´å¤šç¨® embedding æ¨¡å‹ã€‚",
        "å¿«å–æ©Ÿåˆ¶å¯ä»¥é¡¯è‘—æå‡é‡è¤‡æŸ¥è©¢çš„æ•ˆèƒ½ã€‚",
        "GPU åŠ é€Ÿèƒ½å¤ è™•ç†å¤§æ‰¹é‡çš„æ–‡æœ¬å‘é‡åŒ–ä»»å‹™ã€‚",
        "ä½¿ç”¨é‡ç›£æ§å¹«åŠ©è¿½è¹¤ç³»çµ±æ•ˆèƒ½å’Œæˆæœ¬ã€‚"
    ]
    
    print(f"\nè™•ç† {len(test_texts)} å€‹æ¸¬è©¦æ–‡æœ¬...")
    
    # ç¬¬ä¸€æ¬¡èª¿ç”¨ï¼ˆæ‡‰è©²æœƒå¿«å–çµæœï¼‰
    start_time = time.time()
    result1 = await manager.embed_texts(test_texts)
    first_call_time = time.time() - start_time
    
    print(f"âœ“ ç¬¬ä¸€æ¬¡èª¿ç”¨å®Œæˆï¼Œè€—æ™‚: {first_call_time:.3f}s")
    print(f"  - å‘é‡ç¶­åº¦: {result1.dimensions}")
    print(f"  - è™•ç†æ–‡æœ¬æ•¸: {len(result1.texts)}")
    
    # ç¬¬äºŒæ¬¡èª¿ç”¨ï¼ˆæ‡‰è©²å‘½ä¸­å¿«å–ï¼‰
    start_time = time.time()
    result2 = await manager.embed_texts(test_texts)
    second_call_time = time.time() - start_time
    
    print(f"âœ“ ç¬¬äºŒæ¬¡èª¿ç”¨å®Œæˆï¼Œè€—æ™‚: {second_call_time:.3f}s")
    
    # æª¢æŸ¥å¿«å–æ•ˆæœ
    if second_call_time < first_call_time * 0.5:
        print("âœ“ å¿«å–ç”Ÿæ•ˆï¼Œç¬¬äºŒæ¬¡èª¿ç”¨æ˜é¡¯æ›´å¿«")
    else:
        print("? å¿«å–å¯èƒ½æœªç”Ÿæ•ˆæˆ–ä½¿ç”¨æ¨¡æ“¬æœå‹™")
    
    return manager


async def demo_cache_functionality(manager):
    """å¿«å–åŠŸèƒ½ç¤ºç¯„"""
    print("\n=== å¿«å–åŠŸèƒ½ç¤ºç¯„ ===")
    
    # å–å¾—å¿«å–çµ±è¨ˆ
    cache_stats = await manager.get_cache_stats()
    if cache_stats:
        print("å¿«å–çµ±è¨ˆè³‡è¨Š:")
        print(f"  - å¿«å–é¡å‹: {cache_stats.get('cache_type', 'unknown')}")
        print(f"  - å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.1%}")
        print(f"  - ç¸½è«‹æ±‚æ•¸: {cache_stats.get('total_requests', 0)}")
        
        if 'memory_cache' in cache_stats:
            mem_stats = cache_stats['memory_cache']
            print(f"  - è¨˜æ†¶é«”å¿«å–: {mem_stats.get('entry_count', 0)} å€‹æ¢ç›®")
            print(f"  - è¨˜æ†¶é«”ä½¿ç”¨: {mem_stats.get('current_size_mb', 0):.1f}MB")
        
        if 'disk_cache' in cache_stats:
            disk_stats = cache_stats['disk_cache']
            print(f"  - ç£ç¢Ÿå¿«å–: {disk_stats.get('entry_count', 0)} å€‹æ¢ç›®")
            print(f"  - ç£ç¢Ÿä½¿ç”¨: {disk_stats.get('current_size_mb', 0):.1f}MB")
    else:
        print("å¿«å–æœªå•Ÿç”¨æˆ–ç„¡çµ±è¨ˆè³‡è¨Š")
    
    # å¿«å–é ç†±ç¤ºç¯„
    preload_texts = [
        "é è¼‰å…¥æ–‡æœ¬ 1ï¼šäººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¿…é€Ÿã€‚",
        "é è¼‰å…¥æ–‡æœ¬ 2ï¼šæ©Ÿå™¨å­¸ç¿’æ¨¡å‹éœ€è¦å¤§é‡è³‡æ–™ã€‚",
        "é è¼‰å…¥æ–‡æœ¬ 3ï¼šè‡ªç„¶èªè¨€è™•ç†æ˜¯ AI çš„é‡è¦åˆ†æ”¯ã€‚"
    ]
    
    print(f"\nåŸ·è¡Œå¿«å–é ç†±ï¼Œé è¼‰å…¥ {len(preload_texts)} å€‹æ–‡æœ¬...")
    preload_result = await manager.preload_cache(preload_texts, batch_size=2)
    
    if "error" not in preload_result:
        print("âœ“ å¿«å–é ç†±å®Œæˆ")
        print(f"  - è™•ç†æ‰¹æ¬¡æ•¸: {preload_result.get('processed_batches', 0)}")
        print(f"  - å¿«å–æ¢ç›®æ•¸: {preload_result.get('cached_entries', 0)}")
        print(f"  - è€—æ™‚: {preload_result.get('duration_seconds', 0):.2f}s")
    else:
        print(f"âœ— å¿«å–é ç†±å¤±æ•—: {preload_result['error']}")


def demo_device_management():
    """è£ç½®ç®¡ç†ç¤ºç¯„"""
    print("\n=== è£ç½®ç®¡ç†ç¤ºç¯„ ===")
    
    device_manager = get_device_manager()
    
    # é¡¯ç¤ºå¯ç”¨è£ç½®
    print("å¯ç”¨è£ç½®:")
    for device in device_manager.available_devices:
        print(f"  - {device}")
    
    # å–å¾—æœ€ä½³è£ç½®
    optimal_device = device_manager.get_optimal_device(
        memory_required_mb=100,
        prefer_gpu=True
    )
    print(f"\næ¨è–¦è£ç½®: {optimal_device}")
    
    # è£ç½®è©³ç´°è³‡è¨Š
    device_info = device_manager.get_device_info(optimal_device)
    print(f"è£ç½®è³‡è¨Š:")
    print(f"  - é¡å‹: {device_info.get('type', 'unknown')}")
    
    if 'memory_info' in device_info:
        mem_info = device_info['memory_info']
        print(f"  - ç¸½è¨˜æ†¶é«”: {mem_info.get('total_mb', 0):.0f}MB")
        print(f"  - å¯ç”¨è¨˜æ†¶é«”: {mem_info.get('available_mb', 0):.0f}MB")
        print(f"  - ä½¿ç”¨ç‡: {mem_info.get('usage_percent', 0):.1f}%")
    
    # æ‰€æœ‰è£ç½®çµ±è¨ˆ
    all_stats = device_manager.get_all_device_stats()
    print(f"\nç³»çµ±ç¸½è¦½:")
    print(f"  - å¯ç”¨è£ç½®æ•¸: {len(all_stats['available_devices'])}")
    print(f"  - GPU æ•¸é‡: {all_stats['gpu_count']}")


async def demo_memory_optimization():
    """è¨˜æ†¶é«”å„ªåŒ–ç¤ºç¯„"""
    print("\n=== è¨˜æ†¶é«”å„ªåŒ–ç¤ºç¯„ ===")
    
    memory_optimizer = get_memory_optimizer()
    
    # å–å¾—è¨˜æ†¶é«”çµ±è¨ˆ
    stats = memory_optimizer.get_memory_stats()
    print("è¨˜æ†¶é«”çµ±è¨ˆ:")
    print(f"  - ç³»çµ±ç¸½è¨˜æ†¶é«”: {stats.system_total:.0f}MB")
    print(f"  - ç³»çµ±å·²ç”¨è¨˜æ†¶é«”: {stats.system_used:.0f}MB")
    print(f"  - ç³»çµ±å¯ç”¨è¨˜æ†¶é«”: {stats.system_available:.0f}MB")
    print(f"  - ç¨‹åºè¨˜æ†¶é«”ä½¿ç”¨: {stats.process_used:.0f}MB")
    print(f"  - ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‡: {stats.system_usage_ratio:.1%}")
    
    if stats.gpu_total > 0:
        print(f"  - GPU ç¸½è¨˜æ†¶é«”: {stats.gpu_total:.0f}MB")
        print(f"  - GPU å·²ç”¨è¨˜æ†¶é«”: {stats.gpu_used:.0f}MB")
        print(f"  - GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡: {stats.gpu_usage_ratio:.1%}")
    
    # åŸ·è¡Œè¨˜æ†¶é«”æ¸…ç†
    print("\nåŸ·è¡Œè¨˜æ†¶é«”æ¸…ç†...")
    cleanup_result = await memory_optimizer.cleanup_memory()
    
    print("æ¸…ç†çµæœ:")
    print(f"  - æ¸…ç†è€—æ™‚: {cleanup_result['cleanup_time']:.3f}s")
    print(f"  - åŸ·è¡Œçš„æ¸…ç†å‹•ä½œ: {len(cleanup_result['actions_taken'])}")
    
    for action in cleanup_result['actions_taken']:
        print(f"    * {action}")
    
    if 'memory_freed' in cleanup_result:
        freed = cleanup_result['memory_freed']
        total_freed = freed['system_mb'] + freed['gpu_mb']
        if total_freed > 0:
            print(f"  - é‡‹æ”¾è¨˜æ†¶é«”: {total_freed:.1f}MB")


def demo_usage_monitoring(manager):
    """ä½¿ç”¨é‡ç›£æ§ç¤ºç¯„"""
    print("\n=== ä½¿ç”¨é‡ç›£æ§ç¤ºç¯„ ===")
    
    # å–å¾—ä½¿ç”¨é‡çµ±è¨ˆ
    usage_stats = manager.get_usage_stats(time_range_hours=1)
    
    if usage_stats:
        summary = usage_stats['summary']
        print("ä½¿ç”¨é‡çµ±è¨ˆ (æœ€è¿‘ 1 å°æ™‚):")
        print(f"  - ç¸½è«‹æ±‚æ•¸: {summary['total_requests']}")
        print(f"  - æˆåŠŸè«‹æ±‚æ•¸: {summary['successful_requests']}")
        print(f"  - å¤±æ•—è«‹æ±‚æ•¸: {summary['failed_requests']}")
        print(f"  - æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"  - ç¸½è™•ç†æ™‚é–“: {summary['total_processing_time']:.2f}s")
        print(f"  - å¹³å‡è™•ç†æ™‚é–“: {summary['average_processing_time']:.3f}s")
        print(f"  - å¹³å‡ååé‡: {summary['average_throughput']:.1f} é …ç›®/ç§’")
        
        # æ¨¡å‹åˆ†è§£
        if usage_stats['model_breakdown']:
            print("\næ¨¡å‹ä½¿ç”¨åˆ†è§£:")
            for model_name, model_stats in usage_stats['model_breakdown'].items():
                print(f"  - {model_name}:")
                print(f"    * è«‹æ±‚æ•¸: {model_stats['requests']}")
                print(f"    * æˆåŠŸæ•¸: {model_stats['successful']}")
                print(f"    * è™•ç†æ™‚é–“: {model_stats['processing_time']:.2f}s")
        
        # è£ç½®åˆ†è§£
        if usage_stats['device_breakdown']:
            print("\nè£ç½®ä½¿ç”¨åˆ†è§£:")
            for device, count in usage_stats['device_breakdown'].items():
                print(f"  - {device}: {count} æ¬¡")
    else:
        print("ç„¡ä½¿ç”¨é‡çµ±è¨ˆè³‡è¨Š")
    
    # æª¢æŸ¥è­¦å ±
    alerts = manager.get_usage_alerts(unresolved_only=True, limit=5)
    
    if alerts:
        print(f"\næœªè§£æ±ºè­¦å ± ({len(alerts)} å€‹):")
        for alert in alerts:
            print(f"  - [{alert['level'].upper()}] {alert['model_name']}: {alert['message']}")
    else:
        print("\nâœ“ ç„¡æœªè§£æ±ºè­¦å ±")


async def demo_comprehensive_stats(manager):
    """ç¶œåˆçµ±è¨ˆç¤ºç¯„"""
    print("\n=== ç¶œåˆçµ±è¨ˆç¤ºç¯„ ===")
    
    stats = manager.get_comprehensive_stats()
    
    # ç®¡ç†å™¨è³‡è¨Š
    manager_info = stats['manager_info']
    print("ç®¡ç†å™¨è³‡è¨Š:")
    print(f"  - ç¸½æœå‹™æ•¸: {manager_info['total_services']}")
    print(f"  - å·²è¼‰å…¥æœå‹™æ•¸: {manager_info['loaded_services']}")
    print(f"  - é è¨­æœå‹™: {manager_info['default_service']}")
    print(f"  - å•Ÿç”¨å¿«å–: {manager_info['enable_cache']}")
    print(f"  - å•Ÿç”¨ GPU åŠ é€Ÿ: {manager_info['enable_gpu_acceleration']}")
    print(f"  - å•Ÿç”¨ç›£æ§: {manager_info['enable_monitoring']}")
    
    # æœå‹™çµ±è¨ˆ
    if 'services' in stats:
        print("\næœå‹™çµ±è¨ˆ:")
        for service_name, service_stats in stats['services'].items():
            print(f"  - {service_name}:")
            print(f"    * æ¨¡å‹: {service_stats['model_name']}")
            print(f"    * ç¸½è«‹æ±‚æ•¸: {service_stats['total_requests']}")
            print(f"    * å¹³å‡è™•ç†æ™‚é–“: {service_stats['average_processing_time']:.3f}s")
            print(f"    * æˆåŠŸç‡: {service_stats['success_rate']:.1%}")
            print(f"    * è¨˜æ†¶é«”ä½¿ç”¨: {service_stats['memory_usage_mb']:.1f}MB")


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ Embedding æ•ˆèƒ½å„ªåŒ–åŠŸèƒ½ç¤ºç¯„")
    print("=" * 50)
    
    try:
        # åŸºæœ¬ä½¿ç”¨ç¤ºç¯„
        manager = await demo_basic_usage()
        
        # å¿«å–åŠŸèƒ½ç¤ºç¯„
        await demo_cache_functionality(manager)
        
        # è£ç½®ç®¡ç†ç¤ºç¯„
        demo_device_management()
        
        # è¨˜æ†¶é«”å„ªåŒ–ç¤ºç¯„
        await demo_memory_optimization()
        
        # ä½¿ç”¨é‡ç›£æ§ç¤ºç¯„
        demo_usage_monitoring(manager)
        
        # ç¶œåˆçµ±è¨ˆç¤ºç¯„
        await demo_comprehensive_stats(manager)
        
        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰ç¤ºç¯„å®Œæˆï¼")
        
        # æ¸…ç†è³‡æº
        if manager.cache:
            print("\nğŸ§¹ æ¸…ç†å¿«å–...")
            await manager.clear_cache()
        
        if manager.memory_optimizer:
            print("ğŸ§¹ æœ€çµ‚è¨˜æ†¶é«”æ¸…ç†...")
            await manager.cleanup_memory()
        
    except Exception as e:
        print(f"\nâŒ ç¤ºç¯„éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # åŸ·è¡Œç¤ºç¯„
    asyncio.run(main())