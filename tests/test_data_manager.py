"""
æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨

è² è²¬ç®¡ç†æ¸¬è©¦éç¨‹ä¸­ä½¿ç”¨çš„å„ç¨®è³‡æ–™ï¼ŒåŒ…æ‹¬æ¸¬è©¦æ–‡ä»¶ã€å‘é‡è³‡æ–™ã€é…ç½®æª”æ¡ˆç­‰ã€‚
æä¾›è³‡æ–™çš„å»ºç«‹ã€æ¸…ç†ã€é‡è¨­å’Œç‰ˆæœ¬ç®¡ç†åŠŸèƒ½ã€‚
"""

import hashlib
import json
import shutil
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import yaml
from dataclasses import dataclass, asdict


@dataclass
class TestDataInfo:
    """æ¸¬è©¦è³‡æ–™è³‡è¨Š"""
    name: str
    description: str
    data_type: str  # 'document', 'vector', 'config', 'fixture'
    file_path: str
    size_bytes: int
    created_at: str
    checksum: str
    tags: List[str]
    metadata: Dict[str, Any]


class TestDataManager:
    """æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: Union[str, Path] = None):
        """
        åˆå§‹åŒ–æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨
        
        Args:
            base_dir: æ¸¬è©¦è³‡æ–™åŸºç¤ç›®éŒ„ï¼Œé è¨­ç‚º ./test_data
        """
        self.base_dir = Path(base_dir) if base_dir else Path("test_data")
        self.base_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹å­ç›®éŒ„çµæ§‹
        self.documents_dir = self.base_dir / "documents"
        self.vectors_dir = self.base_dir / "vectors"
        self.configs_dir = self.base_dir / "configs"
        self.fixtures_dir = self.base_dir / "fixtures"
        self.temp_dir = self.base_dir / "temp"
        
        for dir_path in [self.documents_dir, self.vectors_dir, self.configs_dir, 
                        self.fixtures_dir, self.temp_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # è³‡æ–™è¨»å†Šè¡¨æª”æ¡ˆ
        self.registry_file = self.base_dir / "data_registry.json"
        self.registry = self._load_registry()
    
    def _load_registry(self) -> Dict[str, TestDataInfo]:
        """è¼‰å…¥è³‡æ–™è¨»å†Šè¡¨"""
        if self.registry_file.exists():
            try:
                with open(self.registry_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {
                        name: TestDataInfo(**info) 
                        for name, info in data.items()
                    }
            except Exception as e:
                print(f"è¼‰å…¥è³‡æ–™è¨»å†Šè¡¨å¤±æ•—: {e}")
        
        return {}
    
    def _save_registry(self):
        """å„²å­˜è³‡æ–™è¨»å†Šè¡¨"""
        registry_data = {
            name: asdict(info) 
            for name, info in self.registry.items()
        }
        
        with open(self.registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry_data, f, ensure_ascii=False, indent=2)
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆæ ¡é©—å’Œ"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()
    
    def register_data(
        self,
        name: str,
        file_path: Union[str, Path],
        data_type: str,
        description: str = "",
        tags: List[str] = None,
        metadata: Dict[str, Any] = None
    ) -> TestDataInfo:
        """
        è¨»å†Šæ¸¬è©¦è³‡æ–™
        
        Args:
            name: è³‡æ–™åç¨±
            file_path: æª”æ¡ˆè·¯å¾‘
            data_type: è³‡æ–™é¡å‹
            description: æè¿°
            tags: æ¨™ç±¤åˆ—è¡¨
            metadata: å…ƒè³‡æ–™
            
        Returns:
            TestDataInfo: è³‡æ–™è³‡è¨Š
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        
        data_info = TestDataInfo(
            name=name,
            description=description,
            data_type=data_type,
            file_path=str(file_path),
            size_bytes=file_path.stat().st_size,
            created_at=datetime.now().isoformat(),
            checksum=self._calculate_checksum(file_path),
            tags=tags or [],
            metadata=metadata or {}
        )
        
        self.registry[name] = data_info
        self._save_registry()
        
        return data_info
    
    def get_data_info(self, name: str) -> Optional[TestDataInfo]:
        """ç²å–è³‡æ–™è³‡è¨Š"""
        return self.registry.get(name)
    
    def list_data(
        self,
        data_type: str = None,
        tags: List[str] = None
    ) -> List[TestDataInfo]:
        """
        åˆ—å‡ºæ¸¬è©¦è³‡æ–™
        
        Args:
            data_type: ç¯©é¸è³‡æ–™é¡å‹
            tags: ç¯©é¸æ¨™ç±¤
            
        Returns:
            List[TestDataInfo]: è³‡æ–™è³‡è¨Šåˆ—è¡¨
        """
        results = []
        
        for data_info in self.registry.values():
            # é¡å‹ç¯©é¸
            if data_type and data_info.data_type != data_type:
                continue
            
            # æ¨™ç±¤ç¯©é¸
            if tags and not any(tag in data_info.tags for tag in tags):
                continue
            
            results.append(data_info)
        
        return results
    
    def create_sample_documents(self) -> Dict[str, Path]:
        """å»ºç«‹ç¯„ä¾‹æ–‡ä»¶é›†åˆ"""
        documents = {}
        
        # ä¸­æ–‡ AI æŠ€è¡“æ–‡ä»¶
        ai_doc = self.documents_dir / "ai_technology.md"
        ai_content = """# äººå·¥æ™ºæ…§æŠ€è¡“æ¦‚è¿°

äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligenceï¼Œç°¡ç¨±AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼ç ”ç©¶ã€é–‹ç™¼ç”¨æ–¼æ¨¡æ“¬ã€å»¶ä¼¸å’Œæ“´å±•äººçš„æ™ºæ…§çš„ç†è«–ã€æ–¹æ³•ã€æŠ€è¡“åŠæ‡‰ç”¨ç³»çµ±ã€‚

## ä¸»è¦æŠ€è¡“é ˜åŸŸ

### æ©Ÿå™¨å­¸ç¿’ (Machine Learning)
æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„æ ¸å¿ƒçµ„æˆéƒ¨åˆ†ï¼Œè®“é›»è…¦èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’è¦å¾‹å’Œæ¨¡å¼ï¼Œè€Œç„¡éœ€é€²è¡Œæ˜ç¢ºç¨‹å¼è¨­è¨ˆã€‚

#### ç›£ç£å­¸ç¿’
- åˆ†é¡å•é¡Œï¼šå¦‚åœ–åƒè­˜åˆ¥ã€åƒåœ¾éƒµä»¶æª¢æ¸¬
- è¿´æ­¸å•é¡Œï¼šå¦‚åƒ¹æ ¼é æ¸¬ã€é¢¨éšªè©•ä¼°

#### ç„¡ç›£ç£å­¸ç¿’
- èšé¡åˆ†æï¼šå®¢æˆ¶åˆ†ç¾¤ã€å¸‚å ´ç´°åˆ†
- é™ç¶­æŠ€è¡“ï¼šè³‡æ–™è¦–è¦ºåŒ–ã€ç‰¹å¾µæå–

#### å¼·åŒ–å­¸ç¿’
- éŠæˆ²AIï¼šå¦‚åœæ£‹ã€è¥¿æ´‹æ£‹
- è‡ªå‹•æ§åˆ¶ï¼šæ©Ÿå™¨äººå°èˆªã€äº¤é€šç®¡ç†

### æ·±åº¦å­¸ç¿’ (Deep Learning)
æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é›†ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„å­¸ç¿’éç¨‹ã€‚

#### å·ç©ç¥ç¶“ç¶²è·¯ (CNN)
- åœ–åƒè­˜åˆ¥å’Œé›»è…¦è¦–è¦º
- é†«å­¸å½±åƒåˆ†æ
- è‡ªå‹•é§•é§›è¦–è¦ºç³»çµ±

#### å¾ªç’°ç¥ç¶“ç¶²è·¯ (RNN)
- è‡ªç„¶èªè¨€è™•ç†
- èªéŸ³è­˜åˆ¥å’Œåˆæˆ
- æ™‚é–“åºåˆ—é æ¸¬

#### Transformer æ¶æ§‹
- æ©Ÿå™¨ç¿»è­¯
- æ–‡æœ¬ç”Ÿæˆå’Œç†è§£
- å¤šæ¨¡æ…‹å­¸ç¿’

## æ‡‰ç”¨é ˜åŸŸ

### é†«ç™‚å¥åº·
- ç–¾ç—…è¨ºæ–·è¼”åŠ©
- è—¥ç‰©ç ”ç™¼åŠ é€Ÿ
- å€‹äººåŒ–é†«ç™‚æ–¹æ¡ˆ

### é‡‘èæœå‹™
- é¢¨éšªè©•ä¼°å’Œç®¡ç†
- æ¼”ç®—æ³•äº¤æ˜“
- åæ¬ºè©æª¢æ¸¬

### æ•™è‚²é ˜åŸŸ
- å€‹äººåŒ–å­¸ç¿’æ¨è–¦
- æ™ºæ…§æ•™å­¸åŠ©æ‰‹
- å­¸ç¿’æ•ˆæœè©•ä¼°

### è£½é€ æ¥­
- å“è³ªæ§åˆ¶è‡ªå‹•åŒ–
- é æ¸¬æ€§ç¶­è­·
- ä¾›æ‡‰éˆå„ªåŒ–

## æŠ€è¡“æŒ‘æˆ°

### è³‡æ–™å“è³ª
- è³‡æ–™åè¦‹å’Œä¸å¹³è¡¡
- éš±ç§ä¿è­·å’Œå®‰å…¨
- è³‡æ–™æ¨™è¨»æˆæœ¬

### æ¨¡å‹å¯è§£é‡‹æ€§
- é»‘ç®±å•é¡Œ
- æ±ºç­–é€æ˜åº¦
- è²¬ä»»æ­¸å±¬

### å€«ç†è€ƒé‡
- æ¼”ç®—æ³•å…¬å¹³æ€§
- å°±æ¥­å½±éŸ¿
- ç¤¾æœƒè²¬ä»»

## æœªä¾†ç™¼å±•è¶¨å‹¢

1. **AGI (Artificial General Intelligence)**ï¼šé€šç”¨äººå·¥æ™ºæ…§çš„ç™¼å±•
2. **é‚Šç·£è¨ˆç®—**ï¼šå°‡AIèƒ½åŠ›éƒ¨ç½²åˆ°é‚Šç·£è¨­å‚™
3. **é‡å­æ©Ÿå™¨å­¸ç¿’**ï¼šçµåˆé‡å­è¨ˆç®—å’Œæ©Ÿå™¨å­¸ç¿’
4. **å¯æŒçºŒAI**ï¼šæ¸›å°‘AIç³»çµ±çš„èƒ½æºæ¶ˆè€—
5. **äººæ©Ÿå”ä½œ**ï¼šå¢å¼ºäººé¡è€Œéå–ä»£äººé¡

AIæŠ€è¡“çš„ç™¼å±•å°‡æŒçºŒæ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ï¼Œå‰µé€ æ›´å¤šå¯èƒ½æ€§ã€‚
"""
        
        with open(ai_doc, 'w', encoding='utf-8') as f:
            f.write(ai_content)
        
        documents['ai_technology'] = ai_doc
        self.register_data(
            'ai_technology',
            ai_doc,
            'document',
            'äººå·¥æ™ºæ…§æŠ€è¡“æ¦‚è¿°æ–‡ä»¶',
            ['ai', 'technology', 'chinese'],
            {'language': 'zh-TW', 'word_count': len(ai_content)}
        )
        
        # æ©Ÿå™¨å­¸ç¿’è©³ç´°æ–‡ä»¶
        ml_doc = self.documents_dir / "machine_learning_guide.txt"
        ml_content = """æ©Ÿå™¨å­¸ç¿’å®Œæ•´æŒ‡å—

æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹å­é ˜åŸŸï¼Œå°ˆæ³¨æ–¼é–‹ç™¼èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’ä¸¦åšå‡ºé æ¸¬æˆ–æ±ºç­–çš„æ¼”ç®—æ³•ã€‚

å­¸ç¿’é¡å‹åˆ†é¡ï¼š

1. ç›£ç£å­¸ç¿’ (Supervised Learning)
ç›£ç£å­¸ç¿’ä½¿ç”¨æ¨™è¨˜çš„è¨“ç·´è³‡æ–™ä¾†å­¸ç¿’å¾è¼¸å…¥åˆ°è¼¸å‡ºçš„æ˜ å°„é—œä¿‚ã€‚

å¸¸è¦‹æ¼”ç®—æ³•ï¼š
- ç·šæ€§è¿´æ­¸ï¼šç”¨æ–¼é€£çºŒæ•¸å€¼é æ¸¬
- é‚è¼¯è¿´æ­¸ï¼šç”¨æ–¼äºŒå…ƒåˆ†é¡å•é¡Œ
- æ±ºç­–æ¨¹ï¼šæ˜“æ–¼ç†è§£å’Œè§£é‡‹çš„åˆ†é¡æ–¹æ³•
- éš¨æ©Ÿæ£®æ—ï¼šå¤šå€‹æ±ºç­–æ¨¹çš„é›†æˆæ–¹æ³•
- æ”¯æ´å‘é‡æ©Ÿ (SVM)ï¼šå°‹æ‰¾æœ€ä½³åˆ†é›¢è¶…å¹³é¢
- ç¥ç¶“ç¶²è·¯ï¼šæ¨¡æ“¬äººè…¦ç¥ç¶“å…ƒçš„é€£æ¥æ–¹å¼

æ‡‰ç”¨å¯¦ä¾‹ï¼š
- é›»å­éƒµä»¶åƒåœ¾ä¿¡ä»¶åˆ†é¡
- æˆ¿åƒ¹é æ¸¬
- é†«ç™‚è¨ºæ–·è¼”åŠ©
- å®¢æˆ¶ä¿¡ç”¨è©•ä¼°

2. ç„¡ç›£ç£å­¸ç¿’ (Unsupervised Learning)
ç„¡ç›£ç£å­¸ç¿’è™•ç†æ²’æœ‰æ¨™ç±¤çš„è³‡æ–™ï¼Œç›®æ¨™æ˜¯ç™¼ç¾è³‡æ–™ä¸­çš„éš±è—çµæ§‹ã€‚

ä¸»è¦æ–¹æ³•ï¼š
- K-means èšé¡ï¼šå°‡è³‡æ–™åˆ†æˆ k å€‹ç¾¤çµ„
- éšå±¤èšé¡ï¼šå»ºç«‹è³‡æ–™çš„æ¨¹ç‹€çµæ§‹
- ä¸»æˆåˆ†åˆ†æ (PCA)ï¼šé™ç¶­å’Œç‰¹å¾µæå–
- ç¨ç«‹æˆåˆ†åˆ†æ (ICA)ï¼šä¿¡è™Ÿåˆ†é›¢
- é—œè¯è¦å‰‡å­¸ç¿’ï¼šç™¼ç¾é …ç›®é–“çš„é—œè¯æ€§

æ‡‰ç”¨å ´æ™¯ï¼š
- å®¢æˆ¶åˆ†ç¾¤å’Œå¸‚å ´ç´°åˆ†
- ç•°å¸¸æª¢æ¸¬
- æ¨è–¦ç³»çµ±
- è³‡æ–™å£“ç¸®

3. å¼·åŒ–å­¸ç¿’ (Reinforcement Learning)
å¼·åŒ–å­¸ç¿’é€éèˆ‡ç’°å¢ƒäº’å‹•ï¼Œå­¸ç¿’å¦‚ä½•é¸æ“‡å‹•ä½œä»¥æœ€å¤§åŒ–ç´¯ç©çå‹µã€‚

æ ¸å¿ƒæ¦‚å¿µï¼š
- æ™ºæ…§é«” (Agent)ï¼šå­¸ç¿’å’Œæ±ºç­–çš„å¯¦é«”
- ç’°å¢ƒ (Environment)ï¼šæ™ºæ…§é«”æ“ä½œçš„å ´æ™¯
- ç‹€æ…‹ (State)ï¼šç’°å¢ƒçš„ç•¶å‰æƒ…æ³
- å‹•ä½œ (Action)ï¼šæ™ºæ…§é«”å¯åŸ·è¡Œçš„æ“ä½œ
- çå‹µ (Reward)ï¼šå‹•ä½œçš„å³æ™‚å›é¥‹

ç¶“å…¸æ¼”ç®—æ³•ï¼š
- Q-Learningï¼šå­¸ç¿’ç‹€æ…‹-å‹•ä½œåƒ¹å€¼å‡½æ•¸
- SARSAï¼šåŒç­–ç•¥æ™‚é–“å·®åˆ†å­¸ç¿’
- Actor-Criticï¼šçµåˆåƒ¹å€¼å‡½æ•¸å’Œç­–ç•¥æ¢¯åº¦
- Deep Q-Network (DQN)ï¼šæ·±åº¦å­¸ç¿’ç‰ˆæœ¬çš„Q-Learning

æˆåŠŸæ‡‰ç”¨ï¼š
- éŠæˆ²AI (AlphaGo, OpenAI Five)
- è‡ªå‹•é§•é§›
- æ©Ÿå™¨äººæ§åˆ¶
- è³‡æºåˆ†é…æœ€ä½³åŒ–

æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡ï¼š

è©•ä¼°æŒ‡æ¨™ï¼š
- åˆ†é¡å•é¡Œï¼šæº–ç¢ºç‡ã€ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸
- è¿´æ­¸å•é¡Œï¼šå‡æ–¹èª¤å·®ã€å¹³å‡çµ•å°èª¤å·®ã€æ±ºå®šä¿‚æ•¸
- èšé¡å•é¡Œï¼šè¼ªå»“ä¿‚æ•¸ã€èª¿æ•´è˜­å¾·æŒ‡æ•¸

äº¤å‰é©—è­‰ï¼š
- k-foldäº¤å‰é©—è­‰ï¼šå°‡è³‡æ–™åˆ†æˆkä»½é€²è¡Œé©—è­‰
- ç•™ä¸€æ³•ï¼šæ¯æ¬¡ç•™ä¸€å€‹æ¨£æœ¬åšé©—è­‰
- æ™‚é–“åºåˆ—åˆ†å‰²ï¼šæŒ‰æ™‚é–“é †åºåˆ†å‰²è³‡æ–™

éæ“¬åˆå’Œæ¬ æ“¬åˆï¼š
- éæ“¬åˆï¼šæ¨¡å‹éæ–¼è¤‡é›œï¼Œè¨˜ä½äº†è¨“ç·´è³‡æ–™çš„é›œè¨Š
- æ¬ æ“¬åˆï¼šæ¨¡å‹éæ–¼ç°¡å–®ï¼Œç„¡æ³•æ•æ‰è³‡æ–™çš„æ¨¡å¼
- æ­£å‰‡åŒ–ï¼šL1ã€L2æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ
- æ—©åœæ³•ï¼šç›£æ§é©—è­‰é›†æ€§èƒ½ï¼Œé©æ™‚åœæ­¢è¨“ç·´

ç‰¹å¾µå·¥ç¨‹ï¼š
- ç‰¹å¾µé¸æ“‡ï¼šé¸æ“‡æœ€ç›¸é—œçš„ç‰¹å¾µ
- ç‰¹å¾µæå–ï¼šå¾åŸå§‹è³‡æ–™ä¸­æå–æœ‰ç”¨è³‡è¨Š
- ç‰¹å¾µç¸®æ”¾ï¼šæ¨™æº–åŒ–ã€æ­£è¦åŒ–è™•ç†
- ç‰¹å¾µçµ„åˆï¼šå‰µå»ºæ–°çš„çµ„åˆç‰¹å¾µ

å¯¦éš›æ‡‰ç”¨è€ƒé‡ï¼š
- è³‡æ–™å“è³ªï¼šå®Œæ•´æ€§ã€æº–ç¢ºæ€§ã€ä¸€è‡´æ€§
- è¨ˆç®—è³‡æºï¼šè¨“ç·´æ™‚é–“ã€è¨˜æ†¶é«”éœ€æ±‚
- æ¨¡å‹è§£é‡‹æ€§ï¼šæ¥­å‹™éœ€æ±‚vsæ¨¡å‹è¤‡é›œåº¦
- éƒ¨ç½²å’Œç¶­è­·ï¼šæ¨¡å‹æ›´æ–°ã€ç›£æ§ã€ç‰ˆæœ¬æ§åˆ¶

æ©Ÿå™¨å­¸ç¿’æ˜¯ä¸€å€‹å¿«é€Ÿç™¼å±•çš„é ˜åŸŸï¼Œæ–°çš„æ¼”ç®—æ³•å’ŒæŠ€è¡“ä¸æ–·æ¶Œç¾ï¼Œéœ€è¦æŒçºŒå­¸ç¿’å’Œå¯¦è¸ã€‚
"""
        
        with open(ml_doc, 'w', encoding='utf-8') as f:
            f.write(ml_content)
        
        documents['machine_learning'] = ml_doc
        self.register_data(
            'machine_learning',
            ml_doc,
            'document',
            'æ©Ÿå™¨å­¸ç¿’å®Œæ•´æŒ‡å—',
            ['ml', 'guide', 'chinese'],
            {'language': 'zh-TW', 'word_count': len(ml_content)}
        )
        
        # æ·±åº¦å­¸ç¿’æŠ€è¡“æ–‡ä»¶
        dl_doc = self.documents_dir / "deep_learning.md"
        dl_content = """# æ·±åº¦å­¸ç¿’æŠ€è¡“è©³è§£

æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é›†ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¤‡é›œè¡¨ç¤ºã€‚

## ç¥ç¶“ç¶²è·¯åŸºç¤

### æ„ŸçŸ¥æ©Ÿ (Perceptron)
æœ€ç°¡å–®çš„ç¥ç¶“ç¶²è·¯å–®å…ƒï¼Œæ¥æ”¶å¤šå€‹è¼¸å…¥ä¸¦ç”¢ç”Ÿä¸€å€‹è¼¸å‡ºã€‚

### å¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP)
åŒ…å«è¼¸å…¥å±¤ã€éš±è—å±¤å’Œè¼¸å‡ºå±¤çš„å‰é¥‹ç¥ç¶“ç¶²è·¯ã€‚

### æ¿€æ´»å‡½æ•¸
- Sigmoidï¼šå°‡è¼¸å‡ºå£“ç¸®åˆ° (0,1) å€é–“
- Tanhï¼šå°‡è¼¸å‡ºå£“ç¸®åˆ° (-1,1) å€é–“  
- ReLUï¼šä¿®æ­£ç·šæ€§å–®å…ƒï¼Œè§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œ
- Leaky ReLUï¼šå…è¨±è² å€¼æœ‰å°å¹…åº¦è¼¸å‡º
- Swishï¼šè‡ªé–€æ§æ¿€æ´»å‡½æ•¸

## é‡è¦æ¶æ§‹

### å·ç©ç¥ç¶“ç¶²è·¯ (CNN)
å°ˆé–€è™•ç†ç¶²æ ¼ç‹€è³‡æ–™ï¼ˆå¦‚åœ–åƒï¼‰çš„ç¥ç¶“ç¶²è·¯æ¶æ§‹ã€‚

#### æ ¸å¿ƒçµ„ä»¶
- å·ç©å±¤ï¼šä½¿ç”¨æ¿¾æ³¢å™¨æå–å±€éƒ¨ç‰¹å¾µ
- æ± åŒ–å±¤ï¼šé™ä½è³‡æ–™ç¶­åº¦ï¼Œä¿ç•™é‡è¦è³‡è¨Š
- å…¨é€£æ¥å±¤ï¼šé€²è¡Œæœ€çµ‚çš„åˆ†é¡æˆ–è¿´æ­¸

#### ç¶“å…¸æ¶æ§‹
- LeNetï¼šæœ€æ—©çš„CNNæ¶æ§‹
- AlexNetï¼šæ·±åº¦å­¸ç¿’å¾©èˆˆçš„é‡Œç¨‹ç¢‘
- VGGï¼šä½¿ç”¨å°æ¿¾æ³¢å™¨çš„æ·±åº¦ç¶²è·¯
- ResNetï¼šå¼•å…¥æ®˜å·®é€£æ¥è§£æ±ºæ¢¯åº¦æ¶ˆå¤±
- DenseNetï¼šå¯†é›†é€£æ¥çš„ç‰¹å¾µé‡ç”¨

### å¾ªç’°ç¥ç¶“ç¶²è·¯ (RNN)
è™•ç†åºåˆ—è³‡æ–™çš„ç¥ç¶“ç¶²è·¯ï¼Œå…·æœ‰è¨˜æ†¶èƒ½åŠ›ã€‚

#### RNNè®Šé«”
- Vanilla RNNï¼šåŸºç¤å¾ªç’°ç¥ç¶“ç¶²è·¯
- LSTMï¼šé•·çŸ­æœŸè¨˜æ†¶ç¶²è·¯ï¼Œè§£æ±ºé•·ç¨‹ä¾è³´å•é¡Œ
- GRUï¼šé–€æ§å¾ªç’°å–®å…ƒï¼ŒLSTMçš„ç°¡åŒ–ç‰ˆæœ¬
- Bidirectional RNNï¼šé›™å‘è™•ç†åºåˆ—è³‡è¨Š

#### æ‡‰ç”¨é ˜åŸŸ
- è‡ªç„¶èªè¨€è™•ç†
- èªéŸ³è­˜åˆ¥
- æ™‚é–“åºåˆ—é æ¸¬
- æ©Ÿå™¨ç¿»è­¯

### Transformer æ¶æ§‹
åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶çš„é©å‘½æ€§æ¶æ§‹ï¼Œä¸ä¾è³´å¾ªç’°æˆ–å·ç©æ“ä½œã€‚

#### æ ¸å¿ƒæ©Ÿåˆ¶
- è‡ªæ³¨æ„åŠ› (Self-Attention)ï¼šè¨ˆç®—åºåˆ—å…§éƒ¨çš„é—œè¯æ€§
- å¤šé ­æ³¨æ„åŠ›ï¼šä¸¦è¡Œè¨ˆç®—å¤šå€‹æ³¨æ„åŠ›è¡¨ç¤º
- ä½ç½®ç·¨ç¢¼ï¼šç‚ºåºåˆ—æ·»åŠ ä½ç½®è³‡è¨Š
- å‰é¥‹ç¶²è·¯ï¼šé»å°é»çš„ç‰¹å¾µè®Šæ›

#### é‡è¦æ¨¡å‹
- BERTï¼šé›™å‘ç·¨ç¢¼å™¨è¡¨ç¤º
- GPTï¼šç”Ÿæˆå¼é è¨“ç·´Transformer
- T5ï¼šæ–‡æœ¬åˆ°æ–‡æœ¬è½‰æ›Transformer
- Vision Transformer (ViT)ï¼šè¦–è¦ºTransformer

## è¨“ç·´æŠ€å·§

### å„ªåŒ–æ¼”ç®—æ³•
- SGDï¼šéš¨æ©Ÿæ¢¯åº¦ä¸‹é™
- Momentumï¼šå‹•é‡æ³•åŠ é€Ÿæ”¶æ–‚
- AdaGradï¼šè‡ªé©æ‡‰æ¢¯åº¦æ¼”ç®—æ³•
- Adamï¼šçµåˆå‹•é‡å’Œè‡ªé©æ‡‰å­¸ç¿’ç‡
- AdamWï¼šæ¬Šé‡è¡°æ¸›ç‰ˆæœ¬çš„Adam

### æ­£å‰‡åŒ–æŠ€è¡“
- Dropoutï¼šéš¨æ©Ÿä¸Ÿæ£„éƒ¨åˆ†ç¥ç¶“å…ƒ
- Batch Normalizationï¼šæ‰¹æ¬¡æ­£è¦åŒ–ç©©å®šè¨“ç·´
- Layer Normalizationï¼šå±¤æ­£è¦åŒ–
- Weight Decayï¼šæ¬Šé‡è¡°æ¸›é˜²æ­¢éæ“¬åˆ
- Early Stoppingï¼šæ—©åœé¿å…éæ“¬åˆ

### å­¸ç¿’ç‡èª¿åº¦
- å›ºå®šå­¸ç¿’ç‡ï¼šæ•´å€‹è¨“ç·´éç¨‹ä¿æŒä¸è®Š
- éšæ¢¯è¡°æ¸›ï¼šæŒ‰éšæ®µé™ä½å­¸ç¿’ç‡
- æŒ‡æ•¸è¡°æ¸›ï¼šæŒ‡æ•¸å½¢å¼è¡°æ¸›
- é¤˜å¼¦é€€ç«ï¼šé¤˜å¼¦å‡½æ•¸èª¿ç¯€å­¸ç¿’ç‡
- å¾ªç’°å­¸ç¿’ç‡ï¼šåœ¨ç¯„åœå…§å¾ªç’°èª¿æ•´

## ç”Ÿæˆå¼æ¨¡å‹

### è®Šåˆ†è‡ªç·¨ç¢¼å™¨ (VAE)
å­¸ç¿’è³‡æ–™çš„æ½›åœ¨è¡¨ç¤ºï¼Œèƒ½å¤ ç”Ÿæˆæ–°çš„è³‡æ–™æ¨£æœ¬ã€‚

### ç”Ÿæˆå°æŠ—ç¶²è·¯ (GAN)
é€šéç”Ÿæˆå™¨å’Œåˆ¤åˆ¥å™¨çš„å°æŠ—è¨“ç·´ç”Ÿæˆé€¼çœŸè³‡æ–™ã€‚

#### GANè®Šé«”
- DCGANï¼šæ·±åº¦å·ç©GAN
- WGANï¼šWasserstein GANæ”¹å–„è¨“ç·´ç©©å®šæ€§
- StyleGANï¼šæ§åˆ¶ç”Ÿæˆåœ–åƒçš„é¢¨æ ¼
- CycleGANï¼šç„¡é…å°è³‡æ–™çš„åœ–åƒè½‰æ›

### æ“´æ•£æ¨¡å‹
é€éé€æ­¥å»å™ªéç¨‹ç”Ÿæˆé«˜å“è³ªåœ–åƒã€‚

## å¯¦éš›æ‡‰ç”¨

### é›»è…¦è¦–è¦º
- åœ–åƒåˆ†é¡å’Œç‰©é«”æª¢æ¸¬
- èªç¾©åˆ†å‰²å’Œä¾‹é …åˆ†å‰²
- äººè‡‰è­˜åˆ¥å’Œè¡¨æƒ…åˆ†æ
- é†«å­¸å½±åƒè¨ºæ–·

### è‡ªç„¶èªè¨€è™•ç†
- æ©Ÿå™¨ç¿»è­¯
- æ–‡æœ¬æ‘˜è¦
- å•ç­”ç³»çµ±
- å°è©±ç³»çµ±

### å¤šæ¨¡æ…‹å­¸ç¿’
- åœ–åƒæè¿°ç”Ÿæˆ
- è¦–è¦ºå•ç­”
- è¦–é »ç†è§£
- è·¨æ¨¡æ…‹æª¢ç´¢

## ç™¼å±•è¶¨å‹¢

### æ¨¡å‹æ•ˆç‡
- æ¨¡å‹å£“ç¸®å’Œå‰ªæ
- çŸ¥è­˜è’¸é¤¾
- é‡åŒ–æŠ€è¡“
- ç¥ç¶“æ¶æ§‹æœç´¢

### å¯è§£é‡‹æ€§
- æ³¨æ„åŠ›è¦–è¦ºåŒ–
- æ¢¯åº¦åˆ†æ
- æ¦‚å¿µæ¿€æ´»å‘é‡
- åäº‹å¯¦è§£é‡‹

### æŒçºŒå­¸ç¿’
- é¿å…ç½é›£æ€§éºå¿˜
- å…ƒå­¸ç¿’
- å°‘æ¨£æœ¬å­¸ç¿’
- é›¶æ¨£æœ¬å­¸ç¿’

æ·±åº¦å­¸ç¿’æŠ€è¡“æŒçºŒå¿«é€Ÿç™¼å±•ï¼Œåœ¨å„å€‹é ˜åŸŸéƒ½æœ‰é‡å¤§çªç ´å’Œæ‡‰ç”¨ã€‚
"""
        
        with open(dl_doc, 'w', encoding='utf-8') as f:
            f.write(dl_content)
        
        documents['deep_learning'] = dl_doc
        self.register_data(
            'deep_learning',  
            dl_doc,
            'document',
            'æ·±åº¦å­¸ç¿’æŠ€è¡“è©³è§£',
            ['dl', 'neural_networks', 'chinese'],
            {'language': 'zh-TW', 'word_count': len(dl_content)}
        )
        
        print(f"å·²å»ºç«‹ {len(documents)} å€‹ç¯„ä¾‹æ–‡ä»¶")
        return documents
    
    def create_test_configs(self) -> Dict[str, Path]:
        """å»ºç«‹æ¸¬è©¦é…ç½®æª”æ¡ˆ"""
        configs = {}
        
        # åŸºæœ¬æ¸¬è©¦é…ç½®
        basic_config = {
            'embedding': {
                'model': 'BAAI/bge-m3',
                'dimension': 768,
                'batch_size': 16,
                'device': 'cpu'
            },
            'vector_store': {
                'type': 'lancedb',
                'path': './test_vectors',
                'table_name': 'test_embeddings'
            },
            'llm': {
                'provider': 'openai',
                'model': 'gpt-3.5-turbo',
                'temperature': 0.7,
                'max_tokens': 1000
            },
            'indexing': {
                'chunk_size': 500,
                'chunk_overlap': 50,
                'min_chunk_size': 100
            },
            'query': {
                'top_k': 10,
                'similarity_threshold': 0.7,
                'max_context_length': 4000
            }
        }
        
        basic_config_file = self.configs_dir / "basic_test_config.yaml"
        with open(basic_config_file, 'w', encoding='utf-8') as f:
            yaml.dump(basic_config, f, allow_unicode=True, default_flow_style=False)
        
        configs['basic'] = basic_config_file
        self.register_data(
            'basic_test_config',
            basic_config_file,
            'config',
            'åŸºæœ¬æ¸¬è©¦é…ç½®æª”æ¡ˆ',
            ['config', 'test', 'basic']
        )
        
        # æ•ˆèƒ½æ¸¬è©¦é…ç½®
        performance_config = {
            'embedding': {
                'model': 'BAAI/bge-m3',
                'dimension': 768,
                'batch_size': 64,  # æ›´å¤§æ‰¹æ¬¡
                'device': 'cpu'
            },
            'vector_store': {
                'type': 'lancedb',
                'path': './perf_test_vectors',
                'table_name': 'perf_embeddings'
            },
            'indexing': {
                'chunk_size': 1000,  # æ›´å¤§åˆ†å¡Š
                'chunk_overlap': 100,
                'min_chunk_size': 200,
                'parallel_workers': 4
            },
            'performance': {
                'max_documents': 1000,
                'max_processing_time': 300,
                'memory_limit_mb': 2048,
                'benchmark_iterations': 10
            }
        }
        
        perf_config_file = self.configs_dir / "performance_test_config.yaml"
        with open(perf_config_file, 'w', encoding='utf-8') as f:
            yaml.dump(performance_config, f, allow_unicode=True, default_flow_style=False)
        
        configs['performance'] = perf_config_file
        self.register_data(
            'performance_test_config',
            perf_config_file,
            'config',
            'æ•ˆèƒ½æ¸¬è©¦é…ç½®æª”æ¡ˆ',
            ['config', 'test', 'performance']
        )
        
        # ä¸­æ–‡ç‰¹å®šé…ç½®
        chinese_config = {
            'text_processing': {
                'language': 'zh-TW',
                'segmentation': 'jieba',
                'stopwords_file': 'chinese_stopwords.txt',
                'min_word_length': 1,
                'max_word_length': 20
            },
            'embedding': {
                'model': 'BAAI/bge-m3',
                'dimension': 768,
                'batch_size': 32,
                'normalize': True,
                'chinese_optimization': True
            },
            'indexing': {
                'chunk_size': 300,  # ä¸­æ–‡å­—ç¬¦è¼ƒçŸ­
                'chunk_overlap': 30,
                'sentence_splitter': 'chinese_aware',
                'preserve_formatting': True
            },
            'query': {
                'chinese_query_expansion': True,
                'synonym_matching': True,
                'traditional_simplified_convert': True
            }
        }
        
        chinese_config_file = self.configs_dir / "chinese_test_config.yaml"
        with open(chinese_config_file, 'w', encoding='utf-8') as f:
            yaml.dump(chinese_config, f, allow_unicode=True, default_flow_style=False)
        
        configs['chinese'] = chinese_config_file
        self.register_data(
            'chinese_test_config',
            chinese_config_file,
            'config',
            'ä¸­æ–‡ç‰¹å®šæ¸¬è©¦é…ç½®æª”æ¡ˆ',
            ['config', 'test', 'chinese']
        )
        
        print(f"å·²å»ºç«‹ {len(configs)} å€‹æ¸¬è©¦é…ç½®æª”æ¡ˆ")
        return configs
    
    def create_test_fixtures(self) -> Dict[str, Path]:
        """å»ºç«‹æ¸¬è©¦å¤¾å…·è³‡æ–™"""
        fixtures = {}
        
        # æ¸¬è©¦æŸ¥è©¢é›†åˆ
        test_queries = {
            'definition_queries': [
                "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
                "æ©Ÿå™¨å­¸ç¿’çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
                "è«‹è§£é‡‹æ·±åº¦å­¸ç¿’çš„æ¦‚å¿µ",
                "ç¥ç¶“ç¶²è·¯æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"
            ],
            'comparison_queries': [
                "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æœ‰ä»€éº¼å€åˆ¥ï¼Ÿ",
                "CNNå’ŒRNNçš„å·®ç•°åœ¨å“ªè£¡ï¼Ÿ",
                "ç›£ç£å­¸ç¿’èˆ‡ç„¡ç›£ç£å­¸ç¿’çš„å°æ¯”",
                "Transformerå’ŒRNNçš„å„ªç¼ºé»æ¯”è¼ƒ"
            ],
            'application_queries': [
                "äººå·¥æ™ºæ…§åœ¨é†«ç™‚é ˜åŸŸçš„æ‡‰ç”¨",
                "æ·±åº¦å­¸ç¿’åœ¨è‡ªç„¶èªè¨€è™•ç†ä¸­çš„ä½¿ç”¨",
                "æ©Ÿå™¨å­¸ç¿’åœ¨é‡‘èæ¥­çš„å¯¦éš›æ¡ˆä¾‹",
                "é›»è…¦è¦–è¦ºæŠ€è¡“çš„å•†æ¥­æ‡‰ç”¨"
            ],
            'technical_queries': [
                "å¦‚ä½•é¸æ“‡åˆé©çš„æ¿€æ´»å‡½æ•¸ï¼Ÿ",
                "ä»€éº¼æ™‚å€™ä½¿ç”¨CNNè€Œä¸æ˜¯RNNï¼Ÿ",
                "å¦‚ä½•è§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼Ÿ",
                "Attentionæ©Ÿåˆ¶çš„å·¥ä½œåŸç†"
            ]
        }
        
        queries_file = self.fixtures_dir / "test_queries.json"
        with open(queries_file, 'w', encoding='utf-8') as f:
            json.dump(test_queries, f, ensure_ascii=False, indent=2)
        
        fixtures['queries'] = queries_file
        self.register_data(
            'test_queries',
            queries_file,
            'fixture',
            'æ¸¬è©¦æŸ¥è©¢é›†åˆ',
            ['queries', 'test', 'chinese']
        )
        
        # é æœŸç­”æ¡ˆç¯„æœ¬
        expected_answers = {
            'ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ': {
                'keywords': ['äººå·¥æ™ºæ…§', 'AI', 'é›»è…¦ç§‘å­¸', 'æ¨¡æ“¬æ™ºæ…§'],
                'min_length': 100,
                'should_mention': ['æ©Ÿå™¨å­¸ç¿’', 'æŠ€è¡“', 'æ‡‰ç”¨'],
                'confidence_threshold': 0.8
            },
            'æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æœ‰ä»€éº¼å€åˆ¥ï¼Ÿ': {
                'keywords': ['æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'ç¥ç¶“ç¶²è·¯', 'å·®åˆ¥'],
                'min_length': 150,
                'should_mention': ['å¤šå±¤', 'ç‰¹å¾µ', 'æ¼”ç®—æ³•'],
                'confidence_threshold': 0.85
            }
        }
        
        answers_file = self.fixtures_dir / "expected_answers.json"
        with open(answers_file, 'w', encoding='utf-8') as f:
            json.dump(expected_answers, f, ensure_ascii=False, indent=2)
        
        fixtures['answers'] = answers_file
        self.register_data(
            'expected_answers',
            answers_file,
            'fixture',
            'é æœŸç­”æ¡ˆç¯„æœ¬',
            ['answers', 'validation', 'test']
        )
        
        # æ•ˆèƒ½åŸºæº–è³‡æ–™
        performance_benchmarks = {
            'document_processing': {
                'max_time_per_document': 2.0,
                'max_memory_per_document_mb': 10.0,
                'min_throughput_docs_per_second': 5.0
            },
            'embedding_generation': {
                'max_time_per_batch': 5.0,
                'max_memory_per_batch_mb': 500.0,
                'min_throughput_texts_per_second': 20.0
            },
            'vector_search': {
                'max_search_time_ms': 100.0,
                'max_memory_per_search_mb': 50.0,
                'min_precision_at_k': 0.8
            },
            'end_to_end_query': {
                'max_response_time_seconds': 10.0,
                'max_memory_usage_mb': 1000.0,
                'min_answer_quality_score': 0.7
            }
        }
        
        benchmarks_file = self.fixtures_dir / "performance_benchmarks.json"
        with open(benchmarks_file, 'w', encoding='utf-8') as f:
            json.dump(performance_benchmarks, f, ensure_ascii=False, indent=2)
        
        fixtures['benchmarks'] = benchmarks_file
        self.register_data(
            'performance_benchmarks',
            benchmarks_file,
            'fixture',
            'æ•ˆèƒ½åŸºæº–è³‡æ–™',
            ['performance', 'benchmarks', 'test']
        )
        
        print(f"å·²å»ºç«‹ {len(fixtures)} å€‹æ¸¬è©¦å¤¾å…·æª”æ¡ˆ")
        return fixtures
    
    def create_temporary_workspace(self, prefix: str = "test_") -> Path:
        """å»ºç«‹è‡¨æ™‚å·¥ä½œç©ºé–“"""
        workspace = Path(tempfile.mkdtemp(prefix=prefix, dir=self.temp_dir))
        
        # å»ºç«‹æ¨™æº–å­ç›®éŒ„
        (workspace / "documents").mkdir()
        (workspace / "data").mkdir()
        (workspace / "output").mkdir()
        (workspace / "logs").mkdir()
        
        return workspace
    
    def cleanup_temporary_data(self, max_age_hours: int = 24):
        """æ¸…ç†è‡¨æ™‚è³‡æ–™"""
        import time
        current_time = time.time()
        
        cleaned_count = 0
        for item in self.temp_dir.iterdir():
            if item.is_dir():
                # æª¢æŸ¥ç›®éŒ„ä¿®æ”¹æ™‚é–“
                mod_time = item.stat().st_mtime
                age_hours = (current_time - mod_time) / 3600
                
                if age_hours > max_age_hours:
                    shutil.rmtree(item)
                    cleaned_count += 1
        
        print(f"å·²æ¸…ç† {cleaned_count} å€‹éæœŸçš„è‡¨æ™‚ç›®éŒ„")
        return cleaned_count
    
    def validate_data_integrity(self) -> Dict[str, bool]:
        """é©—è­‰è³‡æ–™å®Œæ•´æ€§"""
        results = {}
        
        for name, data_info in self.registry.items():
            file_path = Path(data_info.file_path)
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                results[name] = False
                continue
            
            # æª¢æŸ¥æª”æ¡ˆå¤§å°
            current_size = file_path.stat().st_size
            if current_size != data_info.size_bytes:
                results[name] = False
                continue
            
            # æª¢æŸ¥æ ¡é©—å’Œ
            current_checksum = self._calculate_checksum(file_path)
            if current_checksum != data_info.checksum:
                results[name] = False
                continue
            
            results[name] = True
        
        return results
    
    def export_data_manifest(self, output_file: str = "data_manifest.json") -> Path:
        """åŒ¯å‡ºè³‡æ–™æ¸…å–®"""
        manifest = {
            'generated_at': datetime.now().isoformat(),
            'total_data_count': len(self.registry),
            'data_by_type': {},
            'data_registry': {name: asdict(info) for name, info in self.registry.items()}
        }
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        for data_info in self.registry.values():
            data_type = data_info.data_type
            if data_type not in manifest['data_by_type']:
                manifest['data_by_type'][data_type] = 0
            manifest['data_by_type'][data_type] += 1
        
        manifest_file = self.base_dir / output_file
        with open(manifest_file, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        
        print(f"è³‡æ–™æ¸…å–®å·²åŒ¯å‡ºè‡³: {manifest_file}")
        return manifest_file
    
    def reset_all_data(self, confirm: bool = False):
        """é‡è¨­æ‰€æœ‰æ¸¬è©¦è³‡æ–™"""
        if not confirm:
            print("è­¦å‘Šï¼šæ­¤æ“ä½œå°‡åˆªé™¤æ‰€æœ‰æ¸¬è©¦è³‡æ–™ï¼")
            print("è«‹ä½¿ç”¨ reset_all_data(confirm=True) ç¢ºèªåŸ·è¡Œ")
            return
        
        # æ¸…ç©ºè¨»å†Šè¡¨
        self.registry.clear()
        self._save_registry()
        
        # åˆªé™¤æ‰€æœ‰è³‡æ–™æª”æ¡ˆ
        for dir_path in [self.documents_dir, self.vectors_dir, self.configs_dir, 
                        self.fixtures_dir, self.temp_dir]:
            if dir_path.exists():
                shutil.rmtree(dir_path)
                dir_path.mkdir()
        
        print("æ‰€æœ‰æ¸¬è©¦è³‡æ–™å·²é‡è¨­")


def main():
    """æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨ç¤ºä¾‹ç”¨æ³•"""
    # å»ºç«‹æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨
    manager = TestDataManager()
    
    print("ğŸ—‚ï¸ åˆå§‹åŒ–æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨")
    print(f"è³‡æ–™ç›®éŒ„: {manager.base_dir}")
    
    # å»ºç«‹ç¯„ä¾‹è³‡æ–™
    print("\nğŸ“„ å»ºç«‹ç¯„ä¾‹æ–‡ä»¶...")
    documents = manager.create_sample_documents()
    
    print("\nâš™ï¸ å»ºç«‹æ¸¬è©¦é…ç½®...")
    configs = manager.create_test_configs()
    
    print("\nğŸ§ª å»ºç«‹æ¸¬è©¦å¤¾å…·...")
    fixtures = manager.create_test_fixtures()
    
    # åˆ—å‡ºæ‰€æœ‰è³‡æ–™
    print("\nğŸ“Š è³‡æ–™ç¸½è¦½:")
    all_data = manager.list_data()
    for data_info in all_data:
        print(f"  {data_info.name} ({data_info.data_type}): {data_info.description}")
    
    # é©—è­‰è³‡æ–™å®Œæ•´æ€§
    print("\nâœ… é©—è­‰è³‡æ–™å®Œæ•´æ€§...")
    integrity_results = manager.validate_data_integrity()
    valid_count = sum(integrity_results.values())
    total_count = len(integrity_results)
    print(f"æœ‰æ•ˆè³‡æ–™: {valid_count}/{total_count}")
    
    # åŒ¯å‡ºè³‡æ–™æ¸…å–®
    print("\nğŸ“‹ åŒ¯å‡ºè³‡æ–™æ¸…å–®...")
    manifest_file = manager.export_data_manifest()
    
    # æ¸…ç†è‡¨æ™‚è³‡æ–™
    print("\nğŸ§¹ æ¸…ç†éæœŸè‡¨æ™‚è³‡æ–™...")
    manager.cleanup_temporary_data()
    
    print("\nâœ¨ æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨è¨­å®šå®Œæˆï¼")


if __name__ == "__main__":
    main()