#!/usr/bin/env python3
"""
å“è³ªé–˜é–€æª¢æŸ¥å™¨

æª¢æŸ¥æ¸¬è©¦çµæœã€è¦†è“‹ç‡å’Œå…¶ä»–å“è³ªæŒ‡æ¨™ï¼Œæ±ºå®šæ˜¯å¦é€šéå“è³ªé–˜é–€ã€‚
"""

import argparse
import json
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import yaml


class QualityGateChecker:
    """å“è³ªé–˜é–€æª¢æŸ¥å™¨"""
    
    def __init__(self, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–å“è³ªé–˜é–€æª¢æŸ¥å™¨
        
        Args:
            config_file: å“è³ªé–˜é–€é…ç½®æª”æ¡ˆè·¯å¾‘
        """
        self.config = self._load_config(config_file)
        self.results = {
            "overall_status": "UNKNOWN",
            "checks": {},
            "metrics": {},
            "violations": []
        }
    
    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """è¼‰å…¥å“è³ªé–˜é–€é…ç½®"""
        default_config = {
            "thresholds": {
                "test_pass_rate": 90.0,
                "code_coverage": 80.0,
                "integration_pass_rate": 85.0,
                "chinese_test_pass_rate": 90.0,
                "max_test_duration": 300,  # ç§’
                "max_security_issues": 0
            },
            "required_checks": [
                "test_pass_rate",
                "code_coverage",
                "integration_tests",
                "chinese_tests"
            ],
            "optional_checks": [
                "performance_tests",
                "security_scan"
            ],
            "fail_on_missing_optional": False
        }
        
        if config_file and Path(config_file).exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                # åˆä½µé…ç½®
                default_config.update(user_config)
        
        return default_config
    
    def check_test_results(self, test_results_dir: str) -> bool:
        """æª¢æŸ¥æ¸¬è©¦çµæœ"""
        test_dir = Path(test_results_dir)
        
        # æª¢æŸ¥å–®å…ƒæ¸¬è©¦
        unit_test_passed = self._check_junit_xml(
            test_dir, "pytest-unit*.xml", "unit_tests"
        )
        
        # æª¢æŸ¥æ•´åˆæ¸¬è©¦
        integration_test_passed = self._check_junit_xml(
            test_dir, "pytest-integration*.xml", "integration_tests"
        )
        
        # æª¢æŸ¥ä¸­æ–‡æ¸¬è©¦
        chinese_test_passed = self._check_junit_xml(
            test_dir, "pytest-chinese*.xml", "chinese_tests"
        )
        
        # è¨ˆç®—ç¸½é«”é€šéç‡
        all_tests_data = self._collect_all_test_data(test_dir)
        overall_pass_rate = self._calculate_overall_pass_rate(all_tests_data)
        
        self.results["metrics"]["overall_pass_rate"] = overall_pass_rate
        self.results["checks"]["test_pass_rate"] = {
            "status": "PASS" if overall_pass_rate >= self.config["thresholds"]["test_pass_rate"] else "FAIL",
            "value": overall_pass_rate,
            "threshold": self.config["thresholds"]["test_pass_rate"],
            "message": f"æ•´é«”æ¸¬è©¦é€šéç‡: {overall_pass_rate:.1f}%"
        }
        
        if overall_pass_rate < self.config["thresholds"]["test_pass_rate"]:
            self.results["violations"].append(
                f"æ¸¬è©¦é€šéç‡ {overall_pass_rate:.1f}% ä½æ–¼é–¾å€¼ {self.config['thresholds']['test_pass_rate']}%"
            )
        
        return (unit_test_passed and integration_test_passed and 
                chinese_test_passed and 
                overall_pass_rate >= self.config["thresholds"]["test_pass_rate"])
    
    def _check_junit_xml(self, test_dir: Path, pattern: str, check_name: str) -> bool:
        """æª¢æŸ¥ JUnit XML æ¸¬è©¦çµæœ"""
        xml_files = list(test_dir.glob(f"**/{pattern}"))
        
        if not xml_files:
            self.results["checks"][check_name] = {
                "status": "MISSING",
                "message": f"æ‰¾ä¸åˆ° {pattern} æ¸¬è©¦çµæœæª”æ¡ˆ"
            }
            return False
        
        total_tests = 0
        failed_tests = 0
        total_time = 0.0
        
        for xml_file in xml_files:
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                tests = int(root.get('tests', 0))
                failures = int(root.get('failures', 0))
                errors = int(root.get('errors', 0))
                time = float(root.get('time', 0))
                
                total_tests += tests
                failed_tests += failures + errors
                total_time += time
                
            except Exception as e:
                self.results["violations"].append(f"è§£æ {xml_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                return False
        
        if total_tests == 0:
            self.results["checks"][check_name] = {
                "status": "FAIL",
                "message": f"{check_name}: æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ¸¬è©¦"
            }
            return False
        
        pass_rate = (total_tests - failed_tests) / total_tests * 100
        
        # æ ¹æ“šæ¸¬è©¦é¡å‹è¨­å®šä¸åŒçš„é–¾å€¼
        threshold_key = f"{check_name}_pass_rate"
        if threshold_key not in self.config["thresholds"]:
            threshold_key = "test_pass_rate"
        
        threshold = self.config["thresholds"][threshold_key]
        status = "PASS" if pass_rate >= threshold else "FAIL"
        
        self.results["checks"][check_name] = {
            "status": status,
            "value": pass_rate,
            "threshold": threshold,
            "total_tests": total_tests,
            "failed_tests": failed_tests,
            "duration": total_time,
            "message": f"{check_name}: {pass_rate:.1f}% é€šéç‡ ({total_tests - failed_tests}/{total_tests})"
        }
        
        if status == "FAIL":
            self.results["violations"].append(
                f"{check_name} é€šéç‡ {pass_rate:.1f}% ä½æ–¼é–¾å€¼ {threshold}%"
            )
        
        return status == "PASS"
    
    def _collect_all_test_data(self, test_dir: Path) -> List[Dict[str, Any]]:
        """æ”¶é›†æ‰€æœ‰æ¸¬è©¦è³‡æ–™"""
        all_data = []
        xml_files = list(test_dir.glob("**/pytest-*.xml"))
        
        for xml_file in xml_files:
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                all_data.append({
                    "file": str(xml_file),
                    "tests": int(root.get('tests', 0)),
                    "failures": int(root.get('failures', 0)),
                    "errors": int(root.get('errors', 0)),
                    "time": float(root.get('time', 0))
                })
            except Exception as e:
                print(f"è­¦å‘Š: ç„¡æ³•è§£æ {xml_file}: {e}")
        
        return all_data
    
    def _calculate_overall_pass_rate(self, test_data: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æ•´é«”é€šéç‡"""
        total_tests = sum(data["tests"] for data in test_data)
        total_failures = sum(data["failures"] + data["errors"] for data in test_data)
        
        if total_tests == 0:
            return 0.0
        
        return (total_tests - total_failures) / total_tests * 100
    
    def check_code_coverage(self, coverage_file: str) -> bool:
        """æª¢æŸ¥ç¨‹å¼ç¢¼è¦†è“‹ç‡"""
        coverage_path = Path(coverage_file)
        
        if not coverage_path.exists():
            self.results["checks"]["code_coverage"] = {
                "status": "MISSING",
                "message": "æ‰¾ä¸åˆ°è¦†è“‹ç‡å ±å‘Šæª”æ¡ˆ"
            }
            return False
        
        try:
            # è§£æ coverage.xml
            tree = ET.parse(coverage_path)
            root = tree.getroot()
            
            # è¨ˆç®—æ•´é«”è¦†è“‹ç‡
            total_lines = 0
            covered_lines = 0
            
            for package in root.findall('.//package'):
                for class_elem in package.findall('classes/class'):
                    for line in class_elem.findall('lines/line'):
                        total_lines += 1
                        if int(line.get('hits', 0)) > 0:
                            covered_lines += 1
            
            if total_lines == 0:
                coverage_rate = 0.0
            else:
                coverage_rate = covered_lines / total_lines * 100
            
            threshold = self.config["thresholds"]["code_coverage"]
            status = "PASS" if coverage_rate >= threshold else "FAIL"
            
            self.results["metrics"]["code_coverage"] = coverage_rate
            self.results["checks"]["code_coverage"] = {
                "status": status,
                "value": coverage_rate,
                "threshold": threshold,
                "covered_lines": covered_lines,
                "total_lines": total_lines,
                "message": f"ç¨‹å¼ç¢¼è¦†è“‹ç‡: {coverage_rate:.1f}%"
            }
            
            if status == "FAIL":
                self.results["violations"].append(
                    f"ç¨‹å¼ç¢¼è¦†è“‹ç‡ {coverage_rate:.1f}% ä½æ–¼é–¾å€¼ {threshold}%"
                )
            
            return status == "PASS"
            
        except Exception as e:
            self.results["checks"]["code_coverage"] = {
                "status": "ERROR",
                "message": f"è§£æè¦†è“‹ç‡æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            }
            return False
    
    def check_performance_benchmarks(self, benchmark_file: str) -> bool:
        """æª¢æŸ¥æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
        benchmark_path = Path(benchmark_file)
        
        if not benchmark_path.exists():
            self.results["checks"]["performance_tests"] = {
                "status": "MISSING",
                "message": "æ‰¾ä¸åˆ°æ•ˆèƒ½æ¸¬è©¦çµæœæª”æ¡ˆ"
            }
            return not self.config.get("fail_on_missing_optional", False)
        
        try:
            with open(benchmark_path, 'r', encoding='utf-8') as f:
                benchmark_data = json.load(f)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ•ˆèƒ½å›æ­¸
            performance_issues = []
            
            for benchmark in benchmark_data.get("benchmarks", []):
                name = benchmark.get("name", "unknown")
                stats = benchmark.get("stats", {})
                mean_time = stats.get("mean", 0)
                
                # é€™è£¡å¯ä»¥æ·»åŠ æ•ˆèƒ½é–¾å€¼æª¢æŸ¥
                # ä¾‹å¦‚ï¼šæŸäº›æ“ä½œä¸æ‡‰è©²è¶…éç‰¹å®šæ™‚é–“
                if "embedding" in name.lower() and mean_time > 1.0:
                    performance_issues.append(f"{name}: {mean_time:.3f}s > 1.0s")
                elif "query" in name.lower() and mean_time > 0.5:
                    performance_issues.append(f"{name}: {mean_time:.3f}s > 0.5s")
            
            status = "PASS" if not performance_issues else "FAIL"
            
            self.results["checks"]["performance_tests"] = {
                "status": status,
                "benchmarks_count": len(benchmark_data.get("benchmarks", [])),
                "issues": performance_issues,
                "message": f"æ•ˆèƒ½æ¸¬è©¦: {len(benchmark_data.get('benchmarks', []))} å€‹åŸºæº–æ¸¬è©¦"
            }
            
            if performance_issues:
                self.results["violations"].extend(performance_issues)
            
            return status == "PASS"
            
        except Exception as e:
            self.results["checks"]["performance_tests"] = {
                "status": "ERROR",
                "message": f"è§£ææ•ˆèƒ½æ¸¬è©¦æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            }
            return False
    
    def check_security_scan(self, security_file: str) -> bool:
        """æª¢æŸ¥å®‰å…¨æ€§æƒæçµæœ"""
        security_path = Path(security_file)
        
        if not security_path.exists():
            self.results["checks"]["security_scan"] = {
                "status": "MISSING",
                "message": "æ‰¾ä¸åˆ°å®‰å…¨æƒæçµæœæª”æ¡ˆ"
            }
            return not self.config.get("fail_on_missing_optional", False)
        
        try:
            with open(security_path, 'r', encoding='utf-8') as f:
                security_data = json.load(f)
            
            # çµ±è¨ˆå®‰å…¨å•é¡Œ
            high_issues = 0
            medium_issues = 0
            low_issues = 0
            
            for result in security_data.get("results", []):
                severity = result.get("issue_severity", "").lower()
                if severity == "high":
                    high_issues += 1
                elif severity == "medium":
                    medium_issues += 1
                elif severity == "low":
                    low_issues += 1
            
            total_issues = high_issues + medium_issues + low_issues
            max_issues = self.config["thresholds"]["max_security_issues"]
            
            status = "PASS" if total_issues <= max_issues else "FAIL"
            
            self.results["checks"]["security_scan"] = {
                "status": status,
                "total_issues": total_issues,
                "high_issues": high_issues,
                "medium_issues": medium_issues,
                "low_issues": low_issues,
                "threshold": max_issues,
                "message": f"å®‰å…¨æƒæ: {total_issues} å€‹å•é¡Œ (é«˜: {high_issues}, ä¸­: {medium_issues}, ä½: {low_issues})"
            }
            
            if status == "FAIL":
                self.results["violations"].append(
                    f"å®‰å…¨å•é¡Œæ•¸é‡ {total_issues} è¶…éé–¾å€¼ {max_issues}"
                )
            
            return status == "PASS"
            
        except Exception as e:
            self.results["checks"]["security_scan"] = {
                "status": "ERROR",
                "message": f"è§£æå®‰å…¨æƒææª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            }
            return False
    
    def run_all_checks(self, 
                      test_results_dir: str,
                      coverage_file: str = "coverage.xml",
                      benchmark_file: str = "benchmark.json",
                      security_file: str = "bandit-report.json") -> bool:
        """åŸ·è¡Œæ‰€æœ‰å“è³ªæª¢æŸ¥"""
        
        print("ğŸ” é–‹å§‹åŸ·è¡Œå“è³ªé–˜é–€æª¢æŸ¥...")
        
        all_passed = True
        
        # å¿…è¦æª¢æŸ¥
        for check in self.config["required_checks"]:
            if check == "test_pass_rate" or check in ["unit_tests", "integration_tests", "chinese_tests"]:
                if not self.check_test_results(test_results_dir):
                    all_passed = False
            elif check == "code_coverage":
                if not self.check_code_coverage(coverage_file):
                    all_passed = False
        
        # å¯é¸æª¢æŸ¥
        for check in self.config["optional_checks"]:
            if check == "performance_tests":
                if not self.check_performance_benchmarks(benchmark_file):
                    if self.config.get("fail_on_missing_optional", False):
                        all_passed = False
            elif check == "security_scan":
                if not self.check_security_scan(security_file):
                    if self.config.get("fail_on_missing_optional", False):
                        all_passed = False
        
        # è¨­å®šæ•´é«”ç‹€æ…‹
        self.results["overall_status"] = "PASS" if all_passed else "FAIL"
        
        return all_passed
    
    def print_results(self):
        """åˆ—å°æª¢æŸ¥çµæœ"""
        print(f"\nğŸ“Š å“è³ªé–˜é–€æª¢æŸ¥çµæœ: {self.results['overall_status']}")
        print("=" * 50)
        
        # åˆ—å°å„é …æª¢æŸ¥çµæœ
        for check_name, check_data in self.results["checks"].items():
            status = check_data["status"]
            message = check_data.get("message", "")
            
            status_icon = {
                "PASS": "âœ…",
                "FAIL": "âŒ", 
                "MISSING": "âš ï¸",
                "ERROR": "ğŸ”¥"
            }.get(status, "â“")
            
            print(f"{status_icon} {check_name}: {message}")
            
            if "value" in check_data and "threshold" in check_data:
                print(f"   å€¼: {check_data['value']:.1f}, é–¾å€¼: {check_data['threshold']}")
        
        # åˆ—å°é•è¦é …ç›®
        if self.results["violations"]:
            print(f"\nâŒ ç™¼ç¾ {len(self.results['violations'])} å€‹é•è¦é …ç›®:")
            for violation in self.results["violations"]:
                print(f"   â€¢ {violation}")
        
        # åˆ—å°é—œéµæŒ‡æ¨™
        if self.results["metrics"]:
            print(f"\nğŸ“ˆ é—œéµæŒ‡æ¨™:")
            for metric, value in self.results["metrics"].items():
                print(f"   â€¢ {metric}: {value:.1f}%")
    
    def save_results(self, output_file: str):
        """å„²å­˜æª¢æŸ¥çµæœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ æª¢æŸ¥çµæœå·²å„²å­˜åˆ°: {output_file}")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="å“è³ªé–˜é–€æª¢æŸ¥å™¨")
    parser.add_argument("--test-results-dir", default=".", help="æ¸¬è©¦çµæœç›®éŒ„")
    parser.add_argument("--coverage-file", default="coverage.xml", help="è¦†è“‹ç‡æª”æ¡ˆ")
    parser.add_argument("--benchmark-file", default="benchmark.json", help="æ•ˆèƒ½æ¸¬è©¦æª”æ¡ˆ")
    parser.add_argument("--security-file", default="bandit-report.json", help="å®‰å…¨æƒææª”æ¡ˆ")
    parser.add_argument("--config", help="å“è³ªé–˜é–€é…ç½®æª”æ¡ˆ")
    parser.add_argument("--output", default="quality-gate-results.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ")
    
    args = parser.parse_args()
    
    # å»ºç«‹æª¢æŸ¥å™¨
    checker = QualityGateChecker(args.config)
    
    # åŸ·è¡Œæª¢æŸ¥
    passed = checker.run_all_checks(
        args.test_results_dir,
        args.coverage_file,
        args.benchmark_file,
        args.security_file
    )
    
    # åˆ—å°çµæœ
    checker.print_results()
    
    # å„²å­˜çµæœ
    checker.save_results(args.output)
    
    # è¨­å®šé€€å‡ºç¢¼
    if not passed:
        print(f"\nâŒ å“è³ªé–˜é–€æª¢æŸ¥å¤±æ•—ï¼")
        sys.exit(1)
    else:
        print(f"\nâœ… å“è³ªé–˜é–€æª¢æŸ¥é€šéï¼")
        sys.exit(0)


if __name__ == "__main__":
    main()