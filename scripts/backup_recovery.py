#!/usr/bin/env python3
"""
å‚™ä»½å’Œæ¢å¾©ç¨‹åº

æä¾›å®Œæ•´çš„è³‡æ–™å‚™ä»½ã€æ¢å¾©å’Œç½é›£æ¢å¾©åŠŸèƒ½
"""

import hashlib
import json
import os
import shutil
import subprocess
import sys
import tarfile
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

import click
import yaml
from loguru import logger


class BackupManager:
    """å‚™ä»½ç®¡ç†å™¨"""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config = self._load_config(config_path)
        self.backup_dir = Path(self.config["backup_dir"])
        self.data_dir = Path(self.config["data_dir"])
        self.config_dir = Path(self.config["config_dir"])
        
        # å»ºç«‹å‚™ä»½ç›®éŒ„
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        logger.remove()
        logger.add(
            self.backup_dir / "backup.log",
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            rotation="10 MB",
            retention="90 days"
        )
        logger.add(sys.stdout, level="INFO")
    
    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:
        """è¼‰å…¥å‚™ä»½é…ç½®"""
        default_config = {
            "backup_dir": "./backups",
            "data_dir": "./data",
            "config_dir": "./config",
            "retention": {
                "daily": 7,    # ä¿ç•™ 7 å¤©çš„æ¯æ—¥å‚™ä»½
                "weekly": 4,   # ä¿ç•™ 4 é€±çš„é€±å‚™ä»½
                "monthly": 12  # ä¿ç•™ 12 å€‹æœˆçš„æœˆå‚™ä»½
            },
            "compression": True,
            "encryption": False,
            "verify_backup": True,
            "exclude_patterns": [
                "*.tmp",
                "*.log",
                "__pycache__",
                ".DS_Store"
            ],
            "remote_backup": {
                "enabled": False,
                "type": "s3",  # s3, ftp, rsync
                "config": {}
            }
        }
        
        if config_path and config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"è¼‰å…¥é…ç½®å¤±æ•—ï¼Œä½¿ç”¨é è¨­é…ç½®: {e}")
        
        return default_config
    
    def create_backup(self, backup_type: str = "full", description: str = "") -> Optional[str]:
        """å»ºç«‹å‚™ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{backup_type}_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        logger.info(f"ğŸ”„ é–‹å§‹å»ºç«‹å‚™ä»½: {backup_name}")
        
        try:
            # å»ºç«‹å‚™ä»½ç›®éŒ„
            backup_path.mkdir(exist_ok=True)
            
            # å»ºç«‹å‚™ä»½è³‡è¨Š
            backup_info = {
                "name": backup_name,
                "type": backup_type,
                "description": description,
                "created_at": datetime.now().isoformat(),
                "version": self._get_system_version(),
                "files": [],
                "size_bytes": 0,
                "checksum": None
            }
            
            # å‚™ä»½è³‡æ–™ç›®éŒ„
            if self.data_dir.exists():
                data_backup_path = backup_path / "data"
                self._copy_directory(self.data_dir, data_backup_path)
                backup_info["files"].append("data")
                logger.info("âœ… è³‡æ–™ç›®éŒ„å‚™ä»½å®Œæˆ")
            
            # å‚™ä»½é…ç½®ç›®éŒ„
            if self.config_dir.exists():
                config_backup_path = backup_path / "config"
                self._copy_directory(self.config_dir, config_backup_path)
                backup_info["files"].append("config")
                logger.info("âœ… é…ç½®ç›®éŒ„å‚™ä»½å®Œæˆ")
            
            # å‚™ä»½æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆï¼ˆå¦‚æœæ˜¯å®Œæ•´å‚™ä»½ï¼‰
            if backup_type == "full":
                app_files = ["main.py", "pyproject.toml", "README.md"]
                app_backup_path = backup_path / "app"
                app_backup_path.mkdir(exist_ok=True)
                
                for app_file in app_files:
                    src_file = Path(app_file)
                    if src_file.exists():
                        shutil.copy2(src_file, app_backup_path / app_file)
                        backup_info["files"].append(f"app/{app_file}")
                
                # å‚™ä»½æºç¢¼ç›®éŒ„
                src_dir = Path("src")
                if src_dir.exists():
                    src_backup_path = app_backup_path / "src"
                    self._copy_directory(src_dir, src_backup_path)
                    backup_info["files"].append("app/src")
                
                logger.info("âœ… æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆå‚™ä»½å®Œæˆ")
            
            # è¨ˆç®—å‚™ä»½å¤§å°
            backup_info["size_bytes"] = self._calculate_directory_size(backup_path)
            
            # å£“ç¸®å‚™ä»½ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.config["compression"]:
                compressed_path = self._compress_backup(backup_path)
                if compressed_path:
                    # åˆªé™¤åŸå§‹ç›®éŒ„
                    shutil.rmtree(backup_path)
                    backup_path = compressed_path
                    backup_info["compressed"] = True
                    backup_info["size_bytes"] = backup_path.stat().st_size
                    logger.info("âœ… å‚™ä»½å£“ç¸®å®Œæˆ")
            
            # è¨ˆç®—æ ¡é©—å’Œ
            if self.config["verify_backup"]:
                backup_info["checksum"] = self._calculate_checksum(backup_path)
                logger.info("âœ… æ ¡é©—å’Œè¨ˆç®—å®Œæˆ")
            
            # å„²å­˜å‚™ä»½è³‡è¨Š
            info_file = backup_path.parent / f"{backup_name}.info"
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(backup_info, f, ensure_ascii=False, indent=2)
            
            # é ç«¯å‚™ä»½ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.config["remote_backup"]["enabled"]:
                self._upload_to_remote(backup_path, backup_info)
            
            logger.success(f"ğŸ‰ å‚™ä»½å»ºç«‹å®Œæˆ: {backup_name}")
            logger.info(f"   å‚™ä»½å¤§å°: {backup_info['size_bytes'] / (1024**2):.1f}MB")
            logger.info(f"   å‚™ä»½ä½ç½®: {backup_path}")
            
            return backup_name
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½å»ºç«‹å¤±æ•—: {e}")
            # æ¸…ç†å¤±æ•—çš„å‚™ä»½
            if backup_path.exists():
                if backup_path.is_dir():
                    shutil.rmtree(backup_path)
                else:
                    backup_path.unlink()
            return None
    
    def _copy_directory(self, src: Path, dst: Path):
        """è¤‡è£½ç›®éŒ„ï¼Œæ’é™¤æŒ‡å®šæ¨¡å¼"""
        def ignore_patterns(dir_path, names):
            ignored = []
            for pattern in self.config["exclude_patterns"]:
                import fnmatch
                ignored.extend(fnmatch.filter(names, pattern))
            return ignored
        
        shutil.copytree(src, dst, ignore=ignore_patterns, dirs_exist_ok=True)
    
    def _calculate_directory_size(self, path: Path) -> int:
        """è¨ˆç®—ç›®éŒ„å¤§å°"""
        total_size = 0
        if path.is_file():
            return path.stat().st_size
        
        for item in path.rglob('*'):
            if item.is_file():
                total_size += item.stat().st_size
        return total_size
    
    def _compress_backup(self, backup_path: Path) -> Optional[Path]:
        """å£“ç¸®å‚™ä»½"""
        try:
            compressed_path = backup_path.with_suffix('.tar.gz')
            
            with tarfile.open(compressed_path, 'w:gz') as tar:
                tar.add(backup_path, arcname=backup_path.name)
            
            return compressed_path
            
        except Exception as e:
            logger.error(f"å£“ç¸®å‚™ä»½å¤±æ•—: {e}")
            return None
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆæ ¡é©—å’Œ"""
        hash_md5 = hashlib.md5()
        
        if file_path.is_file():
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
        else:
            # ç›®éŒ„çš„è©±ï¼Œè¨ˆç®—æ‰€æœ‰æª”æ¡ˆçš„æ ¡é©—å’Œ
            for file_path in sorted(file_path.rglob('*')):
                if file_path.is_file():
                    with open(file_path, "rb") as f:
                        for chunk in iter(lambda: f.read(4096), b""):
                            hash_md5.update(chunk)
        
        return hash_md5.hexdigest()
    
    def _get_system_version(self) -> str:
        """ç²å–ç³»çµ±ç‰ˆæœ¬"""
        try:
            # å˜—è©¦å¾ pyproject.toml è®€å–ç‰ˆæœ¬
            pyproject_path = Path("pyproject.toml")
            if pyproject_path.exists():
                import toml
                with open(pyproject_path, 'r', encoding='utf-8') as f:
                    pyproject = toml.load(f)
                    return pyproject.get("project", {}).get("version", "unknown")
        except:
            pass
        
        return "unknown"
    
    def _upload_to_remote(self, backup_path: Path, backup_info: Dict[str, Any]):
        """ä¸Šå‚³åˆ°é ç«¯å„²å­˜"""
        try:
            remote_config = self.config["remote_backup"]
            
            if remote_config["type"] == "s3":
                self._upload_to_s3(backup_path, backup_info)
            elif remote_config["type"] == "ftp":
                self._upload_to_ftp(backup_path, backup_info)
            elif remote_config["type"] == "rsync":
                self._upload_to_rsync(backup_path, backup_info)
            
            logger.info("âœ… é ç«¯å‚™ä»½ä¸Šå‚³å®Œæˆ")
            
        except Exception as e:
            logger.error(f"é ç«¯å‚™ä»½ä¸Šå‚³å¤±æ•—: {e}")
    
    def _upload_to_s3(self, backup_path: Path, backup_info: Dict[str, Any]):
        """ä¸Šå‚³åˆ° S3"""
        # é€™è£¡å¯ä»¥å¯¦ä½œ S3 ä¸Šå‚³é‚è¼¯
        pass
    
    def _upload_to_ftp(self, backup_path: Path, backup_info: Dict[str, Any]):
        """ä¸Šå‚³åˆ° FTP"""
        # é€™è£¡å¯ä»¥å¯¦ä½œ FTP ä¸Šå‚³é‚è¼¯
        pass
    
    def _upload_to_rsync(self, backup_path: Path, backup_info: Dict[str, Any]):
        """ä½¿ç”¨ rsync åŒæ­¥"""
        # é€™è£¡å¯ä»¥å¯¦ä½œ rsync åŒæ­¥é‚è¼¯
        pass
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å‚™ä»½"""
        backups = []
        
        for info_file in self.backup_dir.glob("*.info"):
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    backup_info = json.load(f)
                    
                    # æª¢æŸ¥å‚™ä»½æª”æ¡ˆæ˜¯å¦å­˜åœ¨
                    backup_name = backup_info["name"]
                    backup_file = self.backup_dir / f"{backup_name}.tar.gz"
                    backup_dir = self.backup_dir / backup_name
                    
                    if backup_file.exists():
                        backup_info["path"] = str(backup_file)
                        backup_info["exists"] = True
                    elif backup_dir.exists():
                        backup_info["path"] = str(backup_dir)
                        backup_info["exists"] = True
                    else:
                        backup_info["exists"] = False
                    
                    backups.append(backup_info)
                    
            except Exception as e:
                logger.warning(f"è®€å–å‚™ä»½è³‡è¨Šå¤±æ•— {info_file}: {e}")
        
        # æŒ‰å»ºç«‹æ™‚é–“æ’åº
        backups.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        return backups
    
    def restore_backup(self, backup_name: str, target_dir: Optional[Path] = None) -> bool:
        """æ¢å¾©å‚™ä»½"""
        logger.info(f"ğŸ”„ é–‹å§‹æ¢å¾©å‚™ä»½: {backup_name}")
        
        try:
            # æŸ¥æ‰¾å‚™ä»½
            backup_info = self._get_backup_info(backup_name)
            if not backup_info:
                logger.error(f"æ‰¾ä¸åˆ°å‚™ä»½: {backup_name}")
                return False
            
            # ç¢ºå®šå‚™ä»½æª”æ¡ˆè·¯å¾‘
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"
            backup_dir = self.backup_dir / backup_name
            
            if backup_file.exists():
                # è§£å£“ç¸®å‚™ä»½
                temp_dir = self.backup_dir / f"temp_{backup_name}"
                if temp_dir.exists():
                    shutil.rmtree(temp_dir)
                
                with tarfile.open(backup_file, 'r:gz') as tar:
                    tar.extractall(temp_dir)
                
                backup_source = temp_dir / backup_name
            elif backup_dir.exists():
                backup_source = backup_dir
            else:
                logger.error(f"å‚™ä»½æª”æ¡ˆä¸å­˜åœ¨: {backup_name}")
                return False
            
            # é©—è­‰å‚™ä»½å®Œæ•´æ€§
            if self.config["verify_backup"] and backup_info.get("checksum"):
                current_checksum = self._calculate_checksum(backup_source)
                if current_checksum != backup_info["checksum"]:
                    logger.error("å‚™ä»½æ ¡é©—å’Œä¸åŒ¹é…ï¼Œå¯èƒ½å·²æå£")
                    return False
                logger.info("âœ… å‚™ä»½å®Œæ•´æ€§é©—è­‰é€šé")
            
            # ç¢ºå®šæ¢å¾©ç›®æ¨™
            if target_dir is None:
                target_dir = Path(".")
            
            # å‚™ä»½ç•¶å‰è³‡æ–™
            current_backup_name = f"current_backup_{int(time.time())}"
            logger.info(f"å‚™ä»½ç•¶å‰è³‡æ–™è‡³: {current_backup_name}")
            self.create_backup("current", f"æ¢å¾©å‰çš„è‡ªå‹•å‚™ä»½")
            
            # æ¢å¾©è³‡æ–™
            if (backup_source / "data").exists():
                if self.data_dir.exists():
                    shutil.rmtree(self.data_dir)
                shutil.copytree(backup_source / "data", self.data_dir)
                logger.info("âœ… è³‡æ–™ç›®éŒ„æ¢å¾©å®Œæˆ")
            
            # æ¢å¾©é…ç½®
            if (backup_source / "config").exists():
                if self.config_dir.exists():
                    # å‚™ä»½é‡è¦é…ç½®æª”æ¡ˆ
                    important_configs = [".env", "production.yaml"]
                    config_backup = {}
                    for config_file in important_configs:
                        config_path = self.config_dir / config_file
                        if config_path.exists():
                            config_backup[config_file] = config_path.read_text(encoding='utf-8')
                    
                    shutil.rmtree(self.config_dir)
                    shutil.copytree(backup_source / "config", self.config_dir)
                    
                    # æ¢å¾©é‡è¦é…ç½®æª”æ¡ˆ
                    for config_file, content in config_backup.items():
                        (self.config_dir / config_file).write_text(content, encoding='utf-8')
                        logger.info(f"ä¿ç•™ç¾æœ‰é…ç½®: {config_file}")
                
                logger.info("âœ… é…ç½®ç›®éŒ„æ¢å¾©å®Œæˆ")
            
            # æ¢å¾©æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆï¼ˆå¦‚æœæ˜¯å®Œæ•´å‚™ä»½ï¼‰
            if (backup_source / "app").exists():
                app_source = backup_source / "app"
                
                # æ¢å¾©ä¸»è¦æª”æ¡ˆ
                for item in app_source.iterdir():
                    if item.is_file():
                        shutil.copy2(item, target_dir / item.name)
                    elif item.is_dir() and item.name == "src":
                        if (target_dir / "src").exists():
                            shutil.rmtree(target_dir / "src")
                        shutil.copytree(item, target_dir / "src")
                
                logger.info("âœ… æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆæ¢å¾©å®Œæˆ")
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if backup_file.exists() and 'temp_dir' in locals():
                shutil.rmtree(temp_dir)
            
            logger.success(f"ğŸ‰ å‚™ä»½æ¢å¾©å®Œæˆ: {backup_name}")
            logger.info("è«‹é‡å•Ÿæ‡‰ç”¨ç¨‹å¼ä»¥ä½¿æ¢å¾©ç”Ÿæ•ˆ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½æ¢å¾©å¤±æ•—: {e}")
            return False
    
    def _get_backup_info(self, backup_name: str) -> Optional[Dict[str, Any]]:
        """ç²å–å‚™ä»½è³‡è¨Š"""
        info_file = self.backup_dir / f"{backup_name}.info"
        if info_file.exists():
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"è®€å–å‚™ä»½è³‡è¨Šå¤±æ•—: {e}")
        return None
    
    def delete_backup(self, backup_name: str) -> bool:
        """åˆªé™¤å‚™ä»½"""
        logger.info(f"ğŸ—‘ï¸ åˆªé™¤å‚™ä»½: {backup_name}")
        
        try:
            # åˆªé™¤å‚™ä»½æª”æ¡ˆ
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"
            backup_dir = self.backup_dir / backup_name
            info_file = self.backup_dir / f"{backup_name}.info"
            
            deleted_items = []
            
            if backup_file.exists():
                backup_file.unlink()
                deleted_items.append("å£“ç¸®æª”æ¡ˆ")
            
            if backup_dir.exists():
                shutil.rmtree(backup_dir)
                deleted_items.append("ç›®éŒ„")
            
            if info_file.exists():
                info_file.unlink()
                deleted_items.append("è³‡è¨Šæª”æ¡ˆ")
            
            if deleted_items:
                logger.info(f"âœ… å·²åˆªé™¤: {', '.join(deleted_items)}")
                return True
            else:
                logger.warning("å‚™ä»½ä¸å­˜åœ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆªé™¤å‚™ä»½å¤±æ•—: {e}")
            return False
    
    def cleanup_old_backups(self):
        """æ¸…ç†èˆŠå‚™ä»½"""
        logger.info("ğŸ§¹ æ¸…ç†èˆŠå‚™ä»½")
        
        backups = self.list_backups()
        retention = self.config["retention"]
        
        # æŒ‰é¡å‹åˆ†çµ„
        daily_backups = [b for b in backups if b.get("type") == "daily"]
        weekly_backups = [b for b in backups if b.get("type") == "weekly"]
        monthly_backups = [b for b in backups if b.get("type") == "monthly"]
        full_backups = [b for b in backups if b.get("type") == "full"]
        
        # æ¸…ç†æ¯æ—¥å‚™ä»½
        if len(daily_backups) > retention["daily"]:
            for backup in daily_backups[retention["daily"]:]:
                self.delete_backup(backup["name"])
        
        # æ¸…ç†é€±å‚™ä»½
        if len(weekly_backups) > retention["weekly"]:
            for backup in weekly_backups[retention["weekly"]:]:
                self.delete_backup(backup["name"])
        
        # æ¸…ç†æœˆå‚™ä»½
        if len(monthly_backups) > retention["monthly"]:
            for backup in monthly_backups[retention["monthly"]:]:
                self.delete_backup(backup["name"])
        
        # ä¿ç•™æœ€è¿‘çš„å®Œæ•´å‚™ä»½
        if len(full_backups) > 3:  # ä¿ç•™æœ€è¿‘ 3 å€‹å®Œæ•´å‚™ä»½
            for backup in full_backups[3:]:
                self.delete_backup(backup["name"])
        
        logger.info("âœ… èˆŠå‚™ä»½æ¸…ç†å®Œæˆ")
    
    def verify_backup(self, backup_name: str) -> bool:
        """é©—è­‰å‚™ä»½å®Œæ•´æ€§"""
        logger.info(f"ğŸ” é©—è­‰å‚™ä»½: {backup_name}")
        
        try:
            backup_info = self._get_backup_info(backup_name)
            if not backup_info:
                logger.error("æ‰¾ä¸åˆ°å‚™ä»½è³‡è¨Š")
                return False
            
            # æª¢æŸ¥å‚™ä»½æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"
            backup_dir = self.backup_dir / backup_name
            
            if backup_file.exists():
                backup_path = backup_file
            elif backup_dir.exists():
                backup_path = backup_dir
            else:
                logger.error("å‚™ä»½æª”æ¡ˆä¸å­˜åœ¨")
                return False
            
            # é©—è­‰æ ¡é©—å’Œ
            if backup_info.get("checksum"):
                current_checksum = self._calculate_checksum(backup_path)
                if current_checksum == backup_info["checksum"]:
                    logger.info("âœ… æ ¡é©—å’Œé©—è­‰é€šé")
                    return True
                else:
                    logger.error("âŒ æ ¡é©—å’Œé©—è­‰å¤±æ•—")
                    return False
            else:
                logger.warning("æ²’æœ‰æ ¡é©—å’Œè³‡è¨Šï¼Œç„¡æ³•é©—è­‰")
                return True
                
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½é©—è­‰å¤±æ•—: {e}")
            return False


@click.group()
def cli():
    """å‚™ä»½å’Œæ¢å¾©å·¥å…·"""
    pass


@cli.command()
@click.option("--type", "backup_type", default="full", 
              type=click.Choice(["full", "data", "config", "daily", "weekly", "monthly"]),
              help="å‚™ä»½é¡å‹")
@click.option("--description", default="", help="å‚™ä»½æè¿°")
@click.option("--config", type=click.Path(), help="é…ç½®æª”æ¡ˆ")
def backup(backup_type: str, description: str, config: Optional[str]):
    """å»ºç«‹å‚™ä»½"""
    config_path = Path(config) if config else None
    manager = BackupManager(config_path)
    
    backup_name = manager.create_backup(backup_type, description)
    if backup_name:
        click.echo(f"å‚™ä»½å»ºç«‹æˆåŠŸ: {backup_name}")
    else:
        click.echo("å‚™ä»½å»ºç«‹å¤±æ•—")
        sys.exit(1)


@cli.command()
def list():
    """åˆ—å‡ºæ‰€æœ‰å‚™ä»½"""
    manager = BackupManager()
    backups = manager.list_backups()
    
    if not backups:
        click.echo("æ²’æœ‰æ‰¾åˆ°å‚™ä»½")
        return
    
    click.echo(f"{'å‚™ä»½åç¨±':<30} {'é¡å‹':<10} {'å¤§å°':<10} {'å»ºç«‹æ™‚é–“':<20} {'ç‹€æ…‹'}")
    click.echo("-" * 80)
    
    for backup in backups:
        size_mb = backup.get("size_bytes", 0) / (1024**2)
        created_at = backup.get("created_at", "")[:19].replace("T", " ")
        status = "âœ…" if backup.get("exists", False) else "âŒ"
        
        click.echo(f"{backup['name']:<30} {backup.get('type', ''):<10} {size_mb:>8.1f}MB {created_at:<20} {status}")


@cli.command()
@click.argument("backup_name")
@click.option("--target", type=click.Path(), help="æ¢å¾©ç›®æ¨™ç›®éŒ„")
def restore(backup_name: str, target: Optional[str]):
    """æ¢å¾©å‚™ä»½"""
    manager = BackupManager()
    target_path = Path(target) if target else None
    
    success = manager.restore_backup(backup_name, target_path)
    if success:
        click.echo("å‚™ä»½æ¢å¾©æˆåŠŸ")
    else:
        click.echo("å‚™ä»½æ¢å¾©å¤±æ•—")
        sys.exit(1)


@cli.command()
@click.argument("backup_name")
def delete(backup_name: str):
    """åˆªé™¤å‚™ä»½"""
    if not click.confirm(f"ç¢ºå®šè¦åˆªé™¤å‚™ä»½ '{backup_name}' å—ï¼Ÿ"):
        return
    
    manager = BackupManager()
    success = manager.delete_backup(backup_name)
    if success:
        click.echo("å‚™ä»½åˆªé™¤æˆåŠŸ")
    else:
        click.echo("å‚™ä»½åˆªé™¤å¤±æ•—")
        sys.exit(1)


@cli.command()
@click.argument("backup_name")
def verify(backup_name: str):
    """é©—è­‰å‚™ä»½å®Œæ•´æ€§"""
    manager = BackupManager()
    success = manager.verify_backup(backup_name)
    if success:
        click.echo("å‚™ä»½é©—è­‰é€šé")
    else:
        click.echo("å‚™ä»½é©—è­‰å¤±æ•—")
        sys.exit(1)


@cli.command()
def cleanup():
    """æ¸…ç†èˆŠå‚™ä»½"""
    manager = BackupManager()
    manager.cleanup_old_backups()
    click.echo("èˆŠå‚™ä»½æ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    cli()