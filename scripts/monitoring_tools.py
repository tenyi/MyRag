#!/usr/bin/env python3
"""
ç›£æ§å’Œç¶­è­·å·¥å…·

æä¾›ç³»çµ±ç›£æ§ã€æ•ˆèƒ½åˆ†æå’Œç¶­è­·åŠŸèƒ½
"""

import json
import os
import psutil
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

import click
import yaml
from loguru import logger


class SystemMonitor:
    """ç³»çµ±ç›£æ§å™¨"""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config = self._load_config(config_path)
        self.metrics_dir = Path(self.config.get("metrics_dir", "./monitoring/metrics"))
        self.alerts_dir = Path(self.config.get("alerts_dir", "./monitoring/alerts"))
        
        # å»ºç«‹ç›£æ§ç›®éŒ„
        self.metrics_dir.mkdir(parents=True, exist_ok=True)
        self.alerts_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        logger.remove()
        logger.add(
            self.metrics_dir.parent / "monitoring.log",
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            rotation="1 day",
            retention="30 days"
        )
        logger.add(sys.stdout, level="INFO")
    
    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:
        """è¼‰å…¥ç›£æ§é…ç½®"""
        default_config = {
            "metrics_dir": "./monitoring/metrics",
            "alerts_dir": "./monitoring/alerts",
            "collection_interval": 60,  # ç§’
            "retention_days": 30,
            "thresholds": {
                "cpu_percent": 80,
                "memory_percent": 85,
                "disk_percent": 90,
                "response_time_ms": 5000
            },
            "alert_channels": {
                "email": False,
                "webhook": False,
                "log": True
            }
        }
        
        if config_path and config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"è¼‰å…¥é…ç½®å¤±æ•—ï¼Œä½¿ç”¨é è¨­é…ç½®: {e}")
        
        return default_config
    
    def collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        try:
            # CPU æŒ‡æ¨™
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_count = psutil.cpu_count()
            
            # è¨˜æ†¶é«”æŒ‡æ¨™
            memory = psutil.virtual_memory()
            
            # ç£ç¢ŸæŒ‡æ¨™
            disk = psutil.disk_usage('/')
            
            # ç¶²è·¯æŒ‡æ¨™
            network = psutil.net_io_counters()
            
            # é€²ç¨‹æŒ‡æ¨™
            processes = len(psutil.pids())
            
            metrics = {
                "timestamp": time.time(),
                "datetime": datetime.now().isoformat(),
                "system": {
                    "cpu": {
                        "percent": cpu_percent,
                        "count": cpu_count,
                        "load_avg": os.getloadavg() if hasattr(os, 'getloadavg') else None
                    },
                    "memory": {
                        "total_gb": memory.total / (1024**3),
                        "available_gb": memory.available / (1024**3),
                        "used_gb": memory.used / (1024**3),
                        "percent": memory.percent
                    },
                    "disk": {
                        "total_gb": disk.total / (1024**3),
                        "free_gb": disk.free / (1024**3),
                        "used_gb": disk.used / (1024**3),
                        "percent": (disk.used / disk.total) * 100
                    },
                    "network": {
                        "bytes_sent": network.bytes_sent,
                        "bytes_recv": network.bytes_recv,
                        "packets_sent": network.packets_sent,
                        "packets_recv": network.packets_recv
                    },
                    "processes": processes
                }
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†ç³»çµ±æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    def collect_application_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™"""
        try:
            # æŸ¥æ‰¾æ‡‰ç”¨ç¨‹å¼é€²ç¨‹
            app_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']):
                try:
                    if 'chinese_graphrag' in proc.info['name'] or 'uvicorn' in proc.info['name']:
                        app_processes.append({
                            "pid": proc.info['pid'],
                            "name": proc.info['name'],
                            "cpu_percent": proc.info['cpu_percent'],
                            "memory_mb": proc.info['memory_info'].rss / (1024**2) if proc.info['memory_info'] else 0
                        })
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            # API å¥åº·æª¢æŸ¥
            api_health = self._check_api_health()
            
            # è³‡æ–™åº«ç‹€æ…‹
            db_status = self._check_database_status()
            
            metrics = {
                "timestamp": time.time(),
                "datetime": datetime.now().isoformat(),
                "application": {
                    "processes": app_processes,
                    "process_count": len(app_processes),
                    "total_cpu_percent": sum(p["cpu_percent"] for p in app_processes),
                    "total_memory_mb": sum(p["memory_mb"] for p in app_processes),
                    "api_health": api_health,
                    "database_status": db_status
                }
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    def _check_api_health(self) -> Dict[str, Any]:
        """æª¢æŸ¥ API å¥åº·ç‹€æ…‹"""
        try:
            import requests
            
            start_time = time.time()
            response = requests.get("http://localhost:8000/health", timeout=5)
            response_time = (time.time() - start_time) * 1000  # æ¯«ç§’
            
            return {
                "status": "healthy" if response.status_code == 200 else "unhealthy",
                "status_code": response.status_code,
                "response_time_ms": response_time,
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "response_time_ms": None,
                "last_check": datetime.now().isoformat()
            }
    
    def _check_database_status(self) -> Dict[str, Any]:
        """æª¢æŸ¥è³‡æ–™åº«ç‹€æ…‹"""
        try:
            # æª¢æŸ¥å‘é‡è³‡æ–™åº«ç›®éŒ„
            vector_db_path = Path("./data/vector_db")
            vector_db_exists = vector_db_path.exists()
            vector_db_size = sum(f.stat().st_size for f in vector_db_path.rglob('*') if f.is_file()) if vector_db_exists else 0
            
            # æª¢æŸ¥åœ–å½¢è³‡æ–™åº«ç›®éŒ„
            graph_db_path = Path("./data/graph_db")
            graph_db_exists = graph_db_path.exists()
            graph_db_size = sum(f.stat().st_size for f in graph_db_path.rglob('*') if f.is_file()) if graph_db_exists else 0
            
            return {
                "vector_db": {
                    "exists": vector_db_exists,
                    "size_mb": vector_db_size / (1024**2),
                    "path": str(vector_db_path)
                },
                "graph_db": {
                    "exists": graph_db_exists,
                    "size_mb": graph_db_size / (1024**2),
                    "path": str(graph_db_path)
                },
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "last_check": datetime.now().isoformat()
            }
    
    def save_metrics(self, metrics: Dict[str, Any]):
        """å„²å­˜æŒ‡æ¨™è³‡æ–™"""
        try:
            timestamp = int(metrics.get("timestamp", time.time()))
            date_str = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d")
            
            # æŒ‰æ—¥æœŸåˆ†æª”å„²å­˜
            metrics_file = self.metrics_dir / f"metrics_{date_str}.jsonl"
            
            with open(metrics_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(metrics, ensure_ascii=False) + '\n')
            
            logger.debug(f"æŒ‡æ¨™å·²å„²å­˜: {metrics_file}")
            
        except Exception as e:
            logger.error(f"å„²å­˜æŒ‡æ¨™å¤±æ•—: {e}")
    
    def check_alerts(self, metrics: Dict[str, Any]):
        """æª¢æŸ¥è­¦å ±æ¢ä»¶"""
        try:
            thresholds = self.config["thresholds"]
            alerts = []
            
            # æª¢æŸ¥ç³»çµ±æŒ‡æ¨™
            if "system" in metrics:
                system = metrics["system"]
                
                # CPU è­¦å ±
                if system.get("cpu", {}).get("percent", 0) > thresholds["cpu_percent"]:
                    alerts.append({
                        "type": "cpu_high",
                        "severity": "warning",
                        "message": f"CPU ä½¿ç”¨ç‡éé«˜: {system['cpu']['percent']:.1f}%",
                        "value": system["cpu"]["percent"],
                        "threshold": thresholds["cpu_percent"]
                    })
                
                # è¨˜æ†¶é«”è­¦å ±
                if system.get("memory", {}).get("percent", 0) > thresholds["memory_percent"]:
                    alerts.append({
                        "type": "memory_high",
                        "severity": "warning",
                        "message": f"è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {system['memory']['percent']:.1f}%",
                        "value": system["memory"]["percent"],
                        "threshold": thresholds["memory_percent"]
                    })
                
                # ç£ç¢Ÿè­¦å ±
                if system.get("disk", {}).get("percent", 0) > thresholds["disk_percent"]:
                    alerts.append({
                        "type": "disk_high",
                        "severity": "critical",
                        "message": f"ç£ç¢Ÿä½¿ç”¨ç‡éé«˜: {system['disk']['percent']:.1f}%",
                        "value": system["disk"]["percent"],
                        "threshold": thresholds["disk_percent"]
                    })
            
            # æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™
            if "application" in metrics:
                app = metrics["application"]
                
                # API å›æ‡‰æ™‚é–“è­¦å ±
                api_health = app.get("api_health", {})
                response_time = api_health.get("response_time_ms")
                if response_time and response_time > thresholds["response_time_ms"]:
                    alerts.append({
                        "type": "api_slow",
                        "severity": "warning",
                        "message": f"API å›æ‡‰æ™‚é–“éé•·: {response_time:.0f}ms",
                        "value": response_time,
                        "threshold": thresholds["response_time_ms"]
                    })
                
                # API å¥åº·ç‹€æ…‹è­¦å ±
                if api_health.get("status") != "healthy":
                    alerts.append({
                        "type": "api_unhealthy",
                        "severity": "critical",
                        "message": f"API å¥åº·æª¢æŸ¥å¤±æ•—: {api_health.get('status')}",
                        "value": api_health.get("status"),
                        "threshold": "healthy"
                    })
            
            # è™•ç†è­¦å ±
            for alert in alerts:
                self._handle_alert(alert)
            
            return alerts
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥è­¦å ±å¤±æ•—: {e}")
            return []
    
    def _handle_alert(self, alert: Dict[str, Any]):
        """è™•ç†è­¦å ±"""
        try:
            alert["timestamp"] = time.time()
            alert["datetime"] = datetime.now().isoformat()
            
            # è¨˜éŒ„è­¦å ±
            if self.config["alert_channels"]["log"]:
                severity_icon = "ğŸš¨" if alert["severity"] == "critical" else "âš ï¸"
                logger.warning(f"{severity_icon} {alert['message']}")
            
            # å„²å­˜è­¦å ±
            alert_file = self.alerts_dir / f"alerts_{datetime.now().strftime('%Y-%m-%d')}.jsonl"
            with open(alert_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(alert, ensure_ascii=False) + '\n')
            
            # å…¶ä»–è­¦å ±é€šé“ï¼ˆemail, webhook ç­‰ï¼‰
            # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šè­¦å ±é€šé“çš„å¯¦ä½œ
            
        except Exception as e:
            logger.error(f"è™•ç†è­¦å ±å¤±æ•—: {e}")
    
    def cleanup_old_data(self):
        """æ¸…ç†èˆŠè³‡æ–™"""
        try:
            retention_days = self.config["retention_days"]
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            # æ¸…ç†èˆŠçš„æŒ‡æ¨™æª”æ¡ˆ
            for metrics_file in self.metrics_dir.glob("metrics_*.jsonl"):
                try:
                    file_date_str = metrics_file.stem.replace("metrics_", "")
                    file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                    
                    if file_date < cutoff_date:
                        metrics_file.unlink()
                        logger.info(f"å·²åˆªé™¤èˆŠæŒ‡æ¨™æª”æ¡ˆ: {metrics_file}")
                        
                except ValueError:
                    # æª”æ¡ˆåç¨±æ ¼å¼ä¸æ­£ç¢ºï¼Œè·³é
                    continue
            
            # æ¸…ç†èˆŠçš„è­¦å ±æª”æ¡ˆ
            for alert_file in self.alerts_dir.glob("alerts_*.jsonl"):
                try:
                    file_date_str = alert_file.stem.replace("alerts_", "")
                    file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                    
                    if file_date < cutoff_date:
                        alert_file.unlink()
                        logger.info(f"å·²åˆªé™¤èˆŠè­¦å ±æª”æ¡ˆ: {alert_file}")
                        
                except ValueError:
                    continue
            
        except Exception as e:
            logger.error(f"æ¸…ç†èˆŠè³‡æ–™å¤±æ•—: {e}")
    
    def generate_report(self, days: int = 7) -> Dict[str, Any]:
        """ç”Ÿæˆç›£æ§å ±å‘Š"""
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # æ”¶é›†æŒ‡æ¨™è³‡æ–™
            all_metrics = []
            for i in range(days):
                date = start_date + timedelta(days=i)
                date_str = date.strftime("%Y-%m-%d")
                metrics_file = self.metrics_dir / f"metrics_{date_str}.jsonl"
                
                if metrics_file.exists():
                    with open(metrics_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                metrics = json.loads(line.strip())
                                all_metrics.append(metrics)
                            except json.JSONDecodeError:
                                continue
            
            if not all_metrics:
                return {"error": "æ²’æœ‰æ‰¾åˆ°æŒ‡æ¨™è³‡æ–™"}
            
            # è¨ˆç®—çµ±è¨ˆè³‡æ–™
            cpu_values = [m.get("system", {}).get("cpu", {}).get("percent", 0) for m in all_metrics]
            memory_values = [m.get("system", {}).get("memory", {}).get("percent", 0) for m in all_metrics]
            disk_values = [m.get("system", {}).get("disk", {}).get("percent", 0) for m in all_metrics]
            
            # æ”¶é›†è­¦å ±è³‡æ–™
            all_alerts = []
            for i in range(days):
                date = start_date + timedelta(days=i)
                date_str = date.strftime("%Y-%m-%d")
                alert_file = self.alerts_dir / f"alerts_{date_str}.jsonl"
                
                if alert_file.exists():
                    with open(alert_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                alert = json.loads(line.strip())
                                all_alerts.append(alert)
                            except json.JSONDecodeError:
                                continue
            
            report = {
                "period": {
                    "start_date": start_date.isoformat(),
                    "end_date": end_date.isoformat(),
                    "days": days
                },
                "metrics_summary": {
                    "total_data_points": len(all_metrics),
                    "cpu": {
                        "avg": sum(cpu_values) / len(cpu_values) if cpu_values else 0,
                        "max": max(cpu_values) if cpu_values else 0,
                        "min": min(cpu_values) if cpu_values else 0
                    },
                    "memory": {
                        "avg": sum(memory_values) / len(memory_values) if memory_values else 0,
                        "max": max(memory_values) if memory_values else 0,
                        "min": min(memory_values) if memory_values else 0
                    },
                    "disk": {
                        "avg": sum(disk_values) / len(disk_values) if disk_values else 0,
                        "max": max(disk_values) if disk_values else 0,
                        "min": min(disk_values) if disk_values else 0
                    }
                },
                "alerts_summary": {
                    "total_alerts": len(all_alerts),
                    "critical_alerts": len([a for a in all_alerts if a.get("severity") == "critical"]),
                    "warning_alerts": len([a for a in all_alerts if a.get("severity") == "warning"]),
                    "alert_types": {}
                },
                "generated_at": datetime.now().isoformat()
            }
            
            # çµ±è¨ˆè­¦å ±é¡å‹
            for alert in all_alerts:
                alert_type = alert.get("type", "unknown")
                report["alerts_summary"]["alert_types"][alert_type] = \
                    report["alerts_summary"]["alert_types"].get(alert_type, 0) + 1
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç›£æ§å ±å‘Šå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def run_continuous_monitoring(self):
        """åŸ·è¡Œé€£çºŒç›£æ§"""
        logger.info("ğŸ” é–‹å§‹é€£çºŒç›£æ§")
        
        interval = self.config["collection_interval"]
        
        try:
            while True:
                # æ”¶é›†æŒ‡æ¨™
                system_metrics = self.collect_system_metrics()
                app_metrics = self.collect_application_metrics()
                
                # åˆä½µæŒ‡æ¨™
                combined_metrics = {**system_metrics, **app_metrics}
                
                if combined_metrics:
                    # å„²å­˜æŒ‡æ¨™
                    self.save_metrics(combined_metrics)
                    
                    # æª¢æŸ¥è­¦å ±
                    alerts = self.check_alerts(combined_metrics)
                    
                    if alerts:
                        logger.info(f"æª¢æ¸¬åˆ° {len(alerts)} å€‹è­¦å ±")
                
                # å®šæœŸæ¸…ç†èˆŠè³‡æ–™
                if int(time.time()) % 3600 == 0:  # æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
                    self.cleanup_old_data()
                
                # ç­‰å¾…ä¸‹æ¬¡æ”¶é›†
                time.sleep(interval)
                
        except KeyboardInterrupt:
            logger.info("ç›£æ§å·²åœæ­¢")
        except Exception as e:
            logger.error(f"ç›£æ§éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


class MaintenanceTools:
    """ç¶­è­·å·¥å…·"""
    
    def __init__(self):
        self.data_dir = Path("./data")
        self.logs_dir = Path("./logs")
        self.cache_dir = Path("./cache")
    
    def cleanup_logs(self, days: int = 30):
        """æ¸…ç†èˆŠæ—¥èªŒ"""
        logger.info(f"ğŸ§¹ æ¸…ç† {days} å¤©å‰çš„æ—¥èªŒ")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_count = 0
        
        for log_file in self.logs_dir.rglob("*.log*"):
            try:
                if log_file.stat().st_mtime < cutoff_date.timestamp():
                    log_file.unlink()
                    cleaned_count += 1
                    logger.info(f"å·²åˆªé™¤: {log_file}")
            except Exception as e:
                logger.warning(f"åˆªé™¤æ—¥èªŒæª”æ¡ˆå¤±æ•— {log_file}: {e}")
        
        logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±åˆªé™¤ {cleaned_count} å€‹æ—¥èªŒæª”æ¡ˆ")
    
    def cleanup_cache(self):
        """æ¸…ç†å¿«å–"""
        logger.info("ğŸ§¹ æ¸…ç†å¿«å–æª”æ¡ˆ")
        
        cleaned_count = 0
        cleaned_size = 0
        
        for cache_file in self.cache_dir.rglob("*"):
            if cache_file.is_file():
                try:
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    cleaned_count += 1
                    cleaned_size += file_size
                    logger.debug(f"å·²åˆªé™¤å¿«å–: {cache_file}")
                except Exception as e:
                    logger.warning(f"åˆªé™¤å¿«å–æª”æ¡ˆå¤±æ•— {cache_file}: {e}")
        
        logger.info(f"âœ… å¿«å–æ¸…ç†å®Œæˆï¼Œå…±åˆªé™¤ {cleaned_count} å€‹æª”æ¡ˆï¼Œé‡‹æ”¾ {cleaned_size / (1024**2):.1f}MB ç©ºé–“")
    
    def optimize_database(self):
        """å„ªåŒ–è³‡æ–™åº«"""
        logger.info("ğŸ”§ å„ªåŒ–è³‡æ–™åº«")
        
        # é€™è£¡å¯ä»¥æ·»åŠ å…·é«”çš„è³‡æ–™åº«å„ªåŒ–é‚è¼¯
        # ä¾‹å¦‚ï¼šé‡å»ºç´¢å¼•ã€å£“ç¸®è³‡æ–™ã€æ¸…ç†ç¢ç‰‡ç­‰
        
        logger.info("âœ… è³‡æ–™åº«å„ªåŒ–å®Œæˆ")
    
    def check_system_health(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹"""
        logger.info("ğŸ¥ æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹")
        
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "checks": {}
        }
        
        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        try:
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100
            
            health_status["checks"]["disk_space"] = {
                "status": "healthy" if disk_percent < 90 else "warning",
                "usage_percent": disk_percent,
                "free_gb": disk.free / (1024**3)
            }
        except Exception as e:
            health_status["checks"]["disk_space"] = {"status": "error", "error": str(e)}
        
        # æª¢æŸ¥è¨˜æ†¶é«”
        try:
            memory = psutil.virtual_memory()
            
            health_status["checks"]["memory"] = {
                "status": "healthy" if memory.percent < 85 else "warning",
                "usage_percent": memory.percent,
                "available_gb": memory.available / (1024**3)
            }
        except Exception as e:
            health_status["checks"]["memory"] = {"status": "error", "error": str(e)}
        
        # æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼é€²ç¨‹
        try:
            app_processes = []
            for proc in psutil.process_iter(['pid', 'name']):
                if 'chinese_graphrag' in proc.info['name'] or 'uvicorn' in proc.info['name']:
                    app_processes.append(proc.info)
            
            health_status["checks"]["application_processes"] = {
                "status": "healthy" if app_processes else "critical",
                "process_count": len(app_processes),
                "processes": app_processes
            }
        except Exception as e:
            health_status["checks"]["application_processes"] = {"status": "error", "error": str(e)}
        
        # æª¢æŸ¥è³‡æ–™åº«æª”æ¡ˆ
        try:
            vector_db_exists = (self.data_dir / "vector_db").exists()
            graph_db_exists = (self.data_dir / "graph_db").exists()
            
            health_status["checks"]["databases"] = {
                "status": "healthy" if vector_db_exists and graph_db_exists else "warning",
                "vector_db_exists": vector_db_exists,
                "graph_db_exists": graph_db_exists
            }
        except Exception as e:
            health_status["checks"]["databases"] = {"status": "error", "error": str(e)}
        
        # è¨ˆç®—æ•´é«”ç‹€æ…‹
        check_statuses = [check.get("status", "error") for check in health_status["checks"].values()]
        if "critical" in check_statuses:
            health_status["overall_status"] = "critical"
        elif "error" in check_statuses or "warning" in check_statuses:
            health_status["overall_status"] = "warning"
        
        return health_status


@click.group()
def cli():
    """ç›£æ§å’Œç¶­è­·å·¥å…·"""
    pass


@cli.command()
@click.option("--config", type=click.Path(), help="ç›£æ§é…ç½®æª”æ¡ˆ")
@click.option("--interval", type=int, default=60, help="æ”¶é›†é–“éš”ï¼ˆç§’ï¼‰")
def monitor(config: Optional[str], interval: int):
    """å•Ÿå‹•ç³»çµ±ç›£æ§"""
    config_path = Path(config) if config else None
    
    monitor = SystemMonitor(config_path)
    monitor.config["collection_interval"] = interval
    
    monitor.run_continuous_monitoring()


@cli.command()
@click.option("--days", type=int, default=7, help="å ±å‘Šå¤©æ•¸")
@click.option("--output", type=click.Path(), help="è¼¸å‡ºæª”æ¡ˆ")
def report(days: int, output: Optional[str]):
    """ç”Ÿæˆç›£æ§å ±å‘Š"""
    monitor = SystemMonitor()
    report_data = monitor.generate_report(days)
    
    if output:
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        logger.info(f"å ±å‘Šå·²å„²å­˜è‡³: {output}")
    else:
        print(json.dumps(report_data, ensure_ascii=False, indent=2))


@cli.command()
@click.option("--log-days", type=int, default=30, help="ä¿ç•™æ—¥èªŒå¤©æ•¸")
@click.option("--clean-cache", is_flag=True, help="æ¸…ç†å¿«å–")
def cleanup(log_days: int, clean_cache: bool):
    """åŸ·è¡Œç³»çµ±æ¸…ç†"""
    tools = MaintenanceTools()
    
    tools.cleanup_logs(log_days)
    
    if clean_cache:
        tools.cleanup_cache()
    
    tools.optimize_database()


@cli.command()
def health():
    """æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹"""
    tools = MaintenanceTools()
    health_status = tools.check_system_health()
    
    print(json.dumps(health_status, ensure_ascii=False, indent=2))
    
    # æ ¹æ“šå¥åº·ç‹€æ…‹è¨­å®šé€€å‡ºç¢¼
    if health_status["overall_status"] == "critical":
        sys.exit(2)
    elif health_status["overall_status"] == "warning":
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    cli()