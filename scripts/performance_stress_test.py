#!/usr/bin/env python3
"""
æ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦è…³æœ¬

åŸ·è¡Œç³»çµ±æ•ˆèƒ½åŸºæº–æ¸¬è©¦å’Œå£“åŠ›æ¸¬è©¦ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡ä»¶è™•ç†æ•ˆèƒ½æ¸¬è©¦
- æŸ¥è©¢å›æ‡‰æ™‚é–“æ¸¬è©¦
- è¨˜æ†¶é«”ä½¿ç”¨é‡æ¸¬è©¦
- ä¸¦ç™¼è™•ç†èƒ½åŠ›æ¸¬è©¦
- å¤§é‡è³‡æ–™è™•ç†æ¸¬è©¦
"""

import asyncio
import json
import logging
import os
import psutil
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import traceback

import click
import numpy as np
import pandas as pd
from loguru import logger

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.chinese_graphrag.processors.chinese_text_processor import ChineseTextProcessor
from src.chinese_graphrag.embeddings.manager import EmbeddingManager
from src.chinese_graphrag.config.loader import ConfigLoader


class PerformanceStressTester:
    """æ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦å™¨"""
    
    def __init__(self, test_dir: Path, config_path: Optional[Path] = None):
        self.test_dir = test_dir
        self.config_path = config_path or Path("config/settings.yaml")
        self.results: Dict[str, Any] = {
            "timestamp": time.time(),
            "performance_tests": {},
            "stress_tests": {},
            "system_metrics": {},
            "benchmarks": {},
            "summary": {}
        }
        
        # è¨­å®šæ—¥èªŒ
        logger.remove()
        logger.add(
            self.test_dir / "performance_test.log",
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message}",
            rotation="10 MB"
        )
        logger.add(sys.stdout, level="INFO")
        
        # ç³»çµ±ç›£æ§
        self.system_monitor = SystemMonitor()
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ•ˆèƒ½æ¸¬è©¦"""
        logger.info("âš¡ é–‹å§‹æ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦")
        
        try:
            # é–‹å§‹ç³»çµ±ç›£æ§
            self.system_monitor.start_monitoring()
            
            # 1. æ–‡ä»¶è™•ç†æ•ˆèƒ½æ¸¬è©¦
            await self._test_document_processing_performance()
            
            # 2. æŸ¥è©¢å›æ‡‰æ™‚é–“æ¸¬è©¦
            await self._test_query_response_time()
            
            # 3. è¨˜æ†¶é«”ä½¿ç”¨é‡æ¸¬è©¦
            await self._test_memory_usage()
            
            # 4. ä¸¦ç™¼è™•ç†èƒ½åŠ›æ¸¬è©¦
            await self._test_concurrent_processing()
            
            # 5. å¤§é‡è³‡æ–™è™•ç†æ¸¬è©¦
            await self._test_large_dataset_processing()
            
            # 6. ç´¢å¼•å»ºæ§‹æ•ˆèƒ½æ¸¬è©¦
            await self._test_indexing_performance()
            
            # 7. å‘é‡æª¢ç´¢æ•ˆèƒ½æ¸¬è©¦
            await self._test_vector_search_performance()
            
            # 8. ç³»çµ±ç©©å®šæ€§æ¸¬è©¦
            await self._test_system_stability()
            
            # åœæ­¢ç³»çµ±ç›£æ§
            self.system_monitor.stop_monitoring()
            self.results["system_metrics"] = self.system_monitor.get_metrics()
            
            # ç”Ÿæˆæ•ˆèƒ½åŸºæº–
            self._generate_performance_benchmarks()
            
            # ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
            self._generate_test_summary()
            
        except Exception as e:
            logger.error(f"æ•ˆèƒ½æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
            self.system_monitor.stop_monitoring()
            
        return self.results
    
    async def _test_document_processing_performance(self):
        """æ¸¬è©¦æ–‡ä»¶è™•ç†æ•ˆèƒ½"""
        logger.info("ğŸ“„ æ¸¬è©¦æ–‡ä»¶è™•ç†æ•ˆèƒ½")
        test_name = "document_processing_performance"
        
        try:
            processor = ChineseTextProcessor()
            
            # å»ºç«‹ä¸åŒå¤§å°çš„æ¸¬è©¦æ–‡ä»¶
            test_documents = [
                {"size": "small", "content": "äººå·¥æ™ºæ…§" * 100, "expected_time": 1.0},
                {"size": "medium", "content": "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦åˆ†æ”¯" * 500, "expected_time": 3.0},
                {"size": "large", "content": "æ·±åº¦å­¸ç¿’æŠ€è¡“åœ¨å„å€‹é ˜åŸŸéƒ½æœ‰å»£æ³›æ‡‰ç”¨" * 1000, "expected_time": 5.0}
            ]
            
            processing_results = []
            
            for doc in test_documents:
                start_time = time.time()
                
                # åŸ·è¡Œæ–‡ä»¶è™•ç†
                processed_text = processor.preprocess_text(doc["content"])
                chunks = processor.split_text(processed_text, chunk_size=512)
                
                processing_time = time.time() - start_time
                
                result = {
                    "document_size": doc["size"],
                    "original_length": len(doc["content"]),
                    "processed_length": len(processed_text),
                    "chunks_count": len(chunks),
                    "processing_time": processing_time,
                    "expected_time": doc["expected_time"],
                    "performance_ratio": processing_time / doc["expected_time"],
                    "throughput": len(doc["content"]) / processing_time if processing_time > 0 else 0
                }
                
                processing_results.append(result)
                logger.info(f"  {doc['size']} æ–‡ä»¶: {processing_time:.2f}s (é æœŸ: {doc['expected_time']}s)")
            
            # è¨ˆç®—å¹³å‡æ•ˆèƒ½
            avg_performance_ratio = np.mean([r["performance_ratio"] for r in processing_results])
            performance_pass = avg_performance_ratio <= 1.2  # å…è¨± 20% çš„æ•ˆèƒ½å·®ç•°
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if performance_pass else "FAIL",
                "results": processing_results,
                "average_performance_ratio": avg_performance_ratio,
                "total_throughput": sum(r["throughput"] for r in processing_results)
            }
            
            logger.info(f"âœ… æ–‡ä»¶è™•ç†æ•ˆèƒ½: {'é€šé' if performance_pass else 'å¤±æ•—'} (å¹³å‡æ¯”ç‡: {avg_performance_ratio:.2f})")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ æ–‡ä»¶è™•ç†æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_query_response_time(self):
        """æ¸¬è©¦æŸ¥è©¢å›æ‡‰æ™‚é–“"""
        logger.info("â“ æ¸¬è©¦æŸ¥è©¢å›æ‡‰æ™‚é–“")
        test_name = "query_response_time"
        
        try:
            # ä¸åŒè¤‡é›œåº¦çš„æŸ¥è©¢
            test_queries = [
                {"query": "ä»€éº¼æ˜¯AIï¼Ÿ", "complexity": "simple", "expected_time": 1.0},
                {"query": "è«‹è©³ç´°èªªæ˜æ©Ÿå™¨å­¸ç¿’çš„ä¸»è¦æ¼”ç®—æ³•é¡å‹åŠå…¶æ‡‰ç”¨å ´æ™¯", "complexity": "medium", "expected_time": 2.0},
                {"query": "æ¯”è¼ƒæ·±åº¦å­¸ç¿’èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’åœ¨è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ä¸­çš„å„ªç¼ºé»ï¼Œä¸¦åˆ†ææœªä¾†ç™¼å±•è¶¨å‹¢", "complexity": "complex", "expected_time": 3.0}
            ]
            
            query_results = []
            
            for query_data in test_queries:
                start_time = time.time()
                
                # æ¨¡æ“¬æŸ¥è©¢è™•ç†
                await asyncio.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
                response = f"é€™æ˜¯å°æŸ¥è©¢ã€Œ{query_data['query']}ã€çš„å›æ‡‰"
                
                response_time = time.time() - start_time
                
                result = {
                    "query": query_data["query"],
                    "complexity": query_data["complexity"],
                    "response_time": response_time,
                    "expected_time": query_data["expected_time"],
                    "performance_ratio": response_time / query_data["expected_time"],
                    "response_length": len(response)
                }
                
                query_results.append(result)
                logger.info(f"  {query_data['complexity']} æŸ¥è©¢: {response_time:.2f}s")
            
            # è¨ˆç®—å¹³å‡å›æ‡‰æ™‚é–“
            avg_response_time = np.mean([r["response_time"] for r in query_results])
            avg_performance_ratio = np.mean([r["performance_ratio"] for r in query_results])
            response_time_pass = avg_response_time <= 2.0
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if response_time_pass else "FAIL",
                "results": query_results,
                "average_response_time": avg_response_time,
                "average_performance_ratio": avg_performance_ratio
            }
            
            logger.info(f"âœ… æŸ¥è©¢å›æ‡‰æ™‚é–“: {'é€šé' if response_time_pass else 'å¤±æ•—'} (å¹³å‡: {avg_response_time:.2f}s)")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ æŸ¥è©¢å›æ‡‰æ™‚é–“æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_memory_usage(self):
        """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        logger.info("ğŸ’¾ æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨é‡")
        test_name = "memory_usage"
        
        try:
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ¨¡æ“¬è¨˜æ†¶é«”å¯†é›†æ“ä½œ
            memory_tests = []
            
            for i in range(3):
                # å»ºç«‹å¤§é‡è³‡æ–™
                large_data = ["æ¸¬è©¦è³‡æ–™" * 1000] * 1000
                current_memory = process.memory_info().rss / 1024 / 1024
                
                memory_tests.append({
                    "test_step": f"step_{i+1}",
                    "memory_usage_mb": current_memory,
                    "memory_increase_mb": current_memory - initial_memory
                })
                
                # æ¸…ç†è³‡æ–™
                del large_data
                
                await asyncio.sleep(0.1)
            
            # æœ€çµ‚è¨˜æ†¶é«”ä½¿ç”¨é‡
            final_memory = process.memory_info().rss / 1024 / 1024
            max_memory = max(test["memory_usage_mb"] for test in memory_tests)
            memory_threshold = 1024  # 1GB
            
            memory_pass = max_memory <= memory_threshold
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if memory_pass else "FAIL",
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "max_memory_mb": max_memory,
                "memory_threshold_mb": memory_threshold,
                "memory_tests": memory_tests
            }
            
            logger.info(f"âœ… è¨˜æ†¶é«”ä½¿ç”¨é‡: {'é€šé' if memory_pass else 'å¤±æ•—'} (æœ€å¤§: {max_memory:.1f}MB)")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ è¨˜æ†¶é«”ä½¿ç”¨é‡æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_concurrent_processing(self):
        """æ¸¬è©¦ä¸¦ç™¼è™•ç†èƒ½åŠ›"""
        logger.info("ğŸ”„ æ¸¬è©¦ä¸¦ç™¼è™•ç†èƒ½åŠ›")
        test_name = "concurrent_processing"
        
        try:
            processor = ChineseTextProcessor()
            
            # ä¸¦ç™¼æ¸¬è©¦åƒæ•¸
            concurrent_levels = [1, 5, 10, 20]
            test_text = "é€™æ˜¯ä¸€å€‹ç”¨æ–¼æ¸¬è©¦ä¸¦ç™¼è™•ç†èƒ½åŠ›çš„ä¸­æ–‡æ–‡æœ¬ç¯„ä¾‹" * 100
            
            concurrent_results = []
            
            for level in concurrent_levels:
                start_time = time.time()
                
                # å»ºç«‹ä¸¦ç™¼ä»»å‹™
                tasks = []
                for i in range(level):
                    task = asyncio.create_task(self._process_text_async(processor, test_text))
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
                results = await asyncio.gather(*tasks)
                
                total_time = time.time() - start_time
                throughput = level / total_time if total_time > 0 else 0
                
                result = {
                    "concurrent_level": level,
                    "total_time": total_time,
                    "throughput": throughput,
                    "successful_tasks": len([r for r in results if r is not None]),
                    "failed_tasks": len([r for r in results if r is None])
                }
                
                concurrent_results.append(result)
                logger.info(f"  ä¸¦ç™¼ç´šåˆ¥ {level}: {total_time:.2f}s, ååé‡: {throughput:.2f} tasks/s")
            
            # è©•ä¼°ä¸¦ç™¼æ•ˆèƒ½
            max_throughput = max(r["throughput"] for r in concurrent_results)
            concurrent_pass = max_throughput >= 5.0  # è‡³å°‘ 5 tasks/s
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if concurrent_pass else "FAIL",
                "results": concurrent_results,
                "max_throughput": max_throughput,
                "throughput_threshold": 5.0
            }
            
            logger.info(f"âœ… ä¸¦ç™¼è™•ç†èƒ½åŠ›: {'é€šé' if concurrent_pass else 'å¤±æ•—'} (æœ€å¤§ååé‡: {max_throughput:.2f})")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ ä¸¦ç™¼è™•ç†èƒ½åŠ›æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _process_text_async(self, processor, text: str):
        """ç•°æ­¥æ–‡æœ¬è™•ç†"""
        try:
            processed = processor.preprocess_text(text)
            chunks = processor.split_text(processed, chunk_size=256)
            return len(chunks)
        except Exception:
            return None
    
    async def _test_large_dataset_processing(self):
        """æ¸¬è©¦å¤§é‡è³‡æ–™è™•ç†"""
        logger.info("ğŸ“Š æ¸¬è©¦å¤§é‡è³‡æ–™è™•ç†")
        test_name = "large_dataset_processing"
        
        try:
            processor = ChineseTextProcessor()
            
            # å»ºç«‹å¤§å‹è³‡æ–™é›†
            dataset_sizes = [100, 500, 1000]  # æ–‡ä»¶æ•¸é‡
            base_text = "äººå·¥æ™ºæ…§æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œç‚ºå„è¡Œå„æ¥­å¸¶ä¾†é©å‘½æ€§çš„è®ŠåŒ–ã€‚" * 50
            
            dataset_results = []
            
            for size in dataset_sizes:
                start_time = time.time()
                
                # è™•ç†å¤§é‡æ–‡ä»¶
                processed_count = 0
                total_chunks = 0
                
                for i in range(size):
                    try:
                        processed_text = processor.preprocess_text(base_text)
                        chunks = processor.split_text(processed_text, chunk_size=512)
                        processed_count += 1
                        total_chunks += len(chunks)
                    except Exception:
                        continue
                
                processing_time = time.time() - start_time
                throughput = processed_count / processing_time if processing_time > 0 else 0
                
                result = {
                    "dataset_size": size,
                    "processed_documents": processed_count,
                    "total_chunks": total_chunks,
                    "processing_time": processing_time,
                    "throughput": throughput,
                    "success_rate": processed_count / size if size > 0 else 0
                }
                
                dataset_results.append(result)
                logger.info(f"  è³‡æ–™é›†å¤§å° {size}: {processing_time:.2f}s, ååé‡: {throughput:.2f} docs/s")
            
            # è©•ä¼°å¤§é‡è³‡æ–™è™•ç†èƒ½åŠ›
            avg_throughput = np.mean([r["throughput"] for r in dataset_results])
            avg_success_rate = np.mean([r["success_rate"] for r in dataset_results])
            
            large_dataset_pass = avg_throughput >= 10.0 and avg_success_rate >= 0.95
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if large_dataset_pass else "FAIL",
                "results": dataset_results,
                "average_throughput": avg_throughput,
                "average_success_rate": avg_success_rate,
                "throughput_threshold": 10.0,
                "success_rate_threshold": 0.95
            }
            
            logger.info(f"âœ… å¤§é‡è³‡æ–™è™•ç†: {'é€šé' if large_dataset_pass else 'å¤±æ•—'} (å¹³å‡ååé‡: {avg_throughput:.2f})")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ å¤§é‡è³‡æ–™è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_indexing_performance(self):
        """æ¸¬è©¦ç´¢å¼•å»ºæ§‹æ•ˆèƒ½"""
        logger.info("ğŸ” æ¸¬è©¦ç´¢å¼•å»ºæ§‹æ•ˆèƒ½")
        test_name = "indexing_performance"
        
        try:
            # æ¨¡æ“¬ç´¢å¼•å»ºæ§‹æ•ˆèƒ½æ¸¬è©¦
            indexing_scenarios = [
                {"documents": 100, "expected_time": 30, "scenario": "small"},
                {"documents": 500, "expected_time": 120, "scenario": "medium"},
                {"documents": 1000, "expected_time": 300, "scenario": "large"}
            ]
            
            indexing_results = []
            
            for scenario in indexing_scenarios:
                start_time = time.time()
                
                # æ¨¡æ“¬ç´¢å¼•å»ºæ§‹éç¨‹
                await asyncio.sleep(0.5)  # æ¨¡æ“¬è™•ç†æ™‚é–“
                
                indexing_time = time.time() - start_time
                
                result = {
                    "scenario": scenario["scenario"],
                    "document_count": scenario["documents"],
                    "indexing_time": indexing_time,
                    "expected_time": scenario["expected_time"],
                    "performance_ratio": indexing_time / scenario["expected_time"],
                    "throughput": scenario["documents"] / indexing_time if indexing_time > 0 else 0
                }
                
                indexing_results.append(result)
                logger.info(f"  {scenario['scenario']} ç´¢å¼•: {indexing_time:.2f}s")
            
            # è©•ä¼°ç´¢å¼•æ•ˆèƒ½
            avg_performance_ratio = np.mean([r["performance_ratio"] for r in indexing_results])
            indexing_pass = avg_performance_ratio <= 1.5  # å…è¨± 50% çš„æ•ˆèƒ½å·®ç•°
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if indexing_pass else "FAIL",
                "results": indexing_results,
                "average_performance_ratio": avg_performance_ratio
            }
            
            logger.info(f"âœ… ç´¢å¼•å»ºæ§‹æ•ˆèƒ½: {'é€šé' if indexing_pass else 'å¤±æ•—'}")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ ç´¢å¼•å»ºæ§‹æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_vector_search_performance(self):
        """æ¸¬è©¦å‘é‡æª¢ç´¢æ•ˆèƒ½"""
        logger.info("ğŸ” æ¸¬è©¦å‘é‡æª¢ç´¢æ•ˆèƒ½")
        test_name = "vector_search_performance"
        
        try:
            # æ¨¡æ“¬å‘é‡æª¢ç´¢æ•ˆèƒ½æ¸¬è©¦
            search_scenarios = [
                {"vector_count": 1000, "search_queries": 10, "expected_time": 1.0},
                {"vector_count": 10000, "search_queries": 50, "expected_time": 5.0},
                {"vector_count": 100000, "search_queries": 100, "expected_time": 15.0}
            ]
            
            search_results = []
            
            for scenario in search_scenarios:
                start_time = time.time()
                
                # æ¨¡æ“¬å‘é‡æª¢ç´¢
                for _ in range(scenario["search_queries"]):
                    await asyncio.sleep(0.01)  # æ¨¡æ“¬æª¢ç´¢æ™‚é–“
                
                search_time = time.time() - start_time
                
                result = {
                    "vector_count": scenario["vector_count"],
                    "search_queries": scenario["search_queries"],
                    "search_time": search_time,
                    "expected_time": scenario["expected_time"],
                    "performance_ratio": search_time / scenario["expected_time"],
                    "queries_per_second": scenario["search_queries"] / search_time if search_time > 0 else 0
                }
                
                search_results.append(result)
                logger.info(f"  å‘é‡æ•¸ {scenario['vector_count']}: {search_time:.2f}s")
            
            # è©•ä¼°æª¢ç´¢æ•ˆèƒ½
            avg_qps = np.mean([r["queries_per_second"] for r in search_results])
            search_pass = avg_qps >= 10.0  # è‡³å°‘ 10 queries/s
            
            self.results["performance_tests"][test_name] = {
                "status": "PASS" if search_pass else "FAIL",
                "results": search_results,
                "average_qps": avg_qps,
                "qps_threshold": 10.0
            }
            
            logger.info(f"âœ… å‘é‡æª¢ç´¢æ•ˆèƒ½: {'é€šé' if search_pass else 'å¤±æ•—'} (å¹³å‡ QPS: {avg_qps:.2f})")
            
        except Exception as e:
            self.results["performance_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ å‘é‡æª¢ç´¢æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_system_stability(self):
        """æ¸¬è©¦ç³»çµ±ç©©å®šæ€§"""
        logger.info("ğŸ›¡ï¸ æ¸¬è©¦ç³»çµ±ç©©å®šæ€§")
        test_name = "system_stability"
        
        try:
            processor = ChineseTextProcessor()
            
            # é•·æ™‚é–“é‹è¡Œæ¸¬è©¦
            stability_duration = 60  # 60ç§’
            test_interval = 5  # æ¯5ç§’æ¸¬è©¦ä¸€æ¬¡
            
            stability_results = []
            start_time = time.time()
            
            while time.time() - start_time < stability_duration:
                iteration_start = time.time()
                
                try:
                    # åŸ·è¡ŒåŸºæœ¬æ“ä½œ
                    test_text = "ç³»çµ±ç©©å®šæ€§æ¸¬è©¦æ–‡æœ¬" * 100
                    processed = processor.preprocess_text(test_text)
                    chunks = processor.split_text(processed)
                    
                    iteration_time = time.time() - iteration_start
                    
                    stability_results.append({
                        "timestamp": time.time(),
                        "iteration_time": iteration_time,
                        "success": True,
                        "chunks_count": len(chunks)
                    })
                    
                except Exception as e:
                    stability_results.append({
                        "timestamp": time.time(),
                        "iteration_time": time.time() - iteration_start,
                        "success": False,
                        "error": str(e)
                    })
                
                await asyncio.sleep(test_interval)
            
            # è©•ä¼°ç©©å®šæ€§
            total_iterations = len(stability_results)
            successful_iterations = sum(1 for r in stability_results if r["success"])
            success_rate = successful_iterations / total_iterations if total_iterations > 0 else 0
            
            avg_iteration_time = np.mean([r["iteration_time"] for r in stability_results])
            stability_pass = success_rate >= 0.95 and avg_iteration_time <= 2.0
            
            self.results["stress_tests"][test_name] = {
                "status": "PASS" if stability_pass else "FAIL",
                "test_duration": stability_duration,
                "total_iterations": total_iterations,
                "successful_iterations": successful_iterations,
                "success_rate": success_rate,
                "average_iteration_time": avg_iteration_time,
                "stability_results": stability_results[-10:]  # åªä¿ç•™æœ€å¾Œ10å€‹çµæœ
            }
            
            logger.info(f"âœ… ç³»çµ±ç©©å®šæ€§: {'é€šé' if stability_pass else 'å¤±æ•—'} (æˆåŠŸç‡: {success_rate:.2%})")
            
        except Exception as e:
            self.results["stress_tests"][test_name] = {
                "status": "ERROR",
                "error": str(e)
            }
            logger.error(f"âŒ ç³»çµ±ç©©å®šæ€§æ¸¬è©¦å¤±æ•—: {e}")
    
    def _generate_performance_benchmarks(self):
        """ç”Ÿæˆæ•ˆèƒ½åŸºæº–"""
        logger.info("ğŸ“Š ç”Ÿæˆæ•ˆèƒ½åŸºæº–")
        
        benchmarks = {}
        
        # å¾æ¸¬è©¦çµæœä¸­æå–åŸºæº–æ•¸æ“š
        for test_name, test_result in self.results["performance_tests"].items():
            if test_result["status"] == "PASS":
                if "average_response_time" in test_result:
                    benchmarks[f"{test_name}_response_time"] = test_result["average_response_time"]
                if "average_throughput" in test_result:
                    benchmarks[f"{test_name}_throughput"] = test_result["average_throughput"]
                if "max_throughput" in test_result:
                    benchmarks[f"{test_name}_max_throughput"] = test_result["max_throughput"]
        
        # ç³»çµ±è³‡æºåŸºæº–
        if "system_metrics" in self.results:
            metrics = self.results["system_metrics"]
            benchmarks.update({
                "peak_memory_usage_mb": metrics.get("peak_memory_mb", 0),
                "average_cpu_usage_percent": metrics.get("average_cpu_percent", 0),
                "peak_cpu_usage_percent": metrics.get("peak_cpu_percent", 0)
            })
        
        self.results["benchmarks"] = benchmarks
        
        logger.info("ğŸ“Š æ•ˆèƒ½åŸºæº–å·²ç”Ÿæˆ")
    
    def _generate_test_summary(self):
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦"""
        logger.info("ğŸ“‹ ç”Ÿæˆæ¸¬è©¦æ‘˜è¦")
        
        # æ•ˆèƒ½æ¸¬è©¦æ‘˜è¦
        perf_tests = self.results["performance_tests"]
        perf_total = len(perf_tests)
        perf_passed = sum(1 for result in perf_tests.values() if result["status"] == "PASS")
        perf_failed = sum(1 for result in perf_tests.values() if result["status"] == "FAIL")
        perf_errors = sum(1 for result in perf_tests.values() if result["status"] == "ERROR")
        
        # å£“åŠ›æ¸¬è©¦æ‘˜è¦
        stress_tests = self.results["stress_tests"]
        stress_total = len(stress_tests)
        stress_passed = sum(1 for result in stress_tests.values() if result["status"] == "PASS")
        stress_failed = sum(1 for result in stress_tests.values() if result["status"] == "FAIL")
        stress_errors = sum(1 for result in stress_tests.values() if result["status"] == "ERROR")
        
        total_tests = perf_total + stress_total
        total_passed = perf_passed + stress_passed
        total_failed = perf_failed + stress_failed
        total_errors = perf_errors + stress_errors
        
        self.results["summary"] = {
            "performance_tests": {
                "total": perf_total,
                "passed": perf_passed,
                "failed": perf_failed,
                "errors": perf_errors,
                "success_rate": (perf_passed / perf_total * 100) if perf_total > 0 else 0
            },
            "stress_tests": {
                "total": stress_total,
                "passed": stress_passed,
                "failed": stress_failed,
                "errors": stress_errors,
                "success_rate": (stress_passed / stress_total * 100) if stress_total > 0 else 0
            },
            "overall": {
                "total_tests": total_tests,
                "total_passed": total_passed,
                "total_failed": total_failed,
                "total_errors": total_errors,
                "overall_success_rate": (total_passed / total_tests * 100) if total_tests > 0 else 0,
                "overall_status": "PASS" if total_failed == 0 and total_errors == 0 else "FAIL"
            }
        }
        
        summary = self.results["summary"]
        logger.info(f"ğŸ“‹ æ¸¬è©¦æ‘˜è¦:")
        logger.info(f"   æ•ˆèƒ½æ¸¬è©¦: {perf_passed}/{perf_total} é€šé")
        logger.info(f"   å£“åŠ›æ¸¬è©¦: {stress_passed}/{stress_total} é€šé")
        logger.info(f"   æ•´é«”æˆåŠŸç‡: {summary['overall']['overall_success_rate']:.1f}%")
        logger.info(f"   æ•´é«”ç‹€æ…‹: {summary['overall']['overall_status']}")
    
    def save_results(self, output_path: Path):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ æ¸¬è©¦çµæœå·²å„²å­˜è‡³: {output_path}")


class SystemMonitor:
    """ç³»çµ±ç›£æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "disk_usage": [],
            "network_io": []
        }
        self.monitor_thread = None
    
    def start_monitoring(self):
        """é–‹å§‹ç›£æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        logger.info("ğŸ” ç³»çµ±ç›£æ§å·²é–‹å§‹")
    
    def stop_monitoring(self):
        """åœæ­¢ç›£æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        logger.info("ğŸ” ç³»çµ±ç›£æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring:
            try:
                # CPU ä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=1)
                self.metrics["cpu_usage"].append({
                    "timestamp": time.time(),
                    "value": cpu_percent
                })
                
                # è¨˜æ†¶é«”ä½¿ç”¨ç‡
                memory = psutil.virtual_memory()
                self.metrics["memory_usage"].append({
                    "timestamp": time.time(),
                    "value": memory.percent,
                    "used_mb": memory.used / 1024 / 1024,
                    "available_mb": memory.available / 1024 / 1024
                })
                
                # ç£ç¢Ÿä½¿ç”¨ç‡
                disk = psutil.disk_usage('/')
                self.metrics["disk_usage"].append({
                    "timestamp": time.time(),
                    "value": disk.percent,
                    "used_gb": disk.used / 1024 / 1024 / 1024,
                    "free_gb": disk.free / 1024 / 1024 / 1024
                })
                
            except Exception as e:
                logger.warning(f"ç›£æ§æ•¸æ“šæ”¶é›†å¤±æ•—: {e}")
            
            time.sleep(5)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡
    
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å–ç›£æ§æŒ‡æ¨™"""
        if not self.metrics["cpu_usage"]:
            return {}
        
        return {
            "peak_cpu_percent": max(m["value"] for m in self.metrics["cpu_usage"]),
            "average_cpu_percent": np.mean([m["value"] for m in self.metrics["cpu_usage"]]),
            "peak_memory_mb": max(m["used_mb"] for m in self.metrics["memory_usage"]),
            "average_memory_mb": np.mean([m["used_mb"] for m in self.metrics["memory_usage"]]),
            "peak_memory_percent": max(m["value"] for m in self.metrics["memory_usage"]),
            "monitoring_duration": len(self.metrics["cpu_usage"]) * 5,  # ç§’
            "data_points": len(self.metrics["cpu_usage"])
        }


@click.command()
@click.option("--test-dir", type=click.Path(), default="./performance_test_results", 
              help="æ¸¬è©¦çµæœç›®éŒ„")
@click.option("--config", type=click.Path(), default="config/settings.yaml",
              help="é…ç½®æª”æ¡ˆè·¯å¾‘")
@click.option("--output", type=click.Path(), default="performance_test_results.json",
              help="çµæœè¼¸å‡ºæª”æ¡ˆ")
@click.option("--duration", type=int, default=60, help="å£“åŠ›æ¸¬è©¦æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰")
@click.option("--verbose", "-v", is_flag=True, help="è©³ç´°è¼¸å‡º")
def main(test_dir: str, config: str, output: str, duration: int, verbose: bool):
    """åŸ·è¡Œæ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦"""
    
    # è¨­å®šæ—¥èªŒç´šåˆ¥
    if verbose:
        logger.remove()
        logger.add(sys.stdout, level="DEBUG")
    
    # å»ºç«‹æ¸¬è©¦ç›®éŒ„
    test_dir_path = Path(test_dir)
    test_dir_path.mkdir(parents=True, exist_ok=True)
    
    # åŸ·è¡Œæ¸¬è©¦
    tester = PerformanceStressTester(
        test_dir=test_dir_path,
        config_path=Path(config) if config else None
    )
    
    # åŸ·è¡Œç•°æ­¥æ¸¬è©¦
    results = asyncio.run(tester.run_all_tests())
    
    # å„²å­˜çµæœ
    output_path = test_dir_path / output
    tester.save_results(output_path)
    
    # è¼¸å‡ºæœ€çµ‚çµæœ
    summary = results["summary"]["overall"]
    if summary["overall_status"] == "PASS":
        logger.success(f"ğŸ‰ æ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦é€šéï¼æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
        sys.exit(0)
    else:
        logger.error(f"âŒ æ•ˆèƒ½å’Œå£“åŠ›æ¸¬è©¦å¤±æ•—ï¼æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
        sys.exit(1)


if __name__ == "__main__":
    main()